#!/usr/bin/env python3
"""
κ°μ„ λ μλ£ νμΈνλ‹ μ¤ν¬λ¦½νΈ
- 42dot LLM μ‚¬μ©
- κ°μ„ λ νλΌλ―Έν„°
- λ©”λ¨λ¦¬ μµμ ν™”
"""

import os
import json
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from torch.utils.data import Dataset
import warnings
warnings.filterwarnings("ignore")

# ν™κ²½ λ³€μ μ„¤μ •
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["WANDB_DISABLED"] = "true"

class MedicalDataset(Dataset):
    def __init__(self, data, tokenizer, max_length=512):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # λ€ν™” ν•μ‹μΌλ΅ λ³€ν™
        if 'question' in item and 'answer' in item:
            text = f"ν™μ: {item['question']}\nμλ£μ§„: {item['answer']}"
        else:
            text = str(item)
        
        # ν† ν¬λ‚μ΄μ§•
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': encoding['input_ids'].flatten()
        }

class ImprovedMedicalFineTuner:
    def __init__(self, model_name="42dot/42dot_LLM-SFT-1.3B"):
        """κ°μ„ λ μλ£ νμΈνλ„ μ΄κΈ°ν™”"""
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"π”§ μ‚¬μ© λ””λ°”μ΄μ¤: {self.device}")
        
        # λ¨λΈκ³Ό ν† ν¬λ‚μ΄μ € λ΅λ”©
        self._load_model()
        
    def _load_model(self):
        """λ¨λΈκ³Ό ν† ν¬λ‚μ΄μ € λ΅λ”©"""
        print(f"\nπ“¥ λ¨λΈ λ΅λ”© μ¤‘: {self.model_name}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float32,
                device_map="auto" if self.device == "cuda" else None
            )
            
            # ν¨λ”© ν† ν° μ„¤μ •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            print("β… λ¨λΈ λ΅λ“ μ™„λ£")
            
        except Exception as e:
            print(f"β λ¨λΈ λ΅λ”© μ‹¤ν¨: {e}")
            raise
    
    def load_data(self, data_paths):
        """λ°μ΄ν„° λ΅λ”©"""
        print(f"\nπ“ λ°μ΄ν„° λ΅λ”© μ¤‘...")
        
        all_data = []
        for path in data_paths:
            if os.path.exists(path):
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    all_data.extend(data)
                    print(f"β… {path}: {len(data)}κ° μƒν”")
            else:
                print(f"β οΈ {path}: νμΌ μ—†μ")
        
        print(f"π“ μ΄ λ°μ΄ν„°: {len(all_data)}κ° μƒν”")
        return all_data
    
    def fine_tune(self, train_data, val_data, output_dir="./models/medical_finetuned_improved"):
        """κ°μ„ λ νμΈνλ‹ μ‹¤ν–‰"""
        print(f"\nπ€ κ°μ„ λ μλ£ νμΈνλ‹ μ‹μ‘")
        print(f"π“ Train: {len(train_data)}κ°, Val: {len(val_data)}κ°")
        
        # λ°μ΄ν„°μ…‹ μƒμ„±
        train_dataset = MedicalDataset(train_data, self.tokenizer)
        val_dataset = MedicalDataset(val_data, self.tokenizer)
        
        # λ°μ΄ν„° μ½λ μ΄ν„°
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
            pad_to_multiple_of=8
        )
        
        # κ°μ„ λ νλΌλ―Έν„°
        training_args = TrainingArguments(
            output_dir=output_dir,
            overwrite_output_dir=True,
            num_train_epochs=5,  # λ” λ§μ€ μ—ν¬ν¬
            per_device_train_batch_size=1,
            per_device_eval_batch_size=1,
            warmup_steps=100,
            weight_decay=0.01,
            logging_dir=f"{output_dir}/logs",
            logging_steps=10,
            eval_strategy="steps",
            eval_steps=100,
            save_steps=100,
            save_total_limit=3,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            report_to=[],
            dataloader_pin_memory=False,
            remove_unused_columns=False,
            gradient_accumulation_steps=4,
            fp16=False,
            max_steps=1000,  # λ” λ§μ€ μ¤ν…
            learning_rate=2e-5,
            warmup_ratio=0.03
        )
        
        # νΈλ μ΄λ„ μƒμ„±
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer
        )
        
        # ν•™μµ μ‹¤ν–‰
        print(f"\nπ“ ν•™μµ μ‹μ‘...")
        trainer.train()
        
        # λ¨λΈ μ €μ¥
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        
        print(f"\nβ… κ°μ„ λ νμΈνλ‹ μ™„λ£!")
        print(f"π“ λ¨λΈ μ €μ¥ μ„μΉ: {output_dir}")
        
        return trainer

def main():
    """λ©”μΈ μ‹¤ν–‰ ν•¨μ"""
    print("π€ κ°μ„ λ μλ£ νμΈνλ‹ μ‹μ‘")
    print("=" * 50)
    
    # νμΈνλ„ μ΄κΈ°ν™”
    fine_tuner = ImprovedMedicalFineTuner()
    
    # λ°μ΄ν„° λ΅λ”© (Train/Valλ§ μ‚¬μ© - Data Leakage λ°©μ§€)
    train_paths = [
        "data/processed/splits/essential_medical_train.json",
        "data/processed/splits/professional_medical_train.json"
    ]
    
    val_paths = [
        "data/processed/splits/essential_medical_val.json",
        "data/processed/splits/professional_medical_val.json"
    ]
    
    train_data = fine_tuner.load_data(train_paths)
    val_data = fine_tuner.load_data(val_paths)
    
    # νμΈνλ‹ μ‹¤ν–‰
    trainer = fine_tuner.fine_tune(train_data, val_data)
    
    print("\nπ‰ κ°μ„ λ μλ£ νμΈνλ‹ μ™„λ£!")

if __name__ == "__main__":
    main()
