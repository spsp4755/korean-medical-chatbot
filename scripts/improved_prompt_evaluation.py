import os
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from bert_score import score as bert_score
import numpy as np
import time
from datetime import datetime
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")

# í™˜ê²½ ì„¤ì •
os.environ["TOKENIZERS_PARALLELISM"] = "false"

class ImprovedPromptEvaluator:
    def __init__(self, base_model_path="42dot/42dot_LLM-SFT-1.3B", 
                 peft_model_path="models/medical_finetuned_peft"):
        self.base_model_path = base_model_path
        self.peft_model_path = peft_model_path
        self.tokenizer = None
        self.model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        self._load_model()
        
    def _load_model(self):
        """PEFT ëª¨ë¸ ë¡œë“œ"""
        print("PEFT ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_path)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_path,
            torch_dtype=torch.float32,
            device_map="cpu",
            low_cpu_mem_usage=True
        )
        
        self.model = PeftModel.from_pretrained(base_model, self.peft_model_path)
        self.model.eval()
        
        print("PEFT ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def create_improved_prompt(self, question):
        """ê°œì„ ëœ í”„ë¡¬í”„íŠ¸"""
        return f"""ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ì‹¤ìš©ì ì¸ ì˜ë£Œ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤. í™˜ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ë‹µë³€í•´ì£¼ì„¸ìš”:

1. ë¨¼ì € ì‘ê¸‰ìƒí™©ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”
2. ì¼ë°˜ì¸ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ë¡œ ì„¤ëª…í•˜ì„¸ìš”
3. ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì¹˜ë¥¼ ì œì‹œí•˜ì„¸ìš”
4. ë‹µë³€ì€ 200ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ í•˜ì„¸ìš”
5. í•„ìš”ì‹œ ì˜ì‚¬ ìƒë‹´ì„ ê¶Œí•˜ì„¸ìš”

í™˜ì ì§ˆë¬¸: {question}

ì˜ë£Œì§„ ë‹µë³€:"""
    
    def generate_response(self, question, max_length=256):
        """ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„± (Basicê³¼ ë™ì¼í•œ íŒŒë¼ë¯¸í„°)"""
        prompt = self.create_improved_prompt(question)
        
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=inputs.input_ids.shape[1] + max_length,
                num_return_sequences=1,
                temperature=0.3,  # Basicê³¼ ë™ì¼
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.1,
                top_p=0.9,  # Basicê³¼ ë™ì¼
                top_k=50    # Basicê³¼ ë™ì¼
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.replace(prompt, "").strip()
        
        return response
    
    def evaluate_response_quality(self, response):
        """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
        # ì‘ë‹µ ê¸¸ì´
        length = len(response)
        
        # ì‘ê¸‰ìƒí™© í‚¤ì›Œë“œ ì²´í¬
        emergency_keywords = ["ì‘ê¸‰", "ì¦‰ì‹œ", "ë³‘ì›", "ì‘ê¸‰ì‹¤", "119", "ì‹¬ê°", "ìœ„í—˜", "ê³ ì—´", "38ë„"]
        emergency_score = sum(1 for keyword in emergency_keywords if keyword in response)
        
        # ì‹¤ìš©ì  ì¡°ì¹˜ í‚¤ì›Œë“œ ì²´í¬
        practical_keywords = ["í•˜ì„¸ìš”", "ë“œì„¸ìš”", "ë§ˆì‹œì„¸ìš”", "íœ´ì‹", "ìˆ˜ë¶„", "ì˜ì‚¬", "ìƒë‹´", "ë³µìš©", "ì„­ì·¨"]
        practical_score = sum(1 for keyword in practical_keywords if keyword in response)
        
        # ì¼ë°˜ì¸ ì¹œí™”ì  ì–¸ì–´ ì²´í¬
        friendly_keywords = ["ë„ì›€", "ê´œì°®", "ê±±ì •", "ì•ˆì‹¬", "í™•ì¸", "ì²´í¬", "ì¤‘ìš”", "í•„ìš”"]
        friendly_score = sum(1 for keyword in friendly_keywords if keyword in response)
        
        # ì „ë¬¸ ìš©ì–´ ì²´í¬ (ë„ˆë¬´ ë§ìœ¼ë©´ ê°ì )
        medical_terms = ["í•­ìƒì œ", "ê°ìˆ˜ì„±", "ë°°ì–‘", "ì „í•´ì§ˆ", "ë¶ˆê· í˜•", "í•©ë³‘ì¦", "ì§„ë‹¨", "ì¹˜ë£Œë²•"]
        medical_term_count = sum(1 for term in medical_terms if term in response)
        
        # ì‘ê¸‰ìƒí™© íŒë‹¨ ëŠ¥ë ¥
        has_emergency_guidance = emergency_score > 0
        
        # ì‹¤ìš©ì  ì¡°ì¹˜ ì œì‹œ
        has_practical_advice = practical_score > 0
        
        # ì‚¬ìš©ì ì¹œí™”ì 
        is_user_friendly = friendly_score > 0 and medical_term_count <= 2
        
        # ì ì ˆí•œ ê¸¸ì´ (50-300ì)
        is_appropriate_length = 50 <= length <= 300
        
        # ì‘ê¸‰ìƒí™© í‚¤ì›Œë“œ í¬í•¨ë„
        emergency_coverage = emergency_score / len(emergency_keywords)
        
        # ì‹¤ìš©ì  ì¡°ì¹˜ í¬í•¨ë„
        practical_coverage = practical_score / len(practical_keywords)
        
        return {
            "length": length,
            "emergency_score": emergency_score,
            "practical_score": practical_score,
            "friendly_score": friendly_score,
            "medical_term_count": medical_term_count,
            "is_appropriate_length": is_appropriate_length,
            "has_emergency_guidance": has_emergency_guidance,
            "has_practical_advice": has_practical_advice,
            "is_user_friendly": is_user_friendly,
            "emergency_coverage": emergency_coverage,
            "practical_coverage": practical_coverage
        }
    
    def evaluate_bertscore(self, test_data, sample_size=500):
        """BERTScoreë¡œ ì„±ëŠ¥ í‰ê°€ (Basicê³¼ ë™ì¼í•œ ë°©ì‹)"""
        # ìƒ˜í”Œ ë°ì´í„° ì„ íƒ (ë©”ëª¨ë¦¬ ì ˆì•½)
        if len(test_data) > sample_size:
            import random
            test_data = random.sample(test_data, sample_size)
        
        print(f"BERTScore í‰ê°€ ì‹œì‘ (ìƒ˜í”Œ ë°ì´í„°: {len(test_data)}ê°œ)")
        
        questions = [item["question"] for item in test_data]
        ground_truths = [item["answer"] for item in test_data]
        predictions = []
        
        print("ì‘ë‹µ ìƒì„± ì¤‘...")
        for i, question in enumerate(tqdm(questions)):
            try:
                response = self.generate_response(question)
                predictions.append(response)
            except Exception as e:
                print(f"ì˜¤ë¥˜ ë°œìƒ (ì§ˆë¬¸ {i+1}): {e}")
                predictions.append("ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")
        
        print("BERTScore ê³„ì‚° ì¤‘...")
        P, R, F1 = bert_score(predictions, ground_truths, lang="ko")
        
        return {
            "bertscore_precision": P.mean().item(),
            "bertscore_recall": R.mean().item(),
            "bertscore_f1": F1.mean().item(),
            "sample_size": len(test_data)
        }
    
    def evaluate_medical_metrics(self, test_data, sample_size=500):
        """ì˜ë£Œ íŠ¹í™” ë©”íŠ¸ë¦­ í‰ê°€ (Basicê³¼ ë™ì¼í•œ ë°©ì‹)"""
        # ìƒ˜í”Œ ë°ì´í„° ì„ íƒ (ë©”ëª¨ë¦¬ ì ˆì•½)
        if len(test_data) > sample_size:
            import random
            test_data = random.sample(test_data, sample_size)
        
        print(f"ì˜ë£Œ ë©”íŠ¸ë¦­ í‰ê°€ ì‹œì‘ (ìƒ˜í”Œ ë°ì´í„°: {len(test_data)}ê°œ)")
        
        medical_terms = [
            "ì¦ìƒ", "ì¹˜ë£Œ", "ì•½ë¬¼", "ì§„ë‹¨", "ë³‘ì›", "ì˜ì‚¬", "í™˜ì", "ì§ˆë³‘", "ê°ì—¼", "ì—¼ì¦",
            "í†µì¦", "ë°œì—´", "ê¸°ì¹¨", "ë‘í†µ", "ë³µí†µ", "í”¼ë¡œ", "êµ¬í† ", "ì„¤ì‚¬", "ë³€ë¹„", "ìˆ˜ë©´",
            "í˜ˆì••", "í˜ˆë‹¹", "ì½œë ˆìŠ¤í…Œë¡¤", "ì‹¬ì¥", "í", "ê°„", "ì‹ ì¥", "ë‡Œ", "ì‹ ê²½", "ê·¼ìœ¡"
        ]
        
        total_score = 0
        medical_coverage = 0
        safety_score = 0
        response_lengths = []
        
        for item in tqdm(test_data):
            question = item["question"]
            ground_truth = item["answer"]
            
            try:
                response = self.generate_response(question)
                response_lengths.append(len(response))
                
                # ì˜ë£Œ ìš©ì–´ í¬í•¨ë„ ì²´í¬
                medical_terms_in_response = sum(1 for term in medical_terms 
                                             if term in response)
                medical_coverage += medical_terms_in_response / len(medical_terms)
                
                # ì•ˆì „ì„± ì²´í¬ (ìœ„í—˜í•œ í‘œí˜„ ì œê±°)
                dangerous_phrases = ["í™•ì‹¤íˆ", "100%", "ì ˆëŒ€", "ë¬´ì¡°ê±´", "ì§„ë‹¨", "ì¹˜ë£Œë²•"]
                if not any(phrase in response for phrase in dangerous_phrases):
                    safety_score += 1
                
                # ì‘ë‹µ ê¸¸ì´ ì ì ˆì„± (ì˜ë£Œ ì±—ë´‡ì— ìµœì í™”)
                if 20 <= len(response) <= 800:
                    total_score += 1
                    
            except Exception as e:
                print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return {
            "medical_coverage": medical_coverage / len(test_data),
            "safety_score": safety_score / len(test_data),
            "response_quality": total_score / len(test_data),
            "avg_response_length": np.mean(response_lengths),
            "sample_size": len(test_data)
        }

def main():
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ (Basicê³¼ ë™ì¼í•œ ë°©ì‹)
    print("í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ì¤‘...")
    
    test_files = [
        "data/processed/splits/essential_medical_test.json",
        "data/processed/splits/professional_medical_test.json"
    ]
    
    test_data = []
    for file_path in test_files:
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                test_data.extend(data)
                print(f"ë¡œë“œë¨: {file_path} ({len(data)}ê°œ)")
        else:
            print(f"íŒŒì¼ ì—†ìŒ: {file_path}")
    
    if not test_data:
        print("í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    print(f"ì´ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data)}ê°œ")
    
    # Improved í”„ë¡¬í”„íŠ¸ ëª¨ë¸ í‰ê°€
    evaluator = ImprovedPromptEvaluator()
    
    # BERTScore í‰ê°€
    print("\n=== BERTScore í‰ê°€ ===")
    bertscore_results = evaluator.evaluate_bertscore(test_data)
    
    # ì˜ë£Œ ë©”íŠ¸ë¦­ í‰ê°€
    print("\n=== ì˜ë£Œ ë©”íŠ¸ë¦­ í‰ê°€ ===")
    medical_results = evaluator.evaluate_medical_metrics(test_data)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n=== Improved í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ê²°ê³¼ ===")
    print(f"BERTScore Precision: {bertscore_results['bertscore_precision']:.4f}")
    print(f"BERTScore Recall: {bertscore_results['bertscore_recall']:.4f}")
    print(f"BERTScore F1: {bertscore_results['bertscore_f1']:.4f}")
    print(f"ì˜ë£Œ ìš©ì–´ í¬í•¨ë„: {medical_results['medical_coverage']:.4f}")
    print(f"ì•ˆì „ì„± ì ìˆ˜: {medical_results['safety_score']:.4f}")
    print(f"ì‘ë‹µ í’ˆì§ˆ: {medical_results['response_quality']:.4f}")
    print(f"í‰ê·  ì‘ë‹µ ê¸¸ì´: {medical_results['avg_response_length']:.1f}ì")
    print(f"í‰ê°€ ìƒ˜í”Œ ìˆ˜: {bertscore_results['sample_size']}ê°œ")
    
    # ê²°ê³¼ ì €ì¥
    results = {
        "model_type": "PEFT_42dot_LLM_Improved_Prompt",
        "evaluation_strategy": "ì „ì²´_test_ë°ì´í„°_í‰ê°€",
        "temperature": 0.3,
        "bertscore": bertscore_results,
        "medical_metrics": medical_results,
        "evaluation_timestamp": str(torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU")
    }
    
    with open("results/improved_prompt_evaluation.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print("\nê²°ê³¼ê°€ results/improved_prompt_evaluation.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    return results

if __name__ == "__main__":
    main()
