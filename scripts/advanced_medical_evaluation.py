#!/usr/bin/env python3
"""
ê³ ê¸‰ ì˜ë£Œ ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
- BERTScore ê¸°ë°˜ ì •ëŸ‰ì  í‰ê°€
- ì˜ë£Œ ì „ë¬¸ì„± í‰ê°€
- ì‘ê¸‰ìƒí™© ê°ì§€ í‰ê°€
"""

import os
import json
import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from bert_score import score
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["TOKENIZERS_PARALLELISM"] = "false"

class AdvancedMedicalEvaluator:
    def __init__(self, model_path="models/medical_finetuned_advanced"):
        """ê³ ê¸‰ ì˜ë£Œ í‰ê°€ì ì´ˆê¸°í™”"""
        self.model_path = model_path
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ëª¨ë¸ ë¡œë”©
        self._load_model()
        
    def _load_model(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©"""
        print(f"\nğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_path}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.float32,
                device_map="auto" if self.device == "cuda" else None
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def load_test_data(self, sample_size=500):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© (Data Leakage ë°©ì§€)"""
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ì¤‘...")
        
        test_paths = [
            "data/processed/splits/essential_medical_test.json",
            "data/processed/splits/professional_medical_test.json"
        ]
        
        all_data = []
        for path in test_paths:
            if os.path.exists(path):
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    all_data.extend(data)
                    print(f"âœ… {path}: {len(data)}ê°œ ìƒ˜í”Œ")
            else:
                print(f"âš ï¸ {path}: íŒŒì¼ ì—†ìŒ")
        
        # ìƒ˜í”Œë§
        if len(all_data) > sample_size:
            import random
            all_data = random.sample(all_data, sample_size)
            print(f"ğŸ“Š ìƒ˜í”Œë§: {sample_size}ê°œ")
        
        print(f"ğŸ“Š ì´ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(all_data)}ê°œ")
        return all_data
    
    def generate_response(self, question, max_length=200):
        """ì‘ë‹µ ìƒì„±"""
        prompt = f"í™˜ì: {question}\nì˜ë£Œì§„:"
        
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.replace(prompt, "").strip()
        
        return response
    
    def evaluate_medical_professionalism(self, question, response):
        """ì˜ë£Œ ì „ë¬¸ì„± í‰ê°€"""
        medical_keywords = [
            "ì˜ì‚¬", "ë³‘ì›", "ì§„ë£Œ", "ì²˜ë°©", "ì•½ë¬¼", "ì¹˜ë£Œ", "ì¦ìƒ", "ì§„ë‹¨",
            "ì‘ê¸‰", "ì‘ê¸‰ì‹¤", "119", "êµ¬ê¸‰ì°¨", "ìƒëª…", "ìœ„í—˜", "ì¦‰ì‹œ",
            "í˜ˆì••", "í˜ˆë‹¹", "ì²´ì˜¨", "ë§¥ë°•", "í˜¸í¡", "ì˜ì‹", "í†µì¦"
        ]
        
        score = 0
        for keyword in medical_keywords:
            if keyword in response:
                score += 1
        
        return min(score / len(medical_keywords) * 100, 100)
    
    def evaluate_emergency_detection(self, question, response):
        """ì‘ê¸‰ìƒí™© ê°ì§€ í‰ê°€"""
        emergency_keywords = [
            "ì‘ê¸‰", "119", "êµ¬ê¸‰ì°¨", "ì‘ê¸‰ì‹¤", "ìƒëª…", "ìœ„í—˜", "ì¦‰ì‹œ",
            "ì‹¬ì¥", "ë‡Œì¡¸ì¤‘", "ì¶œí˜ˆ", "ì˜ì‹", "í˜¸í¡", "ê°€ìŠ´", "ë³µë¶€"
        ]
        
        question_lower = question.lower()
        response_lower = response.lower()
        
        # ì§ˆë¬¸ì— ì‘ê¸‰ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
        has_emergency = any(keyword in question_lower for keyword in emergency_keywords)
        
        if has_emergency:
            # ì‘ë‹µì— ì‘ê¸‰ ëŒ€ì‘ì´ ìˆëŠ”ì§€ í™•ì¸
            has_emergency_response = any(keyword in response_lower for keyword in emergency_keywords)
            return 100 if has_emergency_response else 0
        else:
            # ì‘ê¸‰ì´ ì•„ë‹Œ ê²½ìš° ì •ìƒ ì‘ë‹µì¸ì§€ í™•ì¸
            return 100 if "ì‘ê¸‰" not in response_lower else 0
    
    def evaluate_symptom_detection(self, question, response):
        """ì¦ìƒ ê°ì§€ í‰ê°€"""
        symptom_keywords = [
            "ë‘í†µ", "ë³µí†µ", "ë°œì—´", "ê¸°ì¹¨", "ì–´ì§€ëŸ¬ì›€", "êµ¬í† ", "ì„¤ì‚¬",
            "ê°€ìŠ´", "ë³µë¶€", "ë¨¸ë¦¬", "ëª©", "ë“±", "íŒ”", "ë‹¤ë¦¬", "í†µì¦"
        ]
        
        question_lower = question.lower()
        response_lower = response.lower()
        
        detected_symptoms = []
        for symptom in symptom_keywords:
            if symptom in question_lower and symptom in response_lower:
                detected_symptoms.append(symptom)
        
        return len(detected_symptoms) / len(symptom_keywords) * 100
    
    def evaluate_response_quality(self, response):
        """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
        if not response or len(response.strip()) < 10:
            return 0
        
        # ê¸¸ì´ ì ìˆ˜ (ë„ˆë¬´ ì§§ê±°ë‚˜ ë„ˆë¬´ ê¸¸ë©´ ê°ì )
        length_score = min(len(response) / 100, 1.0) * 50
        
        # ì™„ì„±ë„ ì ìˆ˜ (ë¬¸ì¥ì´ ì™„ì „í•œì§€)
        completeness_score = 50 if response.endswith(('.', '!', '?')) else 30
        
        return length_score + completeness_score
    
    def evaluate_with_bertscore(self, questions, responses, references):
        """BERTScore ê¸°ë°˜ í‰ê°€"""
        print("\nğŸ” BERTScore í‰ê°€ ì¤‘...")
        
        # BERTScore ê³„ì‚°
        P, R, F1 = score(responses, references, lang="ko", verbose=True)
        
        return {
            "bertscore_precision": P.mean().item(),
            "bertscore_recall": R.mean().item(),
            "bertscore_f1": F1.mean().item()
        }
    
    def evaluate(self, sample_size=500):
        """ì¢…í•© í‰ê°€ ì‹¤í–‰"""
        print(f"\nğŸš€ ê³ ê¸‰ ì˜ë£Œ ëª¨ë¸ í‰ê°€ ì‹œì‘")
        print("=" * 50)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©
        test_data = self.load_test_data(sample_size)
        
        # í‰ê°€ ê²°ê³¼ ì €ì¥
        results = {
            "total_samples": len(test_data),
            "evaluations": [],
            "scores": {
                "medical_professionalism": [],
                "emergency_detection": [],
                "symptom_detection": [],
                "response_quality": [],
                "response_times": []
            }
        }
        
        print(f"\nğŸ“Š í‰ê°€ ì§„í–‰ ì¤‘...")
        
        for i, item in enumerate(tqdm(test_data, desc="í‰ê°€ ì§„í–‰")):
            question = item.get('question', '')
            reference = item.get('answer', '')
            
            # ì‘ë‹µ ìƒì„±
            start_time = time.time()
            response = self.generate_response(question)
            response_time = time.time() - start_time
            
            # ê° í•­ëª©ë³„ í‰ê°€
            medical_score = self.evaluate_medical_professionalism(question, response)
            emergency_score = self.evaluate_emergency_detection(question, response)
            symptom_score = self.evaluate_symptom_detection(question, response)
            quality_score = self.evaluate_response_quality(response)
            
            # ê²°ê³¼ ì €ì¥
            eval_result = {
                "question": question,
                "response": response,
                "reference": reference,
                "medical_professionalism": medical_score,
                "emergency_detection": emergency_score,
                "symptom_detection": symptom_score,
                "response_quality": quality_score,
                "response_time": response_time
            }
            
            results["evaluations"].append(eval_result)
            results["scores"]["medical_professionalism"].append(medical_score)
            results["scores"]["emergency_detection"].append(emergency_score)
            results["scores"]["symptom_detection"].append(symptom_score)
            results["scores"]["response_quality"].append(quality_score)
            results["scores"]["response_times"].append(response_time)
        
        # BERTScore í‰ê°€
        questions = [item.get('question', '') for item in test_data]
        responses = [eval_result["response"] for eval_result in results["evaluations"]]
        references = [item.get('answer', '') for item in test_data]
        
        bertscore_results = self.evaluate_with_bertscore(questions, responses, references)
        results["bertscore"] = bertscore_results
        
        # í†µê³„ ê³„ì‚°
        results["statistics"] = {
            "medical_professionalism": {
                "mean": sum(results["scores"]["medical_professionalism"]) / len(results["scores"]["medical_professionalism"]),
                "max": max(results["scores"]["medical_professionalism"]),
                "min": min(results["scores"]["medical_professionalism"])
            },
            "emergency_detection": {
                "mean": sum(results["scores"]["emergency_detection"]) / len(results["scores"]["emergency_detection"]),
                "max": max(results["scores"]["emergency_detection"]),
                "min": min(results["scores"]["emergency_detection"])
            },
            "symptom_detection": {
                "mean": sum(results["scores"]["symptom_detection"]) / len(results["scores"]["symptom_detection"]),
                "max": max(results["scores"]["symptom_detection"]),
                "min": min(results["scores"]["symptom_detection"])
            },
            "response_quality": {
                "mean": sum(results["scores"]["response_quality"]) / len(results["scores"]["response_quality"]),
                "max": max(results["scores"]["response_quality"]),
                "min": min(results["scores"]["response_quality"])
            },
            "response_time": {
                "mean": sum(results["scores"]["response_times"]) / len(results["scores"]["response_times"]),
                "max": max(results["scores"]["response_times"]),
                "min": min(results["scores"]["response_times"])
            }
        }
        
        # ì „ì²´ ì •í™•ë„ ê³„ì‚°
        overall_accuracy = (
            results["statistics"]["medical_professionalism"]["mean"] +
            results["statistics"]["emergency_detection"]["mean"] +
            results["statistics"]["symptom_detection"]["mean"] +
            results["statistics"]["response_quality"]["mean"]
        ) / 4
        
        results["overall_accuracy"] = overall_accuracy
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š ê³ ê¸‰ ì˜ë£Œ ëª¨ë¸ í‰ê°€ ê²°ê³¼")
        print("=" * 50)
        print(f"ğŸ“ˆ ì „ì²´ ì •í™•ë„: {overall_accuracy:.2f}%")
        print(f"ğŸ¥ ì˜ë£Œ ì „ë¬¸ì„±: {results['statistics']['medical_professionalism']['mean']:.2f}%")
        print(f"ğŸš¨ ì‘ê¸‰ ê°ì§€: {results['statistics']['emergency_detection']['mean']:.2f}%")
        print(f"ğŸ” ì¦ìƒ ê°ì§€: {results['statistics']['symptom_detection']['mean']:.2f}%")
        print(f"ğŸ’¬ ì‘ë‹µ í’ˆì§ˆ: {results['statistics']['response_quality']['mean']:.2f}%")
        print(f"â±ï¸ í‰ê·  ì‘ë‹µ ì‹œê°„: {results['statistics']['response_time']['mean']:.2f}ì´ˆ")
        
        print(f"\nğŸ” BERTScore ê²°ê³¼")
        print(f"ğŸ“Š Precision: {bertscore_results['bertscore_precision']:.4f}")
        print(f"ğŸ“Š Recall: {bertscore_results['bertscore_recall']:.4f}")
        print(f"ğŸ“Š F1-Score: {bertscore_results['bertscore_f1']:.4f}")
        
        # ê²°ê³¼ ì €ì¥
        output_path = "evaluation_results_advanced.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥: {output_path}")
        
        return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ê³ ê¸‰ ì˜ë£Œ ëª¨ë¸ í‰ê°€ ì‹œì‘")
    print("=" * 50)
    
    # í‰ê°€ì ì´ˆê¸°í™”
    evaluator = AdvancedMedicalEvaluator()
    
    # í‰ê°€ ì‹¤í–‰
    results = evaluator.evaluate(sample_size=500)
    
    print("\nğŸ‰ ê³ ê¸‰ ì˜ë£Œ ëª¨ë¸ í‰ê°€ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
