#!/usr/bin/env python3
"""
í™˜ê²½ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ì™€ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
"""

import torch
import psutil
import platform

def check_environment():
    """ì‹œìŠ¤í…œ í™˜ê²½ì„ í™•ì¸í•©ë‹ˆë‹¤."""
    print("ğŸ–¥ï¸ ì‹œìŠ¤í…œ í™˜ê²½ í™•ì¸")
    print("=" * 50)
    
    # ê¸°ë³¸ ì‹œìŠ¤í…œ ì •ë³´
    print(f"ìš´ì˜ì²´ì œ: {platform.system()} {platform.release()}")
    print(f"Python ë²„ì „: {platform.python_version()}")
    print(f"PyTorch ë²„ì „: {torch.__version__}")
    
    # CPU ì •ë³´
    cpu_count = psutil.cpu_count()
    print(f"CPU ì½”ì–´ ìˆ˜: {cpu_count}")
    
    # ë©”ëª¨ë¦¬ ì •ë³´
    memory = psutil.virtual_memory()
    print(f"ì´ RAM: {memory.total / 1024**3:.1f}GB")
    print(f"ì‚¬ìš© ê°€ëŠ¥ RAM: {memory.available / 1024**3:.1f}GB")
    print(f"RAM ì‚¬ìš©ë¥ : {memory.percent}%")
    
    # GPU ì •ë³´
    print("\nğŸ® GPU ì •ë³´:")
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {gpu_count}ê°œ")
        
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
        
        # í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        current_memory = torch.cuda.memory_allocated() / 1024**3
        print(f"í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {current_memory:.2f}GB")
        
    else:
        print("âŒ GPU ì‚¬ìš© ë¶ˆê°€ (CPU ëª¨ë“œë§Œ ê°€ëŠ¥)")
    
    # ëª¨ë¸ í¬ê¸°ë³„ ê¶Œì¥ì‚¬í•­
    print("\nğŸ“‹ ëª¨ë¸ í¬ê¸°ë³„ ê¶Œì¥ì‚¬í•­:")
    print("=" * 50)
    
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        if gpu_memory >= 16:
            print("âœ… 16GB+ GPU: KoLlama2-7b (13.5GB) ì‹¤í–‰ ê°€ëŠ¥")
        elif gpu_memory >= 8:
            print("âš ï¸ 8GB GPU: KoLlama2-7bëŠ” ë©”ëª¨ë¦¬ ë¶€ì¡± ê°€ëŠ¥ì„±")
            print("   ê¶Œì¥: ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©")
        else:
            print("âŒ 8GB ë¯¸ë§Œ GPU: KoLlama2-7b ì‹¤í–‰ ë¶ˆê°€")
            print("   ê¶Œì¥: BERT ê³„ì—´ ëª¨ë¸ ì‚¬ìš©")
    else:
        print("âŒ CPU ëª¨ë“œ: KoLlama2-7b ì‹¤í–‰ ë¶ˆê°€ (ë„ˆë¬´ ëŠë¦¼)")
        print("   ê¶Œì¥: BERT ê³„ì—´ ëª¨ë¸ ì‚¬ìš©")
    
    # ì¶”ì²œ ëª¨ë¸
    print("\nğŸ¯ ì¶”ì²œ ëª¨ë¸:")
    print("=" * 50)
    
    if torch.cuda.is_available() and gpu_memory >= 16:
        print("1. psymon/KoLlama2-7b (í•œêµ­ì–´ LLM, 13.5GB)")
        print("2. beomi/KcBERT-base (í•œêµ­ì–´ BERT, 1.1GB)")
    else:
        print("1. beomi/KcBERT-base (í•œêµ­ì–´ BERT, 1.1GB)")
        print("2. klue/roberta-base (KLUE RoBERTa, 1.1GB)")
        print("3. beomi/KoAlpaca-Polyglot-12.8B-v1.1 (ë” í° ëª¨ë¸, GPU í•„ìš”)")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    check_environment()

if __name__ == "__main__":
    main()
