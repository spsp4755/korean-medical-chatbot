#!/usr/bin/env python3
"""
ì§„ì§œ LLM ì±—ë´‡ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
- test ë°ì´í„°ë§Œ ì‚¬ìš© (data leakage ë°©ì§€)
- ì‹¤ì œ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
- ì •ëŸ‰ì /ì •ì„±ì  í‰ê°€
"""

import os
import json
import time
import random
from datetime import datetime
from typing import Dict, List, Tuple, Any
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from bert_score import score
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["TOKENIZERS_PARALLELISM"] = "false"

class TrueLLMEvaluator:
    def __init__(self, model_path: str = "models/true_llm_chatbot"):
        """ì§„ì§œ LLM ì±—ë´‡ í‰ê°€ì ì´ˆê¸°í™”"""
        self.model_path = model_path
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # BERT ëª¨ë¸ ì„¤ì •
        self.bert_model = "bert-base-multilingual-cased"
        print(f"ğŸ§  BERT ëª¨ë¸: {self.bert_model}")
        
        # ëª¨ë¸ ë¡œë”©
        self._load_model()
        
    def _load_model(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©"""
        print(f"\nğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_path}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None
            )
            
            if self.device == "cpu":
                self.model = self.model.to(self.device)
            
            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            print("ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self._load_base_model()
    
    def _load_base_model(self):
        """ê¸°ë³¸ ëª¨ë¸ ë¡œë”©"""
        print(f"\nğŸ“¥ ê¸°ë³¸ ëª¨ë¸ ë¡œë”© ì¤‘: skt/kogpt2-base-v2")
        
        self.tokenizer = AutoTokenizer.from_pretrained("skt/kogpt2-base-v2")
        self.model = AutoModelForCausalLM.from_pretrained(
            "skt/kogpt2-base-v2",
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto" if self.device == "cuda" else None
        )
        
        if self.device == "cpu":
            self.model = self.model.to(self.device)
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        print("âœ… ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def generate_response(self, question: str) -> str:
        """ëª¨ë¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        try:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = f"ì‚¬ìš©ì: {question}\nì˜ë£Œì§„:"
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
            
            # ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=inputs.shape[1] + 200,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    attention_mask=torch.ones_like(inputs),
                    repetition_penalty=1.2,
                    no_repeat_ngram_size=3
                )
            
            # ë””ì½”ë”©
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = response.replace(prompt, "").strip()
            
            # ì‘ë‹µ ì •ë¦¬
            if "ì˜ë£Œì§„:" in response:
                response = response.split("ì˜ë£Œì§„:")[-1].strip()
            
            # ë°˜ë³µ ì œê±°
            lines = response.split('\n')
            cleaned_lines = []
            for line in lines:
                line = line.strip()
                if line and not any(word in line for word in ["ì‚¬ìš©ì:", "ì˜ë£Œì§„:", "ê°„í˜¸ì‚¬:", "ì§„ë£Œ:"]):
                    cleaned_lines.append(line)
            
            if cleaned_lines:
                return cleaned_lines[0]
            else:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                
        except Exception as e:
            print(f"âš ï¸ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def load_test_data(self, num_samples: int = 200) -> List[Dict]:
        """test ë°ì´í„°ë§Œ ë¡œë”© (data leakage ë°©ì§€)"""
        print(f"\nğŸ“Š Test ë°ì´í„°ë§Œ ë¡œë”© ì¤‘...")
        
        # ì˜¤ì§ test íŒŒì¼ë§Œ ì‚¬ìš©
        test_files = [
            "data/processed/splits/professional_medical_test.json"
        ]
        
        all_data = []
        for file_path in test_files:
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if isinstance(data, list):
                            all_data.extend(data)
                        else:
                            all_data.append(data)
                    print(f"âœ… {file_path}: {len(data) if isinstance(data, list) else 1}ê°œ ìƒ˜í”Œ")
                except Exception as e:
                    print(f"âš ï¸ {file_path} ë¡œë”© ì‹¤íŒ¨: {e}")
            else:
                print(f"âš ï¸ {file_path} íŒŒì¼ ì—†ìŒ")
        
        if not all_data:
            print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # ìƒ˜í”Œë§
        if len(all_data) > num_samples:
            all_data = random.sample(all_data, num_samples)
        
        print(f"âœ… Test ë°ì´í„°ë§Œ ë¡œë“œ ì™„ë£Œ: {len(all_data)}ê°œ ìƒ˜í”Œ")
        print(f"ğŸ”’ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€: train/validation ë°ì´í„° ì œì™¸")
        return all_data
    
    def evaluate_medical_professionalism(self, response: str) -> float:
        """ì˜ë£Œ ì „ë¬¸ì„± í‰ê°€"""
        medical_terms = [
            "ì˜ë£Œ", "ë³‘ì›", "ì˜ì‚¬", "ì§„ë£Œ", "ìƒë‹´", "ì¦ìƒ", "ì¹˜ë£Œ", "ì•½ë¬¼", 
            "ë³µìš©", "ì‘ê¸‰", "ê¸´ê¸‰", "ì§„ë‹¨", "ì§ˆë³‘", "í™˜ì", "ê±´ê°•"
        ]
        
        response_lower = response.lower()
        medical_count = sum(1 for term in medical_terms if term in response_lower)
        return min(medical_count / 5.0, 1.0)
    
    def evaluate_response_quality(self, response: str) -> float:
        """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
        if not response or len(response) < 5:
            return 0.0
        
        # ê¸¸ì´ ì ìˆ˜ (10-300ì ì‚¬ì´ê°€ ì ì ˆ)
        length_score = 1.0 if 10 <= len(response) <= 300 else 0.5
        
        # ì˜ë£Œ ê´€ë ¨ì„± ì ìˆ˜
        medical_keywords = ["ì˜ë£Œ", "ë³‘ì›", "ì¦ìƒ", "ì¹˜ë£Œ", "ì•½ë¬¼", "ì˜ì‚¬"]
        medical_score = sum(1 for keyword in medical_keywords if keyword in response) / len(medical_keywords)
        
        # ë°˜ë³µì„± ì ìˆ˜ (ë°˜ë³µì´ ì ì„ìˆ˜ë¡ ì¢‹ìŒ)
        words = response.split()
        unique_words = len(set(words))
        repetition_score = unique_words / len(words) if words else 0
        
        return (length_score + medical_score + repetition_score) / 3
    
    def evaluate(self, num_samples: int = 200) -> Dict[str, Any]:
        """ì§„ì§œ LLM ì±—ë´‡ í‰ê°€"""
        print(f"\nğŸ” ì§„ì§œ LLM ì±—ë´‡ í‰ê°€ ì‹œì‘ ({num_samples}ê°œ ìƒ˜í”Œ)")
        print("=" * 60)
        print("ğŸ”’ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€: ì˜¤ì§ test ë°ì´í„°ë§Œ ì‚¬ìš©")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©
        test_data = self.load_test_data(num_samples)
        if not test_data:
            return {}
        
        # í‰ê°€ ë³€ìˆ˜
        total_samples = len(test_data)
        total_response_time = 0
        all_responses = []
        all_references = []
        
        print(f"\nğŸ¤– LLM ì‘ë‹µ ìƒì„± ì¤‘: ", end="")
        
        # ê° ìƒ˜í”Œì— ëŒ€í•´ í‰ê°€
        for i, sample in enumerate(tqdm(test_data, desc="LLM ì‘ë‹µ ìƒì„±")):
            question = sample.get("question", "")
            reference = sample.get("answer", "")
            
            if not question or not reference:
                continue
            
            # LLM ì‘ë‹µ ìƒì„±
            start_time = time.time()
            response = self.generate_response(question)
            response_time = time.time() - start_time
            
            # ì‘ë‹µ ìˆ˜ì§‘
            all_responses.append(response)
            all_references.append(reference)
            total_response_time += response_time
        
        # BERTScore ê³„ì‚°
        print(f"\nğŸ§  BERTScore ê³„ì‚° ì¤‘...")
        try:
            P, R, F1 = score(all_responses, all_references, 
                           model_type=self.bert_model, 
                           idf=True, 
                           verbose=False)
            
            precision = P.mean().item()
            recall = R.mean().item()
            f1_score = F1.mean().item()
        except Exception as e:
            print(f"âš ï¸ BERTScore ê³„ì‚° ì˜¤ë¥˜: {e}")
            precision = recall = f1_score = 0.0
        
        # ì˜ë£Œ ì „ë¬¸ì„± ë° ì‘ë‹µ í’ˆì§ˆ í‰ê°€
        medical_scores = []
        quality_scores = []
        
        for response in all_responses:
            medical_scores.append(self.evaluate_medical_professionalism(response))
            quality_scores.append(self.evaluate_response_quality(response))
        
        avg_medical_professionalism = sum(medical_scores) / len(medical_scores) if medical_scores else 0
        avg_response_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        
        # ê²°ê³¼ ê³„ì‚°
        avg_response_time = total_response_time / total_samples if total_samples > 0 else 0
        
        # ê²°ê³¼ ì •ë¦¬
        results = {
            "evaluation_type": "ì§„ì§œ LLM ì±—ë´‡ í‰ê°€ (data leakage ë°©ì§€)",
            "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "data_source": "ì˜¤ì§ test ë°ì´í„°ë§Œ ì‚¬ìš©",
            "total_samples": total_samples,
            "average_response_time": avg_response_time,
            "bertscore_metrics": {
                "precision": precision,
                "recall": recall,
                "f1_score": f1_score
            },
            "quality_metrics": {
                "medical_professionalism": avg_medical_professionalism,
                "response_quality": avg_response_quality
            },
            "goals_achieved": {
                "bertscore_f1_0_7": f1_score >= 0.7,
                "response_time_5_seconds": avg_response_time <= 5.0,
                "medical_professionalism_0_6": avg_medical_professionalism >= 0.6
            }
        }
        
        return results
    
    def print_results(self, results: Dict[str, Any]):
        """ê²°ê³¼ ì¶œë ¥"""
        print(f"\nğŸ¯ ì§„ì§œ LLM ì±—ë´‡ í‰ê°€ ê²°ê³¼")
        print("=" * 60)
        print(f"ğŸ”’ ë°ì´í„° ì†ŒìŠ¤: {results['data_source']}")
        print(f"ğŸ“Š ì´ ìƒ˜í”Œ: {results['total_samples']}ê°œ")
        print(f"â±ï¸ í‰ê·  ì‘ë‹µ ì‹œê°„: {results['average_response_time']:.2f}ì´ˆ")
        
        print(f"\nğŸ§  BERTScore ë©”íŠ¸ë¦­:")
        print(f"   Precision: {results['bertscore_metrics']['precision']:.4f}")
        print(f"   Recall: {results['bertscore_metrics']['recall']:.4f}")
        print(f"   F1-Score: {results['bertscore_metrics']['f1_score']:.4f}")
        
        print(f"\nğŸ“Š í’ˆì§ˆ ë©”íŠ¸ë¦­:")
        print(f"   ì˜ë£Œ ì „ë¬¸ì„±: {results['quality_metrics']['medical_professionalism']:.2%}")
        print(f"   ì‘ë‹µ í’ˆì§ˆ: {results['quality_metrics']['response_quality']:.2%}")
        
        print(f"\nğŸ¯ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€:")
        goals = results['goals_achieved']
        print(f"   BERTScore F1 0.7 ëª©í‘œ: {'âœ… ë‹¬ì„±' if goals['bertscore_f1_0_7'] else 'âŒ ë¯¸ë‹¬ì„±'}")
        print(f"   ì‘ë‹µ ì‹œê°„ 5ì´ˆ ëª©í‘œ: {'âœ… ë‹¬ì„±' if goals['response_time_5_seconds'] else 'âŒ ë¯¸ë‹¬ì„±'}")
        print(f"   ì˜ë£Œ ì „ë¬¸ì„± 60% ëª©í‘œ: {'âœ… ë‹¬ì„±' if goals['medical_professionalism_0_6'] else 'âŒ ë¯¸ë‹¬ì„±'}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì§„ì§œ LLM ì±—ë´‡ í‰ê°€")
    print("=" * 60)
    print("ğŸ”’ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€: ì˜¤ì§ test ë°ì´í„°ë§Œ ì‚¬ìš©")
    
    # í‰ê°€ì ì´ˆê¸°í™”
    evaluator = TrueLLMEvaluator()
    
    # í‰ê°€ ì˜µì…˜ ì„ íƒ
    print(f"\nğŸ“Š í‰ê°€ ì˜µì…˜:")
    print(f"   1. ë¹ ë¥¸ í‰ê°€: 100ê°œ ìƒ˜í”Œ")
    print(f"   2. í‘œì¤€ í‰ê°€: 200ê°œ ìƒ˜í”Œ")
    print(f"   3. ì „ì²´ í‰ê°€: ëª¨ë“  test ìƒ˜í”Œ")
    
    choice = input("\nì„ íƒí•˜ì„¸ìš” (1-3): ").strip()
    
    if choice == "1":
        num_samples = 100
    elif choice == "2":
        num_samples = 200
    elif choice == "3":
        num_samples = 10000  # ì¶©ë¶„íˆ í° ìˆ˜
    else:
        print("ê¸°ë³¸ê°’ 200ê°œ ìƒ˜í”Œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        num_samples = 200
    
    # í‰ê°€ ì‹¤í–‰
    results = evaluator.evaluate(num_samples)
    
    if results:
        # ê²°ê³¼ ì¶œë ¥
        evaluator.print_results(results)
        
        # ê²°ê³¼ ì €ì¥
        os.makedirs("models", exist_ok=True)
        result_file = f"models/true_llm_evaluation_results_{results['timestamp']}.json"
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {result_file}")
        print(f"\nâœ… ì§„ì§œ LLM ì±—ë´‡ í‰ê°€ ì™„ë£Œ!")
    else:
        print("âŒ í‰ê°€ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()
