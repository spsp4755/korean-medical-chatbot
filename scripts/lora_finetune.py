#!/usr/bin/env python3
"""
LoRA νμΈνλ‹ μ‹¤ν–‰ μ¤ν¬λ¦½νΈ
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.optimization.lora import LoRAFineTuner
import yaml


def main():
    """LoRA νμΈνλ‹ μ‹¤ν— μ‹¤ν–‰"""
    print("π€ LoRA νμΈνλ‹ μ‹¤ν— μ‹μ‘")
    print("=" * 60)
    
    # μ„¤μ • λ΅λ“
    with open('configs/config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # λ² μ΄μ¤ λ¨λΈ: μ›λ³Έ Student λ¨λΈ μ‚¬μ© (ν† ν¬λ‚μ΄μ € νΈν™μ„± λ¬Έμ  ν•΄κ²°)
    base_model = "skt/kogpt2-base-v2"
    
    print(f"π“ λ² μ΄μ¤ λ¨λΈ: {base_model}")
    print(f"π― λ©ν‘: LoRA νμΈνλ‹μΌλ΅ μ†λ„ ν–¥μƒ 100% λ‹¬μ„±")
    
    # LoRA FineTuner μ΄κΈ°ν™”
    lora_tuner = LoRAFineTuner(
        model_name=base_model,
        output_dir="models/lora"
    )
    
    # LoRA νμΈνλ‹ μ‹¤ν–‰
    results = lora_tuner.run_lora_finetuning(
        r=16,                    # rank
        lora_alpha=32,          # LoRA alpha
        lora_dropout=0.1,       # LoRA dropout
        num_epochs=3,           # ν•™μµ μ—ν¬ν¬
        batch_size=2,           # λ°°μΉ ν¬κΈ°
        learning_rate=2e-4      # ν•™μµλ¥ 
    )
    
    # κ²°κ³Ό λ¶„μ„
    lora_tuner.analyze_results(results)
    
    print("\nβ… LoRA νμΈνλ‹ μ‹¤ν— μ™„λ£!")


if __name__ == "__main__":
    main()
