#!/usr/bin/env python3
"""
ë¹ ë¥¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ëª¨ë¸ ë¡œë“œ ë° ê¸°ë³¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import time

def test_model_loading(model_name: str) -> dict:
    """ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print(f"ğŸ”„ {model_name} ë¡œë”© í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    try:
        start_time = time.time()
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ëª¨ë¸ ë¡œë“œ
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto" if torch.cuda.is_available() else "cpu"
        )
        
        load_time = time.time() - start_time
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1024**3  # GB
        else:
            memory_used = 0
        
        return {
            "success": True,
            "load_time": load_time,
            "memory_used_gb": memory_used,
            "device": str(next(model.parameters()).device)
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

def test_simple_inference(model_name: str) -> dict:
    """ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    print(f"ğŸ§  {model_name} ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto" if torch.cuda.is_available() else "cpu"
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        test_prompts = [
            "ì•ˆë…•í•˜ì„¸ìš”.",
            "ì˜ë£Œ ìƒë‹´ì„ ë°›ê³  ì‹¶ìŠµë‹ˆë‹¤.",
            "ê°ê¸° ì¦ìƒì´ ìˆìŠµë‹ˆë‹¤."
        ]
        
        results = []
        
        for prompt in test_prompts:
            inputs = tokenizer(prompt, return_tensors="pt")
            
            # polyglot-ko ëª¨ë¸ì˜ ê²½ìš° token_type_ids ì œê±°
            if "polyglot" in model_name.lower():
                inputs.pop("token_type_ids", None)
            
            start_time = time.time()
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=30,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            inference_time = time.time() - start_time
            
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = generated_text[len(prompt):].strip()
            
            results.append({
                "prompt": prompt,
                "response": response,
                "inference_time": inference_time
            })
        
        return {
            "success": True,
            "results": results
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ë¹ ë¥¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ë“¤ (ë² ì´ìŠ¤ë¼ì¸ í›„ë³´ ëª¨ë¸ë“¤)
    models = [
        "skt/kogpt2-base-v2",           # GPT-2 ê¸°ë°˜ (124M, ~500MB) âœ…
        "42dot/42dot_LLM-SFT-1.3B"      # SFT ì ìš© (1.3B, ~2.6GB) â­
    ]
    
    for model_name in models:
        print(f"\n{'='*50}")
        print(f"ğŸ“Š {model_name} í…ŒìŠ¤íŠ¸")
        print(f"{'='*50}")
        
        # 1. ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
        load_result = test_model_loading(model_name)
        
        if load_result["success"]:
            print(f"âœ… ë¡œë”© ì„±ê³µ: {load_result['load_time']:.2f}ì´ˆ")
            print(f"âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {load_result['memory_used_gb']:.2f}GB")
            print(f"âœ… ë””ë°”ì´ìŠ¤: {load_result['device']}")
            
            # 2. ì¶”ë¡  í…ŒìŠ¤íŠ¸
            inference_result = test_simple_inference(model_name)
            
            if inference_result["success"]:
                print(f"âœ… ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                for result in inference_result["results"]:
                    print(f"  Q: {result['prompt']}")
                    print(f"  A: {result['response']}")
                    print(f"  ì‹œê°„: {result['inference_time']:.2f}ì´ˆ")
                    print()
            else:
                print(f"âŒ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {inference_result['error']}")
        else:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {load_result['error']}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

if __name__ == "__main__":
    main()
