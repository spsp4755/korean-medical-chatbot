#!/usr/bin/env python3
"""
ì§„ì§œ LLM ì±—ë´‡ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- train/valid/test ë°ì´í„° ë¶„ë¦¬ ì‚¬ìš©
- data leakage ë°©ì§€
- ì‹¤ì œ ëŒ€í™”í˜• ì±—ë´‡ í•™ìŠµ
"""

import os
import json
import torch
from torch.utils.data import Dataset as TorchDataset
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
# from datasets import Dataset  # datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´ë„ ì‘ë™
import warnings
warnings.filterwarnings("ignore")

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["WANDB_DISABLED"] = "true"  # wandb ì™„ì „ ë¹„í™œì„±í™”

class SimpleDataset(TorchDataset):
    """ê°„ë‹¨í•œ PyTorch Dataset"""
    def __init__(self, data):
        self.data = data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]

class TrueLLMTrainer:
    def __init__(self, model_name="skt/kogpt2-base-v2"):
        """ì§„ì§œ LLM ì±—ë´‡ í•™ìŠµì ì´ˆê¸°í™”"""
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©
        self._load_model()
        
    def _load_model(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©"""
        print(f"\nğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto" if self.device == "cuda" else None
        )
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        if self.device == "cpu":
            self.model = self.model.to(self.device)
            
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def load_data(self, split="train"):
        """ë°ì´í„° ë¡œë”© (data leakage ë°©ì§€)"""
        print(f"\nğŸ“Š {split} ë°ì´í„° ë¡œë”© ì¤‘...")
        
        # ì˜¤ì§ í•´ë‹¹ splitë§Œ ë¡œë”©
        data_files = {
            "train": "data/processed/splits/professional_medical_train.json",
            "validation": "data/processed/splits/professional_medical_val.json",
            "test": "data/processed/splits/professional_medical_test.json"
        }
        
        file_path = data_files[split]
        if not os.path.exists(file_path):
            print(f"âŒ {file_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"âœ… {split} ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data)}ê°œ ìƒ˜í”Œ")
        return data
    
    def create_conversation_format(self, data):
        """ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜"""
        print(f"\nğŸ”„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜ ì¤‘...")
        
        formatted_data = []
        for item in data:
            question = item.get("question", "").strip()
            answer = item.get("answer", "").strip()
            
            if question and answer:
                # ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                conversation = f"ì‚¬ìš©ì: {question}\nì˜ë£Œì§„: {answer}"
                formatted_data.append({"text": conversation})
        
        print(f"âœ… ë³€í™˜ ì™„ë£Œ: {len(formatted_data)}ê°œ ëŒ€í™”")
        return formatted_data
    
    def tokenize_function(self, examples):
        """í† í¬ë‚˜ì´ì§• í•¨ìˆ˜"""
        # ë°°ì¹˜ ì²˜ë¦¬ìš©ìœ¼ë¡œ ìˆ˜ì •
        if isinstance(examples["text"], list):
            # ë°°ì¹˜ ì²˜ë¦¬
            tokenized = self.tokenizer(
                examples["text"],
                truncation=True,
                padding=True,
                max_length=512,
                return_tensors="pt"
            )
        else:
            # ë‹¨ì¼ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            tokenized = self.tokenizer(
                [examples["text"]],
                truncation=True,
                padding=True,
                max_length=512,
                return_tensors="pt"
            )
        
        # ë¼ë²¨ ì„¤ì • (ì…ë ¥ê³¼ ë™ì¼)
        tokenized["labels"] = tokenized["input_ids"].clone()
        
        return tokenized
    
    def prepare_dataset(self, data):
        """ë°ì´í„°ì…‹ ì¤€ë¹„"""
        print(f"\nğŸ“‹ ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")
        
        # ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        formatted_data = self.create_conversation_format(data)
        
        # ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì§• (ë°°ì¹˜ ì²˜ë¦¬ ì—†ì´)
        tokenized_data = []
        for item in formatted_data:
            # ê°œë³„ í† í¬ë‚˜ì´ì§•
            tokens = self.tokenizer(
                item["text"],
                truncation=True,
                padding="max_length",
                max_length=512,
                return_tensors="pt"
            )
            
            tokenized_data.append({
                "input_ids": tokens["input_ids"].squeeze(0),  # ë°°ì¹˜ ì°¨ì› ì œê±°
                "attention_mask": tokens["attention_mask"].squeeze(0),
                "labels": tokens["input_ids"].squeeze(0)  # ë¼ë²¨ì€ ì…ë ¥ê³¼ ë™ì¼
            })
        
        print(f"âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: {len(tokenized_data)}ê°œ ìƒ˜í”Œ")
        return tokenized_data
    
    def train(self, train_data, validation_data, output_dir="models/true_llm_chatbot"):
        """ëª¨ë¸ í•™ìŠµ"""
        print(f"\nğŸš€ ì§„ì§œ LLM ì±—ë´‡ í•™ìŠµ ì‹œì‘")
        print("=" * 50)
        
        # ë°ì´í„°ì…‹ ì¤€ë¹„
        train_data_processed = self.prepare_dataset(train_data)
        val_data_processed = self.prepare_dataset(validation_data)
        
        # PyTorch Datasetìœ¼ë¡œ ë³€í™˜
        train_dataset = SimpleDataset(train_data_processed)
        val_dataset = SimpleDataset(val_data_processed)
        
        # ë°ì´í„° ì½œë ˆì´í„° (íŒ¨ë”© ë¬¸ì œ í•´ê²°)
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,  # Causal LMì´ë¯€ë¡œ MLM ë¹„í™œì„±í™”
            pad_to_multiple_of=8  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ 8ì˜ ë°°ìˆ˜ë¡œ íŒ¨ë”©
        )
        
        # í•™ìŠµ ì¸ìˆ˜ ì„¤ì •
        training_args = TrainingArguments(
            output_dir=output_dir,
            overwrite_output_dir=True,
            num_train_epochs=3,
            per_device_train_batch_size=2,  # KoGPT2ëŠ” ì‘ì€ ëª¨ë¸ì´ë¯€ë¡œ 2ë¡œ ì¦ê°€
            per_device_eval_batch_size=2,   # KoGPT2ëŠ” ì‘ì€ ëª¨ë¸ì´ë¯€ë¡œ 2ë¡œ ì¦ê°€
            warmup_steps=50,                # warmup ë‹¨ê³„ ë³µì›
            weight_decay=0.01,
            logging_dir=f"{output_dir}/logs",
            logging_steps=10,
            eval_strategy="steps",  # evaluation_strategy -> eval_strategy
            eval_steps=100,
            save_steps=100,
            save_total_limit=3,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            report_to=[],  # wandb ì™„ì „ ë¹„í™œì„±í™”
            dataloader_pin_memory=False,
            remove_unused_columns=False,
            gradient_accumulation_steps=4,  # KoGPT2ëŠ” ì‘ì€ ëª¨ë¸ì´ë¯€ë¡œ 4ë¡œ ê°ì†Œ
            fp16=False,                     # MPSì—ì„œëŠ” FP16 ì§€ì› ì•ˆë¨
            max_steps=500                   # í•™ìŠµ ë‹¨ê³„ ê°ì†Œ (ë©”ëª¨ë¦¬ ì ˆì•½)
        )
        
        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer
        )
        
        # í•™ìŠµ ì‹œì‘
        print(f"\nğŸ“š í•™ìŠµ ì‹œì‘...")
        trainer.train()
        
        # ëª¨ë¸ ì €ì¥
        print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        
        print(f"âœ… í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {output_dir}")
        return trainer
    
    def evaluate(self, test_data):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€"""
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€ ì¤‘...")
        
        test_dataset = self.prepare_dataset(test_data)
        
        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False
        )
        
        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        trainer = Trainer(
            model=self.model,
            data_collator=data_collator,
            tokenizer=self.tokenizer
        )
        
        # í‰ê°€ ì‹¤í–‰
        eval_results = trainer.evaluate(test_dataset)
        
        print(f"ğŸ“ˆ í‰ê°€ ê²°ê³¼:")
        for key, value in eval_results.items():
            print(f"   {key}: {value:.4f}")
        
        return eval_results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì§„ì§œ LLM ì±—ë´‡ í•™ìŠµ ì‹œì‘")
    print("=" * 50)
    
    # í•™ìŠµì ì´ˆê¸°í™”
    trainer = TrueLLMTrainer()
    
    # ë°ì´í„° ë¡œë”© (data leakage ë°©ì§€)
    print(f"\nğŸ“Š ë°ì´í„° ë¡œë”© (data leakage ë°©ì§€)")
    train_data = trainer.load_data("train")
    val_data = trainer.load_data("validation")
    test_data = trainer.load_data("test")
    
    if not train_data or not val_data:
        print("âŒ í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        return
    
    # í•™ìŠµ ì‹¤í–‰
    trainer.train(train_data, val_data)
    
    # í…ŒìŠ¤íŠ¸ í‰ê°€
    if test_data:
        trainer.evaluate(test_data)
    
    print(f"\nâœ… ì§„ì§œ LLM ì±—ë´‡ í•™ìŠµ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
