import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import time
import json
from datetime import datetime

class ImprovedPromptMedicalChatbot:
    def __init__(self, base_model_path="42dot/42dot_LLM-SFT-1.3B", 
                 peft_model_path="models/medical_finetuned_peft"):
        self.base_model_path = base_model_path
        self.peft_model_path = peft_model_path
        self.tokenizer = None
        self.model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        self._load_model()
        
        # ëŒ€í™” ê¸°ë¡ ì €ì¥
        self.conversation_log = []
        
    def _load_model(self):
        """PEFT ëª¨ë¸ ë¡œë“œ"""
        print("PEFT ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_path)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (CPU ëª¨ë“œë¡œ MPS ì˜¤ë¥˜ í•´ê²°)
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_path,
            torch_dtype=torch.float32,
            device_map="cpu",
            low_cpu_mem_usage=True
        )
        
        # PEFT ëª¨ë¸ ë¡œë“œ
        self.model = PeftModel.from_pretrained(base_model, self.peft_model_path)
        self.model.eval()
        
        print("PEFT ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def create_improved_prompt(self, question):
        """ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompt = f"""ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ì‹¤ìš©ì ì¸ ì˜ë£Œ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤. í™˜ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ë‹µë³€í•´ì£¼ì„¸ìš”:

1. ë¨¼ì € ì‘ê¸‰ìƒí™©ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”
2. ì¼ë°˜ì¸ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ë¡œ ì„¤ëª…í•˜ì„¸ìš”
3. ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì¹˜ë¥¼ ì œì‹œí•˜ì„¸ìš”
4. ë‹µë³€ì€ 200ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ í•˜ì„¸ìš”
5. í•„ìš”ì‹œ ì˜ì‚¬ ìƒë‹´ì„ ê¶Œí•˜ì„¸ìš”

í™˜ì ì§ˆë¬¸: {question}

ì˜ë£Œì§„ ë‹µë³€:"""
        return prompt
    
    def generate_response(self, question, max_length=200, temperature=0.3):
        """ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„± (ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)"""
        start_time = time.time()
        
        # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        prompt = self.create_improved_prompt(question)
        
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=inputs.input_ids.shape[1] + max_length,
                num_return_sequences=1,
                temperature=temperature,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.1,
                top_p=0.9,
                top_k=50
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.replace(prompt, "").strip()
        
        end_time = time.time()
        response_time = end_time - start_time
        
        # ëŒ€í™” ê¸°ë¡ ì €ì¥
        self.conversation_log.append({
            "timestamp": datetime.now().isoformat(),
            "question": question,
            "response": response,
            "response_time": response_time,
            "prompt_type": "improved"
        })
        
        return response, response_time
    
    def save_conversation_log(self, filename="results/improved_prompt_conversation.json"):
        """ëŒ€í™” ê¸°ë¡ ì €ì¥"""
        os.makedirs("results", exist_ok=True)
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(self.conversation_log, f, ensure_ascii=False, indent=2)
        print(f"ëŒ€í™” ê¸°ë¡ì´ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    chatbot = ImprovedPromptMedicalChatbot()
    
    print("\n" + "="*60)
    print("ğŸ¥ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì˜ë£Œ ìƒë‹´ ì±—ë´‡")
    print("ğŸ’¡ ë” ì¹œê·¼í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤!")
    print("ğŸ’¡ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. 'quit'ì„ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")
    print("="*60)
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_questions = [
        "ì¥ì—¼ì— ê±¸ë¦°ê±° ê°™ì€ë° ì–´ë–»ê²Œ í•´ì•¼í•˜ë‚˜ìš”?",
        "ë¨¸ë¦¬ê°€ ì•„í”ˆë° ì§„í†µì œ ë¨¹ì–´ë„ ë ê¹Œìš”?",
        "ì—´ì´ ë‚˜ëŠ”ë° ë³‘ì› ê°€ì•¼ í• ê¹Œìš”?",
        "ë³µí†µì´ ì‹¬í•œë° ì‘ê¸‰ì‹¤ ê°€ì•¼ í•˜ë‚˜ìš”?"
    ]
    
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤:")
    for i, q in enumerate(test_questions, 1):
        print(f"{i}. {q}")
    
    print("\n" + "="*60)
    
    while True:
        user_input = input("\nğŸ‘¤ í™˜ì: ")
        if user_input.lower() in ["quit", "exit", "ì¢…ë£Œ", "q"]:
            print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        response, response_time = chatbot.generate_response(user_input)
        print(f"ğŸ¤– ì˜ë£Œì§„: {response}")
        print(f"ğŸ“Š ì‘ë‹µ ì‹œê°„: {response_time:.3f}ì´ˆ")
    
    # ëŒ€í™” ê¸°ë¡ ì €ì¥
    chatbot.save_conversation_log()

if __name__ == "__main__":
    main()


