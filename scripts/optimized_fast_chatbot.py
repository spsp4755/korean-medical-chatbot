import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import time
import json
from datetime import datetime

class OptimizedFastChatbot:
    def __init__(self, base_model_path="42dot/42dot_LLM-SFT-1.3B", 
                 peft_model_path="models/medical_finetuned_peft"):
        self.base_model_path = base_model_path
        self.peft_model_path = peft_model_path
        self.tokenizer = None
        self.model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        self._load_model()
        
        # ëŒ€í™” ê¸°ë¡ ì €ì¥
        self.conversation_log = []
        
    def _load_model(self):
        """PEFT ëª¨ë¸ ë¡œë“œ (ìµœì í™”)"""
        print("PEFT ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_path)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ëª¨ë¸ ë¡œë“œ ìµœì í™”
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_path,
            torch_dtype=torch.float32,
            device_map="cpu",
            low_cpu_mem_usage=True,
            use_cache=True,  # ìºì‹œ ì‚¬ìš©ìœ¼ë¡œ ì†ë„ í–¥ìƒ
            use_safetensors=False  # SafeTensors ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
        )
        
        self.model = PeftModel.from_pretrained(base_model, self.peft_model_path)
        self.model.eval()
        
        # ëª¨ë¸ ìµœì í™”
        self.model = torch.compile(self.model, mode="reduce-overhead")  # PyTorch 2.0 ì»´íŒŒì¼
        
        print("PEFT ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def create_optimized_prompt(self, question):
        """ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ (ë” ê°„ê²°í•˜ê³  ëª…í™•)"""
        prompt = f"""ì˜ë£Œ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤. ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”:

1. ì‘ê¸‰ìƒí™©ì´ë©´ "ì¦‰ì‹œ ë³‘ì›" ì•ˆë‚´
2. ì¼ë°˜ì¸ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…
3. êµ¬ì²´ì ì¸ ì¡°ì¹˜ ì œì‹œ
4. 150ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ

ì§ˆë¬¸: {question}
ë‹µë³€:"""
        return prompt
    
    def generate_response(self, question, max_length=150, temperature=0.1):
        """ìµœì í™”ëœ ì‘ë‹µ ìƒì„±"""
        start_time = time.time()
        
        prompt = self.create_optimized_prompt(question)
        
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=256  # ì…ë ¥ ê¸¸ì´ ë‹¨ì¶•
        )
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=inputs.input_ids.shape[1] + max_length,
                num_return_sequences=1,
                temperature=temperature,  # ë” ë‚®ì€ temperatureë¡œ ì¼ê´€ì„± í–¥ìƒ
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.05,  # ë°˜ë³µ ê°ì†Œ
                top_p=0.8,  # ë” ì§‘ì¤‘ëœ ìƒì„±
                top_k=30,   # ë” ì œí•œëœ ì„ íƒ
                early_stopping=True,  # ì¡°ê¸° ì¢…ë£Œë¡œ ì†ë„ í–¥ìƒ
                use_cache=True  # ìºì‹œ ì‚¬ìš©
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.replace(prompt, "").strip()
        
        end_time = time.time()
        response_time = end_time - start_time
        
        # ëŒ€í™” ê¸°ë¡ ì €ì¥
        self.conversation_log.append({
            "timestamp": datetime.now().isoformat(),
            "question": question,
            "response": response,
            "response_time": response_time,
            "prompt_type": "optimized_fast"
        })
        
        return response, response_time
    
    def save_conversation_log(self, filename="results/optimized_fast_conversation.json"):
        """ëŒ€í™” ê¸°ë¡ ì €ì¥"""
        os.makedirs("results", exist_ok=True)
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(self.conversation_log, f, ensure_ascii=False, indent=2)
        print(f"ëŒ€í™” ê¸°ë¡ì´ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    chatbot = OptimizedFastChatbot()
    
    print("\n" + "="*60)
    print("ğŸš€ ìµœì í™”ëœ ê³ ì† ì˜ë£Œ ìƒë‹´ ì±—ë´‡")
    print("ğŸ’¡ ë¹ ë¥´ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤!")
    print("ğŸ’¡ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. 'quit'ì„ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")
    print("="*60)
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_questions = [
        "ì¥ì—¼ì— ê±¸ë¦°ê±° ê°™ì€ë° ì–´ë–»ê²Œ í•´ì•¼í•˜ë‚˜ìš”?",
        "ë¨¸ë¦¬ê°€ ì•„í”ˆë° ì§„í†µì œ ë¨¹ì–´ë„ ë ê¹Œìš”?",
        "ì—´ì´ ë‚˜ëŠ”ë° ë³‘ì› ê°€ì•¼ í• ê¹Œìš”?",
        "ë³µí†µì´ ì‹¬í•œë° ì‘ê¸‰ì‹¤ ê°€ì•¼ í•˜ë‚˜ìš”?",
        "ì €ì²´ì˜¨ì¦ì´ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?"  # ì´ì „ì— ì˜ëª» ë‹µë³€í•œ ì§ˆë¬¸
    ]
    
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤:")
    for i, q in enumerate(test_questions, 1):
        print(f"{i}. {q}")
    
    print("\n" + "="*60)
    
    while True:
        user_input = input("\nğŸ‘¤ í™˜ì: ")
        if user_input.lower() in ["quit", "exit", "ì¢…ë£Œ", "q"]:
            print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        response, response_time = chatbot.generate_response(user_input)
        print(f"ğŸ¤– ì˜ë£Œì§„: {response}")
        print(f"ğŸ“Š ì‘ë‹µ ì‹œê°„: {response_time:.3f}ì´ˆ")
    
    # ëŒ€í™” ê¸°ë¡ ì €ì¥
    chatbot.save_conversation_log()

if __name__ == "__main__":
    main()
