#!/usr/bin/env python3
"""
Knowledge Distillation ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.optimization.distillation import KnowledgeDistiller
import yaml


def main():
    """Knowledge Distillation ì‹¤í—˜ ì‹¤í–‰"""
    print("ğŸš€ Knowledge Distillation ì‹¤í—˜ ì‹œì‘")
    print("=" * 60)
    
    # ì„¤ì • ë¡œë“œ
    with open('configs/config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # Teacher ëª¨ë¸: 20% Pruned ëª¨ë¸ ì‚¬ìš©
    teacher_model = "models/pruned/model_20_percent_pruned"
    
    # Student ëª¨ë¸: ë” ì‘ì€ ëª¨ë¸ ì„ íƒ
    student_model = "skt/kogpt2-base-v2"  # 124M íŒŒë¼ë¯¸í„°
    
    print(f"ğŸ“Š Teacher ëª¨ë¸: {teacher_model}")
    print(f"ğŸ“Š Student ëª¨ë¸: {student_model}")
    print(f"ğŸ¯ ëª©í‘œ: Teacher ì§€ì‹ì„ Studentë¡œ ì „ë‹¬í•˜ì—¬ ê²½ëŸ‰í™”")
    
    # Distiller ì´ˆê¸°í™”
    distiller = KnowledgeDistiller(
        teacher_model_name=teacher_model,
        student_model_name=student_model,
        output_dir="models/distilled"
    )
    
    # Distillation ì‹¤í–‰
    results = distiller.run_distillation(
        temperature=3.0,      # Soft targetsì˜ ë¶€ë“œëŸ¬ì›€ ì¡°ì ˆ
        alpha=0.7,           # Distillation loss weight
        num_epochs=2,        # í•™ìŠµ ì—í¬í¬ (ì‹œê°„ ì ˆì•½)
        batch_size=2,        # ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ ì ˆì•½)
        learning_rate=5e-5   # í•™ìŠµë¥ 
    )
    
    # ê²°ê³¼ ë¶„ì„
    distiller.analyze_results(results)
    
    print("\nâœ… Knowledge Distillation ì‹¤í—˜ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
