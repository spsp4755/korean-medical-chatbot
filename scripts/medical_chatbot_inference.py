#!/usr/bin/env python3
"""
í•™ìŠµëœ ì˜ë£Œ ëª¨ë¸ì„ í™œìš©í•œ ì±—ë´‡
- Essential í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©
- ì‹¤ì‹œê°„ ëŒ€í™” ê°€ëŠ¥
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import warnings
warnings.filterwarnings("ignore")

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["TOKENIZERS_PARALLELISM"] = "false"

class MedicalChatbot:
    def __init__(self, model_path="models/medical_finetuned_advanced"):
        """ì˜ë£Œ ì±—ë´‡ ì´ˆê¸°í™”"""
        self.model_path = model_path
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ëª¨ë¸ ë¡œë”©
        self._load_model()
        
    def _load_model(self):
        """í•™ìŠµëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©"""
        print(f"\nğŸ“¥ í•™ìŠµëœ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_path}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                low_cpu_mem_usage=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            print("âœ… í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def generate_response(self, question, max_length=200):
        """ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        prompt = f"í™˜ì: {question}\nì˜ë£Œì§„:"
        
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.replace(prompt, "").strip()
        
        return response
    
    def chat(self):
        """ëŒ€í™”í˜• ì±—ë´‡ ì‹¤í–‰"""
        print("\nğŸ¥ ì˜ë£Œ ìƒë‹´ ì±—ë´‡ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ’¡ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. 'quit'ì„ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")
        print("=" * 50)
        
        while True:
            try:
                # ì‚¬ìš©ì ì…ë ¥
                question = input("\nğŸ‘¤ í™˜ì: ").strip()
                
                if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                    print("\nğŸ‘‹ ì˜ë£Œ ìƒë‹´ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                    break
                
                if not question:
                    continue
                
                # ì‘ë‹µ ìƒì„±
                print("ğŸ¤– ì˜ë£Œì§„: ", end="", flush=True)
                response = self.generate_response(question)
                print(response)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì˜ë£Œ ìƒë‹´ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì˜ë£Œ ì±—ë´‡ ì‹œì‘")
    print("=" * 50)
    
    # ì±—ë´‡ ì´ˆê¸°í™”
    chatbot = MedicalChatbot()
    
    # ëŒ€í™” ì‹œì‘
    chatbot.chat()

if __name__ == "__main__":
    main()


