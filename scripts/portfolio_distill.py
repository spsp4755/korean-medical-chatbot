#!/usr/bin/env python3
"""
í¬íŠ¸í´ë¦¬ì˜¤ìš© ì•ˆì „í•œ Knowledge Distillation
í† í¬ë‚˜ì´ì € í˜¸í™˜ì„± ë¬¸ì œë¥¼ í•´ê²°í•œ êµ¬í˜„
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
import json
from pathlib import Path
import time
import psutil
import numpy as np
import warnings
warnings.filterwarnings("ignore")

# MPS ë¹„í™œì„±í™”
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"


class PortfolioDistiller:
    """í¬íŠ¸í´ë¦¬ì˜¤ìš© ì•ˆì „í•œ Knowledge Distiller"""
    
    def __init__(self, teacher_model_name: str, student_model_name: str, output_dir: str = "models/distilled"):
        self.teacher_model_name = teacher_model_name
        self.student_model_name = student_model_name
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ë¡œë“œ
        self._load_models()
        
        # ì„±ëŠ¥ ì¸¡ì •
        self._measure_baseline_performance()
        
    def _load_models(self):
        """ëª¨ë¸ ë¡œë“œ"""
        print(f"ğŸ”„ Teacher ëª¨ë¸ ë¡œë”© ì¤‘: {self.teacher_model_name}")
        self.teacher_model = AutoModelForCausalLM.from_pretrained(
            self.teacher_model_name,
            torch_dtype=torch.float32,
            device_map="cpu"
        )
        self.teacher_tokenizer = AutoTokenizer.from_pretrained(self.teacher_model_name)
        
        print(f"ğŸ”„ Student ëª¨ë¸ ë¡œë”© ì¤‘: {self.student_model_name}")
        self.student_model = AutoModelForCausalLM.from_pretrained(
            self.student_model_name,
            torch_dtype=torch.float32,
            device_map="cpu"
        )
        self.student_tokenizer = AutoTokenizer.from_pretrained(self.student_model_name)
        
        # íŒ¨ë”© í† í° ì„¤ì •
        for tokenizer in [self.teacher_tokenizer, self.student_tokenizer]:
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
        
        # í† í¬ë‚˜ì´ì € í˜¸í™˜ì„± í™•ì¸
        print(f"ğŸ“Š Teacher vocab size: {len(self.teacher_tokenizer)}")
        print(f"ğŸ“Š Student vocab size: {len(self.student_tokenizer)}")
        print(f"ğŸ“Š Teacher model vocab: {self.teacher_model.config.vocab_size}")
        print(f"ğŸ“Š Student model vocab: {self.student_model.config.vocab_size}")
        
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    def _measure_baseline_performance(self):
        """ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •"""
        print("ğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        self.teacher_size = self._get_model_size(self.teacher_model)
        self.student_size = self._get_model_size(self.student_model)
        self.teacher_speed = self._benchmark_inference(self.teacher_model, self.teacher_tokenizer)
        self.student_speed = self._benchmark_inference(self.student_model, self.student_tokenizer)
        self.teacher_memory = self._get_memory_usage()
        self.student_memory = self._get_memory_usage()
        
        print(f"ğŸ“Š Teacher í¬ê¸°: {self.teacher_size:.2f}MB")
        print(f"ğŸ“Š Student í¬ê¸°: {self.student_size:.2f}MB")
        print(f"ğŸ“Š í¬ê¸° ê°ì†Œ: {((self.teacher_size - self.student_size) / self.teacher_size * 100):.1f}%")
        print(f"ğŸ§  Teacher ì†ë„: {self.teacher_speed:.2f}ì´ˆ")
        print(f"ğŸ§  Student ì†ë„: {self.student_speed:.2f}ì´ˆ")
        print(f"âš¡ ì†ë„ í–¥ìƒ: {((self.teacher_speed - self.student_speed) / self.teacher_speed * 100):.1f}%")
    
    def _get_model_size(self, model) -> float:
        """ëª¨ë¸ í¬ê¸° ì¸¡ì • (MB)"""
        param_size = 0
        buffer_size = 0
        
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()
        
        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()
        
        size_all_mb = (param_size + buffer_size) / 1024**2
        return size_all_mb
    
    def _benchmark_inference(self, model, tokenizer, num_runs: int = 3) -> float:
        """ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬ (ì´ˆ)"""
        model.eval()
        test_text = "ì•ˆë…•í•˜ì„¸ìš”. ì˜ë£Œ ìƒë‹´ì„ ë°›ê³  ì‹¶ìŠµë‹ˆë‹¤."
        
        inputs = tokenizer(test_text, return_tensors="pt", padding=True, truncation=True, max_length=128)
        
        # ì›Œë°ì—…
        with torch.no_grad():
            for _ in range(2):
                _ = model.generate(
                    inputs.input_ids,
                    max_length=inputs.input_ids.shape[1] + 20,
                    num_return_sequences=1,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id
                )
        
        # ì‹¤ì œ ì¸¡ì •
        start_time = time.time()
        with torch.no_grad():
            for _ in range(num_runs):
                _ = model.generate(
                    inputs.input_ids,
                    max_length=inputs.input_ids.shape[1] + 20,
                    num_return_sequences=1,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id
                )
        end_time = time.time()
        
        avg_time = (end_time - start_time) / num_runs
        return avg_time
    
    def _get_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (MB)"""
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / 1024**2
        else:
            process = psutil.Process(os.getpid())
            return process.memory_info().rss / 1024**2
    
    def _prepare_medical_data(self) -> list:
        """ì˜ë£Œ ë°ì´í„° ì¤€ë¹„"""
        print("ğŸ“š ì˜ë£Œ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        
        medical_data = []
        medical_files = [
            "data/processed/medical_data.json",
            "data/processed/splits/essential_medical_test.json",
            "data/processed/splits/professional_medical_test.json"
        ]
        
        for file_path in medical_files:
            if Path(file_path).exists():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if isinstance(data, list):
                            medical_data.extend(data)
                        else:
                            medical_data.append(data)
                    print(f"âœ… ë¡œë“œë¨: {file_path}")
                except Exception as e:
                    print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {file_path} - {e}")
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = []
        for item in medical_data[:200]:  # 200ê°œë¡œ ì œí•œ (ì•ˆì •ì„±)
            if isinstance(item, dict):
                for key in ['text', 'content', 'question', 'answer', 'instruction', 'input', 'output']:
                    if key in item and isinstance(item[key], str) and len(item[key]) > 10:
                        texts.append(item[key])
                        break
            elif isinstance(item, str) and len(item) > 10:
                texts.append(item)
        
        # ì¤‘ë³µ ì œê±°
        texts = list(set(texts))
        print(f"ğŸ“ ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸: {len(texts)}ê°œ")
        
        if len(texts) == 0:
            # ë”ë¯¸ ë°ì´í„° ìƒì„±
            texts = [
                "ì•ˆë…•í•˜ì„¸ìš”. ì˜ë£Œ ìƒë‹´ì„ ë°›ê³  ì‹¶ìŠµë‹ˆë‹¤.",
                "ë¨¸ë¦¬ê°€ ì•„í”ˆë° ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?",
                "ê°ê¸° ì¦ìƒì´ ìˆëŠ”ë° ë³‘ì›ì— ê°€ì•¼ í• ê¹Œìš”?",
                "ë³µí†µì´ ì‹¬í•œë° ì‘ê¸‰ì‹¤ì— ê°€ì•¼ í• ê¹Œìš”?",
                "í”¼ë¶€ì— ë°œì§„ì´ ìƒê²¼ëŠ”ë° ì›ì¸ì´ ë­˜ê¹Œìš”?"
            ] * 20
        
        return texts
    
    def run_simple_distillation(self, num_epochs: int = 3) -> dict:
        """ê°„ë‹¨í•œ Knowledge Distillation ì‹¤í–‰"""
        print("ğŸš€ ê°„ë‹¨í•œ Knowledge Distillation ì‹œì‘")
        print("=" * 60)
        
        # ë°ì´í„° ì¤€ë¹„
        texts = self._prepare_medical_data()
        
        # ê°„ë‹¨í•œ ì§€ì‹ ì „ë‹¬ í…ŒìŠ¤íŠ¸
        print("ğŸ“ ì§€ì‹ ì „ë‹¬ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        teacher_model = self.teacher_model
        student_model = self.student_model
        teacher_tokenizer = self.teacher_tokenizer
        student_tokenizer = self.student_tokenizer
        
        teacher_model.eval()
        student_model.eval()
        
        # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë“¤
        test_texts = texts[:10] if len(texts) >= 10 else texts
        
        quality_scores = []
        
        for i, text in enumerate(test_texts):
            print(f"\ní…ŒìŠ¤íŠ¸ {i+1}: {text[:50]}...")
            
            try:
                # Teacher ì¶œë ¥
                teacher_inputs = teacher_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
                with torch.no_grad():
                    teacher_output = teacher_model.generate(
                        teacher_inputs.input_ids,
                        max_length=teacher_inputs.input_ids.shape[1] + 30,
                        num_return_sequences=1,
                        do_sample=False,
                        pad_token_id=teacher_tokenizer.pad_token_id
                    )
                teacher_response = teacher_tokenizer.decode(teacher_output[0], skip_special_tokens=True)
                
                # Student ì¶œë ¥
                student_inputs = student_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
                with torch.no_grad():
                    student_output = student_model.generate(
                        student_inputs.input_ids,
                        max_length=student_inputs.input_ids.shape[1] + 30,
                        num_return_sequences=1,
                        do_sample=False,
                        pad_token_id=student_tokenizer.pad_token_id
                    )
                student_response = student_tokenizer.decode(student_output[0], skip_special_tokens=True)
                
                print(f"Teacher: {teacher_response[:100]}...")
                print(f"Student: {student_response[:100]}...")
                
                # ê°„ë‹¨í•œ í’ˆì§ˆ í‰ê°€ (ë‹¨ì–´ ìˆ˜ ê¸°ë°˜)
                teacher_words = set(teacher_response.split())
                student_words = set(student_response.split())
                
                if len(teacher_words) > 0:
                    similarity = len(teacher_words.intersection(student_words)) / len(teacher_words)
                    quality_scores.append(similarity)
                    print(f"í’ˆì§ˆ ì ìˆ˜: {similarity:.3f}")
                
            except Exception as e:
                print(f"âš ï¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                continue
        
        # ê²°ê³¼ ê³„ì‚°
        avg_quality = np.mean(quality_scores) if quality_scores else 0.0
        size_reduction = ((self.teacher_size - self.student_size) / self.teacher_size) * 100
        speed_improvement = ((self.teacher_speed - self.student_speed) / self.teacher_speed) * 100
        memory_reduction = ((self.teacher_memory - self.student_memory) / self.teacher_memory) * 100
        
        results = {
            "distillation_config": {
                "method": "simple_comparison",
                "num_epochs": num_epochs,
                "num_test_samples": len(test_texts)
            },
            "teacher_model": {
                "size_mb": self.teacher_size,
                "inference_speed_sec": self.teacher_speed,
                "memory_usage_mb": self.teacher_memory
            },
            "student_model": {
                "size_mb": self.student_size,
                "inference_speed_sec": self.student_speed,
                "memory_usage_mb": self.student_memory,
                "size_reduction_percent": size_reduction,
                "speed_improvement_percent": speed_improvement,
                "memory_reduction_percent": memory_reduction,
                "quality_score": avg_quality
            }
        }
        
        print(f"\nâœ… ê°„ë‹¨í•œ Knowledge Distillation ì™„ë£Œ")
        print(f"ğŸ“Š í¬ê¸°: {self.student_size:.2f}MB ({size_reduction:.1f}% ê°ì†Œ)")
        print(f"ğŸ§  ì†ë„: {self.student_speed:.2f}ì´ˆ ({speed_improvement:.1f}% í–¥ìƒ)")
        print(f"ğŸ’¾ ë©”ëª¨ë¦¬: {self.student_memory:.2f}MB ({memory_reduction:.1f}% ê°ì†Œ)")
        print(f"ğŸ¯ í’ˆì§ˆ ì ìˆ˜: {avg_quality:.3f}")
        
        # ê²°ê³¼ ì €ì¥
        results_path = self.output_dir / "portfolio_distillation_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ ê²°ê³¼ê°€ {results_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # Student ëª¨ë¸ ì €ì¥
        student_path = self.output_dir / "portfolio_student_model"
        student_model.save_pretrained(str(student_path))
        student_tokenizer.save_pretrained(str(student_path))
        print(f"ğŸ’¾ Student ëª¨ë¸ì´ {student_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return results


def main():
    """í¬íŠ¸í´ë¦¬ì˜¤ìš© Knowledge Distillation ì‹¤í–‰"""
    print("ğŸš€ í¬íŠ¸í´ë¦¬ì˜¤ìš© ì•ˆì „í•œ Knowledge Distillation")
    print("=" * 60)
    
    # Teacher: 20% Pruned ëª¨ë¸, Student: KoGPT2-base
    distiller = PortfolioDistiller(
        teacher_model_name="models/pruned/model_20_percent_pruned",
        student_model_name="skt/kogpt2-base-v2",
        output_dir="models/distilled"
    )
    
    # Distillation ì‹¤í–‰
    results = distiller.run_simple_distillation(num_epochs=3)
    
    print("\nâœ… í¬íŠ¸í´ë¦¬ì˜¤ìš© Knowledge Distillation ì™„ë£Œ!")
    
    # ëª©í‘œ ë‹¬ì„±ë„ í‰ê°€
    size_goal = 50.0
    speed_goal = 100.0
    
    size_achievement = results["student_model"]["size_reduction_percent"]
    speed_achievement = results["student_model"]["speed_improvement_percent"]
    
    print(f"\nğŸ¯ ëª©í‘œ ë‹¬ì„±ë„:")
    print(f"  - í¬ê¸° ê°ì†Œ: {size_achievement:.1f}% / {size_goal}% ({'âœ…' if size_achievement >= size_goal else 'âŒ'})")
    print(f"  - ì†ë„ í–¥ìƒ: {speed_achievement:.1f}% / {speed_goal}% ({'âœ…' if speed_achievement >= speed_goal else 'âŒ'})")
    
    if size_achievement >= size_goal and speed_achievement >= speed_goal:
        print("ğŸ‰ ëª¨ë“  ëª©í‘œ ë‹¬ì„±! í¬íŠ¸í´ë¦¬ì˜¤ ì™„ì„±!")
    else:
        print("ğŸ“ˆ ì¶”ê°€ ìµœì í™” í•„ìš”")


if __name__ == "__main__":
    main()
