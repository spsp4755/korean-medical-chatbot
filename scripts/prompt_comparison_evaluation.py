import os
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import time
from datetime import datetime
from tqdm import tqdm

class PromptComparisonEvaluator:
    def __init__(self, base_model_path="42dot/42dot_LLM-SFT-1.3B", 
                 peft_model_path="models/medical_finetuned_peft"):
        self.base_model_path = base_model_path
        self.peft_model_path = peft_model_path
        self.tokenizer = None
        self.model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        self._load_model()
        
    def _load_model(self):
        """PEFT ëª¨ë¸ ë¡œë“œ"""
        print("PEFT ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_path)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_path,
            torch_dtype=torch.float32,
            device_map="cpu",
            low_cpu_mem_usage=True
        )
        
        self.model = PeftModel.from_pretrained(base_model, self.peft_model_path)
        self.model.eval()
        
        print("PEFT ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def create_basic_prompt(self, question):
        """ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ (Before)"""
        return f"ì§ˆë¬¸: {question}\në‹µë³€:"
    
    def create_improved_prompt(self, question):
        """ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ (After)"""
        return f"""ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ì‹¤ìš©ì ì¸ ì˜ë£Œ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤. í™˜ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ë‹µë³€í•´ì£¼ì„¸ìš”:

1. ë¨¼ì € ì‘ê¸‰ìƒí™©ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”
2. ì¼ë°˜ì¸ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ë¡œ ì„¤ëª…í•˜ì„¸ìš”
3. ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì¹˜ë¥¼ ì œì‹œí•˜ì„¸ìš”
4. ë‹µë³€ì€ 200ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ í•˜ì„¸ìš”
5. í•„ìš”ì‹œ ì˜ì‚¬ ìƒë‹´ì„ ê¶Œí•˜ì„¸ìš”

í™˜ì ì§ˆë¬¸: {question}

ì˜ë£Œì§„ ë‹µë³€:"""
    
    def generate_response(self, question, prompt_type="basic", max_length=200):
        """ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        start_time = time.time()
        
        if prompt_type == "basic":
            prompt = self.create_basic_prompt(question)
        else:
            prompt = self.create_improved_prompt(question)
        
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=inputs.input_ids.shape[1] + max_length,
                num_return_sequences=1,
                temperature=0.3,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.1,
                top_p=0.9,
                top_k=50
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.replace(prompt, "").strip()
        
        end_time = time.time()
        response_time = end_time - start_time
        
        return response, response_time
    
    def evaluate_response_quality(self, response):
        """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
        # ì‘ë‹µ ê¸¸ì´
        length = len(response)
        
        # ì‘ê¸‰ìƒí™© í‚¤ì›Œë“œ ì²´í¬
        emergency_keywords = ["ì‘ê¸‰", "ì¦‰ì‹œ", "ë³‘ì›", "ì‘ê¸‰ì‹¤", "119", "ì‹¬ê°", "ìœ„í—˜"]
        emergency_score = sum(1 for keyword in emergency_keywords if keyword in response)
        
        # ì‹¤ìš©ì  ì¡°ì¹˜ í‚¤ì›Œë“œ ì²´í¬
        practical_keywords = ["í•˜ì„¸ìš”", "ë“œì„¸ìš”", "ë§ˆì‹œì„¸ìš”", "íœ´ì‹", "ìˆ˜ë¶„", "ì˜ì‚¬", "ìƒë‹´"]
        practical_score = sum(1 for keyword in practical_keywords if keyword in response)
        
        # ì¼ë°˜ì¸ ì¹œí™”ì  ì–¸ì–´ ì²´í¬
        friendly_keywords = ["ë„ì›€", "ê´œì°®", "ê±±ì •", "ì•ˆì‹¬", "í™•ì¸", "ì²´í¬"]
        friendly_score = sum(1 for keyword in friendly_keywords if keyword in response)
        
        # ì „ë¬¸ ìš©ì–´ ì²´í¬ (ë„ˆë¬´ ë§ìœ¼ë©´ ê°ì )
        medical_terms = ["í•­ìƒì œ", "ê°ìˆ˜ì„±", "ë°°ì–‘", "ì „í•´ì§ˆ", "ë¶ˆê· í˜•", "í•©ë³‘ì¦"]
        medical_term_count = sum(1 for term in medical_terms if term in response)
        
        return {
            "length": length,
            "emergency_score": emergency_score,
            "practical_score": practical_score,
            "friendly_score": friendly_score,
            "medical_term_count": medical_term_count,
            "is_appropriate_length": 50 <= length <= 300,
            "has_emergency_guidance": emergency_score > 0,
            "has_practical_advice": practical_score > 0,
            "is_user_friendly": friendly_score > 0 and medical_term_count <= 2
        }
    
    def compare_prompts(self, test_questions):
        """í”„ë¡¬í”„íŠ¸ ë¹„êµ í‰ê°€"""
        print("ğŸ” í”„ë¡¬í”„íŠ¸ ë¹„êµ í‰ê°€ ì‹œì‘...")
        
        results = {
            "basic_prompt": [],
            "improved_prompt": [],
            "comparison": {}
        }
        
        for question in tqdm(test_questions, desc="ì§ˆë¬¸ í‰ê°€ ì¤‘"):
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ í‰ê°€
            basic_response, basic_time = self.generate_response(question, "basic")
            basic_quality = self.evaluate_response_quality(basic_response)
            
            # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë¡œ í‰ê°€
            improved_response, improved_time = self.generate_response(question, "improved")
            improved_quality = self.evaluate_response_quality(improved_response)
            
            results["basic_prompt"].append({
                "question": question,
                "response": basic_response,
                "response_time": basic_time,
                "quality_metrics": basic_quality
            })
            
            results["improved_prompt"].append({
                "question": question,
                "response": improved_response,
                "response_time": improved_time,
                "quality_metrics": improved_quality
            })
        
        # ë¹„êµ ë¶„ì„
        basic_metrics = [item["quality_metrics"] for item in results["basic_prompt"]]
        improved_metrics = [item["quality_metrics"] for item in results["improved_prompt"]]
        
        results["comparison"] = {
            "avg_response_length": {
                "basic": sum(m["length"] for m in basic_metrics) / len(basic_metrics),
                "improved": sum(m["length"] for m in improved_metrics) / len(improved_metrics)
            },
            "avg_response_time": {
                "basic": sum(item["response_time"] for item in results["basic_prompt"]) / len(results["basic_prompt"]),
                "improved": sum(item["response_time"] for item in results["improved_prompt"]) / len(results["improved_prompt"])
            },
            "emergency_guidance_ratio": {
                "basic": sum(1 for m in basic_metrics if m["has_emergency_guidance"]) / len(basic_metrics),
                "improved": sum(1 for m in improved_metrics if m["has_emergency_guidance"]) / len(improved_metrics)
            },
            "practical_advice_ratio": {
                "basic": sum(1 for m in basic_metrics if m["has_practical_advice"]) / len(basic_metrics),
                "improved": sum(1 for m in improved_metrics if m["has_practical_advice"]) / len(improved_metrics)
            },
            "user_friendly_ratio": {
                "basic": sum(1 for m in basic_metrics if m["is_user_friendly"]) / len(basic_metrics),
                "improved": sum(1 for m in improved_metrics if m["is_user_friendly"]) / len(improved_metrics)
            },
            "appropriate_length_ratio": {
                "basic": sum(1 for m in basic_metrics if m["is_appropriate_length"]) / len(basic_metrics),
                "improved": sum(1 for m in improved_metrics if m["is_appropriate_length"]) / len(improved_metrics)
            }
        }
        
        return results

def main():
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_questions = [
        "ì¥ì—¼ì— ê±¸ë¦°ê±° ê°™ì€ë° ì–´ë–»ê²Œ í•´ì•¼í•˜ë‚˜ìš”?",
        "ë¨¸ë¦¬ê°€ ì•„í”ˆë° ì§„í†µì œ ë¨¹ì–´ë„ ë ê¹Œìš”?",
        "ì—´ì´ ë‚˜ëŠ”ë° ë³‘ì› ê°€ì•¼ í• ê¹Œìš”?",
        "ë³µí†µì´ ì‹¬í•œë° ì‘ê¸‰ì‹¤ ê°€ì•¼ í•˜ë‚˜ìš”?",
        "ê°ê¸° ì¦ìƒì´ ìˆëŠ”ë° ì•½êµ­ì—ì„œ ì•½ì„ ì‚¬ë„ ë ê¹Œìš”?",
        "ì–´ì§€ëŸ¬ìš´ë° ë­˜ í•´ì•¼ í• ê¹Œìš”?",
        "ê°€ìŠ´ì´ ë‹µë‹µí•œë° ì‹¬ì¥ ë¬¸ì œì¼ê¹Œìš”?",
        "ë°°ê°€ ì•„í”ˆë° ì‹ì¤‘ë…ì¼ê¹Œìš”?"
    ]
    
    print("ğŸš€ í”„ë¡¬í”„íŠ¸ ë¹„êµ í‰ê°€ ì‹œì‘")
    print("=" * 60)
    
    evaluator = PromptComparisonEvaluator()
    results = evaluator.compare_prompts(test_questions)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“Š í”„ë¡¬í”„íŠ¸ ë¹„êµ ê²°ê³¼")
    print("=" * 60)
    
    comp = results["comparison"]
    print(f"í‰ê·  ì‘ë‹µ ê¸¸ì´:")
    print(f"  ê¸°ë³¸ í”„ë¡¬í”„íŠ¸: {comp['avg_response_length']['basic']:.1f}ì")
    print(f"  ê°œì„  í”„ë¡¬í”„íŠ¸: {comp['avg_response_length']['improved']:.1f}ì")
    print(f"  ê°œì„ ë„: {((comp['avg_response_length']['improved'] - comp['avg_response_length']['basic']) / comp['avg_response_length']['basic'] * 100):+.1f}%")
    
    print(f"\nì‘ê¸‰ìƒí™© ì•ˆë‚´ ë¹„ìœ¨:")
    print(f"  ê¸°ë³¸ í”„ë¡¬í”„íŠ¸: {comp['emergency_guidance_ratio']['basic']:.1%}")
    print(f"  ê°œì„  í”„ë¡¬í”„íŠ¸: {comp['emergency_guidance_ratio']['improved']:.1%}")
    print(f"  ê°œì„ ë„: {((comp['emergency_guidance_ratio']['improved'] - comp['emergency_guidance_ratio']['basic']) * 100):+.1f}%p")
    
    print(f"\nì‹¤ìš©ì  ì¡°ì¹˜ ì œì‹œ ë¹„ìœ¨:")
    print(f"  ê¸°ë³¸ í”„ë¡¬í”„íŠ¸: {comp['practical_advice_ratio']['basic']:.1%}")
    print(f"  ê°œì„  í”„ë¡¬í”„íŠ¸: {comp['practical_advice_ratio']['improved']:.1%}")
    print(f"  ê°œì„ ë„: {((comp['practical_advice_ratio']['improved'] - comp['practical_advice_ratio']['basic']) * 100):+.1f}%p")
    
    print(f"\nì‚¬ìš©ì ì¹œí™”ì  ë‹µë³€ ë¹„ìœ¨:")
    print(f"  ê¸°ë³¸ í”„ë¡¬í”„íŠ¸: {comp['user_friendly_ratio']['basic']:.1%}")
    print(f"  ê°œì„  í”„ë¡¬í”„íŠ¸: {comp['user_friendly_ratio']['improved']:.1%}")
    print(f"  ê°œì„ ë„: {((comp['user_friendly_ratio']['improved'] - comp['user_friendly_ratio']['basic']) * 100):+.1f}%p")
    
    print(f"\nì ì ˆí•œ ê¸¸ì´ ë‹µë³€ ë¹„ìœ¨:")
    print(f"  ê¸°ë³¸ í”„ë¡¬í”„íŠ¸: {comp['appropriate_length_ratio']['basic']:.1%}")
    print(f"  ê°œì„  í”„ë¡¬í”„íŠ¸: {comp['appropriate_length_ratio']['improved']:.1%}")
    print(f"  ê°œì„ ë„: {((comp['appropriate_length_ratio']['improved'] - comp['appropriate_length_ratio']['basic']) * 100):+.1f}%p")
    
    # ê²°ê³¼ ì €ì¥
    os.makedirs("results", exist_ok=True)
    with open("results/prompt_comparison_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ê²°ê³¼ê°€ results/prompt_comparison_results.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
