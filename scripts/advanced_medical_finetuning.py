#!/usr/bin/env python3
"""
ê³ ê¸‰ ì˜ë£Œ íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
- 42dot LLM ì‚¬ìš©
- 42dot ê³µì‹ íŒŒë¼ë¯¸í„° ì ìš©
- ë©”ëª¨ë¦¬ ìµœì í™”
"""

import os
import json
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from torch.utils.data import Dataset
import warnings
warnings.filterwarnings("ignore")

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["WANDB_DISABLED"] = "true"

class MedicalDataset(Dataset):
    def __init__(self, data, tokenizer, max_length=256):  # ê·¹í•œ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ 256ìœ¼ë¡œ ì„¤ì •
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        if 'question' in item and 'answer' in item:
            text = f"í™˜ì: {item['question']}\nì˜ë£Œì§„: {item['answer']}"
        else:
            text = str(item)
        
        # í† í¬ë‚˜ì´ì§•
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': encoding['input_ids'].flatten()
        }

class AdvancedMedicalFineTuner:
    def __init__(self, model_name="42dot/42dot_LLM-SFT-1.3B"):
        """ê³ ê¸‰ ì˜ë£Œ íŒŒì¸íŠœë„ˆ ì´ˆê¸°í™”"""
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©
        self._load_model()
        
    def _load_model(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©"""
        print(f"\nğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float32,  # MPS í˜¸í™˜ì„±ì„ ìœ„í•´ float32 ì‚¬ìš©
                device_map="auto" if self.device == "cuda" else None,
                low_cpu_mem_usage=True,     # CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì†Œí™”
                use_cache=False             # ìºì‹œ ë¹„í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
            )
            
            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def load_data(self, data_paths):
        """ë°ì´í„° ë¡œë”©"""
        print(f"\nğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...")
        
        all_data = []
        for path in data_paths:
            if os.path.exists(path):
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    all_data.extend(data)
                    print(f"âœ… {path}: {len(data)}ê°œ ìƒ˜í”Œ")
            else:
                print(f"âš ï¸ {path}: íŒŒì¼ ì—†ìŒ")
        
        print(f"ğŸ“Š ì´ ë°ì´í„°: {len(all_data)}ê°œ ìƒ˜í”Œ")
        return all_data
    
    def fine_tune(self, train_data, val_data, output_dir="./models/medical_finetuned_advanced"):
        """ê³ ê¸‰ íŒŒì¸íŠœë‹ ì‹¤í–‰"""
        print(f"\nğŸš€ ê³ ê¸‰ ì˜ë£Œ íŒŒì¸íŠœë‹ ì‹œì‘")
        print(f"ğŸ“Š Train: {len(train_data)}ê°œ, Val: {len(val_data)}ê°œ")
        
        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = MedicalDataset(train_data, self.tokenizer)
        val_dataset = MedicalDataset(val_data, self.tokenizer)
        
        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
            pad_to_multiple_of=8
        )
        
        # ê·¹í•œ ë©”ëª¨ë¦¬ ì ˆì•½ íŒŒë¼ë¯¸í„°
        training_args = TrainingArguments(
            output_dir=output_dir,
            overwrite_output_dir=True,
            num_train_epochs=2,             # ì—í¬í¬ ìˆ˜ ê°ì†Œ (3â†’2)
            per_device_train_batch_size=1,  # ìµœì†Œ ë°°ì¹˜ í¬ê¸°
            per_device_eval_batch_size=1,   # ìµœì†Œ ë°°ì¹˜ í¬ê¸°
            warmup_steps=10,                # ì›Œë°ì—… ë‹¨ê³„ ìµœì†Œí™” (25â†’10)
            weight_decay=0.0,               # 42dot ê³µì‹: 0
            logging_dir=f"{output_dir}/logs",
            logging_steps=100,              # ë¡œê¹… ë¹ˆë„ ë” ê°ì†Œ (50â†’100)
            eval_strategy="steps",
            eval_steps=500,                 # í‰ê°€ ë¹ˆë„ ë” ê°ì†Œ (200â†’500)
            save_steps=500,                 # ì €ì¥ ë¹ˆë„ ë” ê°ì†Œ (200â†’500)
            save_total_limit=1,             # ì²´í¬í¬ì¸íŠ¸ 1ê°œë§Œ ìœ ì§€ (2â†’1)
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            report_to=[],
            dataloader_pin_memory=False,
            remove_unused_columns=False,
            gradient_accumulation_steps=32,  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ê°ì†Œ (128â†’32)
            fp16=False,                     # MPS í˜¸í™˜ì„±
            max_steps=300,                  # í•™ìŠµ ë‹¨ê³„ ë” ê°ì†Œ (500â†’300)
            learning_rate=1e-5,             # í•™ìŠµë¥  ê°ì†Œ (2e-5â†’1e-5)
            warmup_ratio=0.01,              # ì›Œë°ì—… ë¹„ìœ¨ ê°ì†Œ (0.03â†’0.01)
            max_grad_norm=1.0,              # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì™„í™” (0.5â†’1.0)
            lr_scheduler_type="linear",     # ì„ í˜• ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ ë³€ê²½ (cosineâ†’linear)
            dataloader_num_workers=0,       # ë©€í‹°í”„ë¡œì„¸ì‹± ë¹„í™œì„±í™”
            dataloader_drop_last=True,      # ë§ˆì§€ë§‰ ë°°ì¹˜ ë“œë¡­
            dataloader_persistent_workers=False,  # ì›Œì»¤ ì§€ì†ì„± ë¹„í™œì„±í™”
            save_safetensors=False,         # SafeTensors ì €ì¥ ë¹„í™œì„±í™”
            save_only_model=True,           # ëª¨ë¸ë§Œ ì €ì¥ (í† í¬ë‚˜ì´ì € ì œì™¸)
            ignore_data_skip=True,          # ë°ì´í„° ìŠ¤í‚µ ë¬´ì‹œ
            skip_memory_metrics=True        # ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­ ìŠ¤í‚µ
        )
        
        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer
        )
        
        # í•™ìŠµ ì‹¤í–‰
        print(f"\nğŸ“š í•™ìŠµ ì‹œì‘...")
        trainer.train()
        
        # ëª¨ë¸ ì €ì¥
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        
        print(f"\nâœ… ê³ ê¸‰ íŒŒì¸íŠœë‹ ì™„ë£Œ!")
        print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {output_dir}")
        
        return trainer

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ê³ ê¸‰ ì˜ë£Œ íŒŒì¸íŠœë‹ ì‹œì‘")
    print("=" * 50)
    
    # íŒŒì¸íŠœë„ˆ ì´ˆê¸°í™”
    fine_tuner = AdvancedMedicalFineTuner()
    
    # ë°ì´í„° ë¡œë”© (ì£¼ê´€ì‹ ë°ì´í„°ë§Œ ì‚¬ìš© - Essential + Professional)
    train_paths = [
        "data/processed/splits/essential_medical_train_subjective.json",
        "data/processed/splits/professional_medical_train_subjective.json"
    ]
    
    val_paths = [
        "data/processed/splits/essential_medical_val_subjective.json",
        "data/processed/splits/professional_medical_val_subjective.json"
    ]
    
    train_data = fine_tuner.load_data(train_paths)
    val_data = fine_tuner.load_data(val_paths)
    
    # íŒŒì¸íŠœë‹ ì‹¤í–‰
    trainer = fine_tuner.fine_tune(train_data, val_data)
    
    print("\nğŸ‰ ê³ ê¸‰ ì˜ë£Œ íŒŒì¸íŠœë‹ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
