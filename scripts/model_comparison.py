#!/usr/bin/env python3
"""
í•œêµ­ì–´ LLM ëª¨ë¸ ë¹„êµ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
KULLM-2 vs Polyglot-Ko ëª¨ë¸ì˜ ì„±ëŠ¥, í¬ê¸°, ì†ë„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
"""

import json
import time
import torch
from pathlib import Path
from typing import Dict, List, Any
from transformers import AutoTokenizer, AutoModelForCausalLM
import psutil
import yaml

def load_config(config_path: str = "configs/config.yaml") -> Dict[str, Any]:
    """ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def get_model_info(model_name: str) -> Dict[str, Any]:
    """ëª¨ë¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto" if torch.cuda.is_available() else "cpu"
        )
        
        # ëª¨ë¸ í¬ê¸° ê³„ì‚°
        param_count = sum(p.numel() for p in model.parameters())
        model_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)
        
        return {
            "model_name": model_name,
            "vocab_size": tokenizer.vocab_size,
            "max_length": tokenizer.model_max_length,
            "parameter_count": param_count,
            "model_size_mb": model_size_mb,
            "model_size_gb": model_size_mb / 1024,
            "tokenizer_type": type(tokenizer).__name__,
            "model_type": type(model).__name__
        }
    except Exception as e:
        return {
            "model_name": model_name,
            "error": str(e)
        }

def benchmark_inference_speed(model_name: str, test_prompts: List[str], num_runs: int = 3) -> Dict[str, Any]:
    """ì¶”ë¡  ì†ë„ë¥¼ ë²¤ì¹˜ë§ˆí¬í•©ë‹ˆë‹¤."""
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto" if torch.cuda.is_available() else "cpu"
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        times = []
        tokens_generated = []
        
        for _ in range(num_runs):
            for prompt in test_prompts:
                inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
                
                start_time = time.time()
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=50,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id
                    )
                end_time = time.time()
                
                generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]
                tokens_generated.append(len(generated_tokens))
                times.append(end_time - start_time)
        
        avg_time = sum(times) / len(times)
        avg_tokens = sum(tokens_generated) / len(tokens_generated)
        tokens_per_second = avg_tokens / avg_time if avg_time > 0 else 0
        
        return {
            "avg_inference_time": avg_time,
            "avg_tokens_generated": avg_tokens,
            "tokens_per_second": tokens_per_second,
            "total_runs": len(times)
        }
    except Exception as e:
        return {
            "error": str(e)
        }

def test_korean_understanding(model_name: str, test_cases: List[Dict[str, str]]) -> Dict[str, Any]:
    """í•œêµ­ì–´ ì´í•´ ëŠ¥ë ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto" if torch.cuda.is_available() else "cpu"
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        results = []
        
        for test_case in test_cases:
            prompt = test_case["prompt"]
            expected_keywords = test_case.get("expected_keywords", [])
            
            inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=100,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = generated_text[len(prompt):].strip()
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            keyword_score = 0
            if expected_keywords:
                for keyword in expected_keywords:
                    if keyword.lower() in response.lower():
                        keyword_score += 1
                keyword_score = keyword_score / len(expected_keywords)
            
            results.append({
                "prompt": prompt,
                "response": response,
                "expected_keywords": expected_keywords,
                "keyword_score": keyword_score
            })
        
        avg_keyword_score = sum(r["keyword_score"] for r in results) / len(results)
        
        return {
            "test_results": results,
            "avg_keyword_score": avg_keyword_score,
            "total_tests": len(results)
        }
    except Exception as e:
        return {
            "error": str(e)
        }

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ í•œêµ­ì–´ LLM ëª¨ë¸ ë¹„êµ ë¶„ì„ ì‹œì‘")
    
    # ì„¤ì • ë¡œë“œ
    config = load_config()
    
    # ë¹„êµí•  ëª¨ë¸ë“¤ (ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í•œêµ­ì–´ LLM ëª¨ë¸)
    models = [
        "psymon/KoLlama2-7b",  # í•œêµ­ì–´ ìµœì í™” Llama2 ëª¨ë¸
        "beomi/KcBERT-base"    # í•œêµ­ì–´ BERT ëª¨ë¸ (ë¹„êµìš©)
    ]
    
    # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    test_prompts = [
        "ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì‹ ê°€ìš”?",
        "ì˜ë£Œ ìƒë‹´ì„ ë°›ê³  ì‹¶ìŠµë‹ˆë‹¤.",
        "ê°ê¸° ì¦ìƒì´ ìˆëŠ”ë° ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?",
        "í•œêµ­ì˜ ì „í†µ ìŒì‹ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    ]
    
    # í•œêµ­ì–´ ì´í•´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    korean_test_cases = [
        {
            "prompt": "ì˜ë£Œ ìƒë‹´: ë‘í†µì´ ì‹¬í•˜ê³  ë©”ìŠ¤êº¼ì›€ì´ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ì§ˆë³‘ì¼ ê°€ëŠ¥ì„±ì´ ìˆë‚˜ìš”?",
            "expected_keywords": ["ë‘í†µ", "ë©”ìŠ¤êº¼ì›€", "ì˜ì‚¬", "ì§„ë£Œ", "ë³‘ì›"]
        },
        {
            "prompt": "í•œêµ­ì–´ ë¬¸ë²• ì§ˆë¬¸: 'ë‚˜ëŠ” í•™êµì— ê°„ë‹¤'ì™€ 'ë‚˜ëŠ” í•™êµì— ê°€ê³  ìˆë‹¤'ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "expected_keywords": ["í˜„ì¬", "ì§„í–‰", "ì‹œì œ", "ë¬¸ë²•"]
        },
        {
            "prompt": "ìˆ˜í•™ ë¬¸ì œ: 2x + 5 = 13ì¼ ë•Œ xì˜ ê°’ì€?",
            "expected_keywords": ["4", "ë°©ì •ì‹", "í•´", "ê³„ì‚°"]
        }
    ]
    
    results = {}
    
    for model_name in models:
        print(f"\nğŸ“Š {model_name} ë¶„ì„ ì¤‘...")
        
        # 1. ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘
        print("  - ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        model_info = get_model_info(model_name)
        results[model_name] = {"model_info": model_info}
        
        if "error" in model_info:
            print(f"  âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_info['error']}")
            continue
        
        print(f"  âœ… ëª¨ë¸ í¬ê¸°: {model_info['model_size_gb']:.2f}GB")
        print(f"  âœ… íŒŒë¼ë¯¸í„° ìˆ˜: {model_info['parameter_count']:,}")
        
        # 2. ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬
        print("  - ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬ ì¤‘...")
        speed_results = benchmark_inference_speed(model_name, test_prompts)
        results[model_name]["speed_benchmark"] = speed_results
        
        if "error" not in speed_results:
            print(f"  âœ… í‰ê·  ì¶”ë¡  ì‹œê°„: {speed_results['avg_inference_time']:.2f}ì´ˆ")
            print(f"  âœ… í† í°/ì´ˆ: {speed_results['tokens_per_second']:.2f}")
        
        # 3. í•œêµ­ì–´ ì´í•´ í…ŒìŠ¤íŠ¸
        print("  - í•œêµ­ì–´ ì´í•´ í…ŒìŠ¤íŠ¸ ì¤‘...")
        korean_results = test_korean_understanding(model_name, korean_test_cases)
        results[model_name]["korean_understanding"] = korean_results
        
        if "error" not in korean_results:
            print(f"  âœ… í•œêµ­ì–´ ì´í•´ ì ìˆ˜: {korean_results['avg_keyword_score']:.2f}")
    
    # ê²°ê³¼ ì €ì¥
    output_path = Path("outputs/model_comparison_results.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“‹ ê²°ê³¼ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ê°„ë‹¨í•œ ë¹„êµ ìš”ì•½
    print("\nğŸ“Š ëª¨ë¸ ë¹„êµ ìš”ì•½:")
    for model_name, result in results.items():
        if "error" not in result.get("model_info", {}):
            info = result["model_info"]
            speed = result.get("speed_benchmark", {})
            korean = result.get("korean_understanding", {})
            
            print(f"\nğŸ”¹ {model_name}:")
            print(f"  - í¬ê¸°: {info['model_size_gb']:.2f}GB")
            print(f"  - íŒŒë¼ë¯¸í„°: {info['parameter_count']:,}")
            if "tokens_per_second" in speed:
                print(f"  - ì†ë„: {speed['tokens_per_second']:.2f} í† í°/ì´ˆ")
            if "avg_keyword_score" in korean:
                print(f"  - í•œêµ­ì–´ ì ìˆ˜: {korean['avg_keyword_score']:.2f}")

if __name__ == "__main__":
    main()
