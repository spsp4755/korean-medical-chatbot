#!/usr/bin/env python3
"""
í¬íŠ¸í´ë¦¬ì˜¤ìš© ê°•í™”ëœ Knowledge Distillation
ì•ˆì •ì„±ê³¼ ì •í™•ì„±ì„ ëª¨ë‘ ê³ ë ¤í•œ êµ¬í˜„
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
import numpy as np
from typing import Dict, Any, List, Tuple, Optional
import time
import psutil
import os
from pathlib import Path
import json
import yaml
from datasets import Dataset
import logging
from torch.utils.data import DataLoader
import warnings
warnings.filterwarnings("ignore")

# MPS ë¹„í™œì„±í™” (Apple Silicon Mac í˜¸í™˜ì„±)
import os
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RobustDistillationTrainer(Trainer):
    """ê°•í™”ëœ Knowledge Distillation Trainer"""
    
    def __init__(self, teacher_model, temperature=3.0, alpha=0.7, **kwargs):
        super().__init__(**kwargs)
        self.teacher_model = teacher_model
        self.temperature = temperature
        self.alpha = alpha
        
        # Teacher ëª¨ë¸ì„ evaluation ëª¨ë“œë¡œ ì„¤ì •
        self.teacher_model.eval()
        
        # ë²„ì „ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ ì˜¤ë²„ë¼ì´ë“œ
        self._setup_compute_loss()
        
    def _setup_compute_loss(self):
        """compute_loss ë©”ì„œë“œ ë™ì  ì„¤ì •"""
        import inspect
        
        # í˜„ì¬ Trainerì˜ compute_loss ì‹œê·¸ë‹ˆì²˜ í™•ì¸
        try:
            sig = inspect.signature(super().compute_loss)
            params = list(sig.parameters.keys())
            
            if 'num_items_in_batch' in params:
                # ìµœì‹  ë²„ì „
                self._compute_loss_method = self._compute_loss_v2
            else:
                # ì´ì „ ë²„ì „
                self._compute_loss_method = self._compute_loss_v1
                
        except Exception as e:
            logger.warning(f"ì‹œê·¸ë‹ˆì²˜ í™•ì¸ ì‹¤íŒ¨, ê¸°ë³¸ ë°©ë²• ì‚¬ìš©: {e}")
            self._compute_loss_method = self._compute_loss_v1
    
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        """ë²„ì „ í˜¸í™˜ compute_loss"""
        return self._compute_loss_method(model, inputs, return_outputs, **kwargs)
    
    def _compute_loss_v1(self, model, inputs, return_outputs=False):
        """ì´ì „ ë²„ì „ í˜¸í™˜"""
        return self._compute_distillation_loss(model, inputs, return_outputs)
    
    def _compute_loss_v2(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """ìµœì‹  ë²„ì „ í˜¸í™˜"""
        return self._compute_distillation_loss(model, inputs, return_outputs)
    
    def _compute_distillation_loss(self, model, inputs, return_outputs=False):
        """ì‹¤ì œ Knowledge Distillation Loss ê³„ì‚°"""
        # Student ëª¨ë¸ ì¶œë ¥
        student_outputs = model(**inputs)
        student_logits = student_outputs.logits
        
        # Teacher ëª¨ë¸ ì¶œë ¥ (gradient ê³„ì‚° ì—†ì´)
        with torch.no_grad():
            teacher_outputs = self.teacher_model(**inputs)
            teacher_logits = teacher_outputs.logits
        
        # Hard targets (ground truth)
        labels = inputs.get("labels")
        if labels is not None:
            # Cross entropy loss for hard targets
            hard_loss = F.cross_entropy(
                student_logits.view(-1, student_logits.size(-1)),
                labels.view(-1),
                ignore_index=-100
            )
        else:
            hard_loss = 0.0
        
        # Soft targets (teacher knowledge)
        # Temperature scaling ì ìš©
        student_soft = F.log_softmax(student_logits / self.temperature, dim=-1)
        teacher_soft = F.softmax(teacher_logits / self.temperature, dim=-1)
        
        # KL divergence loss for soft targets
        soft_loss = F.kl_div(
            student_soft,
            teacher_soft,
            reduction='batchmean'
        ) * (self.temperature ** 2)
        
        # Combined loss
        total_loss = self.alpha * soft_loss + (1 - self.alpha) * hard_loss
        
        return (total_loss, student_outputs) if return_outputs else total_loss


class RobustKnowledgeDistiller:
    """í¬íŠ¸í´ë¦¬ì˜¤ìš© ê°•í™”ëœ Knowledge Distiller"""
    
    def __init__(self, teacher_model_name: str, student_model_name: str, output_dir: str = "models/distilled"):
        self.teacher_model_name = teacher_model_name
        self.student_model_name = student_model_name
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ë¡œë“œ
        self._load_models()
        
        # ì„±ëŠ¥ ì¸¡ì •
        self._measure_baseline_performance()
        
    def _load_models(self):
        """ëª¨ë¸ ë¡œë“œ"""
        print(f"ğŸ”„ Teacher ëª¨ë¸ ë¡œë”© ì¤‘: {self.teacher_model_name}")
        self.teacher_model = AutoModelForCausalLM.from_pretrained(
            self.teacher_model_name,
            torch_dtype=torch.float32,
            device_map="cpu"  # CPU ê°•ì œ ì‚¬ìš© (MPS ë¬¸ì œ í•´ê²°)
        )
        self.teacher_tokenizer = AutoTokenizer.from_pretrained(self.teacher_model_name)
        
        print(f"ğŸ”„ Student ëª¨ë¸ ë¡œë”© ì¤‘: {self.student_model_name}")
        self.student_model = AutoModelForCausalLM.from_pretrained(
            self.student_model_name,
            torch_dtype=torch.float32,
            device_map="cpu"  # CPU ê°•ì œ ì‚¬ìš© (MPS ë¬¸ì œ í•´ê²°)
        )
        self.student_tokenizer = AutoTokenizer.from_pretrained(self.student_model_name)
        
        # íŒ¨ë”© í† í° ì„¤ì • ë° í˜¸í™˜ì„± í™•ì¸
        for tokenizer in [self.teacher_tokenizer, self.student_tokenizer]:
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
        
        # í† í¬ë‚˜ì´ì € í˜¸í™˜ì„± í™•ì¸
        print(f"ğŸ“Š Teacher vocab size: {len(self.teacher_tokenizer)}")
        print(f"ğŸ“Š Student vocab size: {len(self.student_tokenizer)}")
        
        # Student ëª¨ë¸ì˜ vocabulary í¬ê¸° í™•ì¸
        student_vocab_size = self.student_model.config.vocab_size
        teacher_vocab_size = self.teacher_model.config.vocab_size
        print(f"ğŸ“Š Teacher model vocab: {teacher_vocab_size}")
        print(f"ğŸ“Š Student model vocab: {student_vocab_size}")
        
        # Student í† í¬ë‚˜ì´ì €ë¥¼ Teacherì™€ ë§ì¶¤ (í•„ìš”ì‹œ)
        if len(self.student_tokenizer) != student_vocab_size:
            print("âš ï¸ Student í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ vocabulary ë¶ˆì¼ì¹˜ ê°ì§€")
            # Student í† í¬ë‚˜ì´ì €ë¥¼ ëª¨ë¸ì— ë§ì¶¤
            self.student_tokenizer.pad_token = self.student_tokenizer.eos_token
        
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    def _measure_baseline_performance(self):
        """ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •"""
        print("ğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        self.teacher_size = self._get_model_size(self.teacher_model)
        self.student_size = self._get_model_size(self.student_model)
        self.teacher_speed = self._benchmark_inference(self.teacher_model, self.teacher_tokenizer)
        self.student_speed = self._benchmark_inference(self.student_model, self.student_tokenizer)
        self.teacher_memory = self._get_memory_usage()
        self.student_memory = self._get_memory_usage()
        
        print(f"ğŸ“Š Teacher í¬ê¸°: {self.teacher_size:.2f}MB")
        print(f"ğŸ“Š Student í¬ê¸°: {self.student_size:.2f}MB")
        print(f"ğŸ“Š í¬ê¸° ê°ì†Œ: {((self.teacher_size - self.student_size) / self.teacher_size * 100):.1f}%")
        print(f"ğŸ§  Teacher ì†ë„: {self.teacher_speed:.2f}ì´ˆ")
        print(f"ğŸ§  Student ì†ë„: {self.student_speed:.2f}ì´ˆ")
    
    def _get_model_size(self, model) -> float:
        """ëª¨ë¸ í¬ê¸° ì¸¡ì • (MB)"""
        param_size = 0
        buffer_size = 0
        
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()
        
        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()
        
        size_all_mb = (param_size + buffer_size) / 1024**2
        return size_all_mb
    
    def _benchmark_inference(self, model, tokenizer, num_runs: int = 3) -> float:
        """ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬ (ì´ˆ)"""
        model.eval()
        test_text = "ì•ˆë…•í•˜ì„¸ìš”. ì˜ë£Œ ìƒë‹´ì„ ë°›ê³  ì‹¶ìŠµë‹ˆë‹¤."
        
        inputs = tokenizer(test_text, return_tensors="pt", padding=True, truncation=True, max_length=128)
        
        # ì›Œë°ì—…
        with torch.no_grad():
            for _ in range(2):
                _ = model.generate(
                    inputs.input_ids,
                    max_length=inputs.input_ids.shape[1] + 20,
                    num_return_sequences=1,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id
                )
        
        # ì‹¤ì œ ì¸¡ì •
        start_time = time.time()
        with torch.no_grad():
            for _ in range(num_runs):
                _ = model.generate(
                    inputs.input_ids,
                    max_length=inputs.input_ids.shape[1] + 20,
                    num_return_sequences=1,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id
                )
        end_time = time.time()
        
        avg_time = (end_time - start_time) / num_runs
        return avg_time
    
    def _get_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (MB)"""
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / 1024**2
        else:
            process = psutil.Process(os.getpid())
            return process.memory_info().rss / 1024**2
    
    def _prepare_medical_data(self) -> Dataset:
        """ì˜ë£Œ ë°ì´í„° ì¤€ë¹„"""
        print("ğŸ“š ì˜ë£Œ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        
        medical_data = []
        medical_files = [
            "data/processed/medical_data.json",
            "data/processed/splits/essential_medical_test.json",
            "data/processed/splits/professional_medical_test.json"
        ]
        
        for file_path in medical_files:
            if Path(file_path).exists():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if isinstance(data, list):
                            medical_data.extend(data)
                        else:
                            medical_data.append(data)
                    print(f"âœ… ë¡œë“œë¨: {file_path}")
                except Exception as e:
                    logger.warning(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {file_path} - {e}")
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = []
        for item in medical_data[:500]:  # 500ê°œë¡œ ì œí•œ (ì•ˆì •ì„±)
            if isinstance(item, dict):
                for key in ['text', 'content', 'question', 'answer', 'instruction', 'input', 'output']:
                    if key in item and isinstance(item[key], str) and len(item[key]) > 10:
                        texts.append(item[key])
                        break
            elif isinstance(item, str) and len(item) > 10:
                texts.append(item)
        
        # ì¤‘ë³µ ì œê±°
        texts = list(set(texts))
        print(f"ğŸ“ ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸: {len(texts)}ê°œ")
        
        if len(texts) == 0:
            # ë”ë¯¸ ë°ì´í„° ìƒì„±
            texts = [
                "ì•ˆë…•í•˜ì„¸ìš”. ì˜ë£Œ ìƒë‹´ì„ ë°›ê³  ì‹¶ìŠµë‹ˆë‹¤.",
                "ë¨¸ë¦¬ê°€ ì•„í”ˆë° ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?",
                "ê°ê¸° ì¦ìƒì´ ìˆëŠ”ë° ë³‘ì›ì— ê°€ì•¼ í• ê¹Œìš”?",
                "ë³µí†µì´ ì‹¬í•œë° ì‘ê¸‰ì‹¤ì— ê°€ì•¼ í• ê¹Œìš”?",
                "í”¼ë¶€ì— ë°œì§„ì´ ìƒê²¼ëŠ”ë° ì›ì¸ì´ ë­˜ê¹Œìš”?"
            ] * 20
        
        return Dataset.from_dict({"text": texts})
    
    def _tokenize_data(self, dataset: Dataset) -> Dataset:
        """ë°ì´í„° í† í¬ë‚˜ì´ì§•"""
        def tokenize_function(examples):
            return self.student_tokenizer(
                examples["text"],
                truncation=True,
                padding=True,
                max_length=128,
                return_tensors="pt"
            )
        
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names
        )
        
        return tokenized_dataset
    
    def run_robust_distillation(self, 
                               temperature: float = 3.0,
                               alpha: float = 0.7,
                               num_epochs: int = 2,
                               batch_size: int = 2,
                               learning_rate: float = 5e-5) -> Dict[str, Any]:
        """ê°•í™”ëœ Knowledge Distillation ì‹¤í–‰"""
        print("ğŸš€ ê°•í™”ëœ Knowledge Distillation ì‹œì‘")
        print("=" * 60)
        
        # ë°ì´í„° ì¤€ë¹„
        dataset = self._prepare_medical_data()
        tokenized_dataset = self._tokenize_data(dataset)
        
        # Data Collator ì„¤ì •
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.student_tokenizer,
            mlm=False
        )
        
        # Training Arguments ì„¤ì • (CPU ìµœì í™”)
        training_args = TrainingArguments(
            output_dir=str(self.output_dir / "training_output"),
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            warmup_steps=50,
            weight_decay=0.01,
            logging_dir=str(self.output_dir / "logs"),
            logging_steps=5,
            save_steps=100,
            eval_strategy="no",
            save_total_limit=1,
            learning_rate=learning_rate,
            fp16=False,  # CPU í™˜ê²½ì—ì„œ ì•ˆì •ì„± ìš°ì„ 
            dataloader_pin_memory=False,
            remove_unused_columns=False,
            report_to=None,  # wandb ë“± ë¹„í™œì„±í™”
            use_cpu=True,  # CPU ê°•ì œ ì‚¬ìš©
            dataloader_num_workers=0,  # ë©€í‹°í”„ë¡œì„¸ì‹± ë¹„í™œì„±í™”
        )
        
        # Robust Trainer ì´ˆê¸°í™”
        trainer = RobustDistillationTrainer(
            teacher_model=self.teacher_model,
            temperature=temperature,
            alpha=alpha,
            model=self.student_model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
            tokenizer=self.student_tokenizer,
        )
        
        # Distillation ì‹¤í–‰
        print("ğŸ“ Knowledge Distillation í•™ìŠµ ì‹œì‘...")
        start_time = time.time()
        
        try:
            trainer.train()
            training_time = time.time() - start_time
            print(f"âœ… Distillation ì™„ë£Œ: {training_time:.2f}ì´ˆ")
            
            # Distilled ëª¨ë¸ ì €ì¥
            distilled_model_path = self.output_dir / "robust_distilled_model"
            trainer.save_model(str(distilled_model_path))
            self.student_tokenizer.save_pretrained(str(distilled_model_path))
            
            # Distilled ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
            print("ğŸ“Š Distilled ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
            distilled_size = self._get_model_size(self.student_model)
            distilled_speed = self._benchmark_inference(self.student_model, self.student_tokenizer)
            distilled_memory = self._get_memory_usage()
            
            # ê²°ê³¼ ê³„ì‚°
            size_reduction = ((self.teacher_size - distilled_size) / self.teacher_size) * 100
            speed_improvement = ((self.teacher_speed - distilled_speed) / self.teacher_speed) * 100
            memory_reduction = ((self.teacher_memory - distilled_memory) / self.teacher_memory) * 100
            
            results = {
                "distillation_config": {
                    "temperature": temperature,
                    "alpha": alpha,
                    "num_epochs": num_epochs,
                    "batch_size": batch_size,
                    "learning_rate": learning_rate,
                    "training_time_sec": training_time
                },
                "teacher_model": {
                    "size_mb": self.teacher_size,
                    "inference_speed_sec": self.teacher_speed,
                    "memory_usage_mb": self.teacher_memory
                },
                "student_model": {
                    "size_mb": self.student_size,
                    "inference_speed_sec": self.student_speed,
                    "memory_usage_mb": self.student_memory
                },
                "distilled_model": {
                    "size_mb": distilled_size,
                    "inference_speed_sec": distilled_speed,
                    "memory_usage_mb": distilled_memory,
                    "size_reduction_percent": size_reduction,
                    "speed_improvement_percent": speed_improvement,
                    "memory_reduction_percent": memory_reduction
                }
            }
            
            print(f"âœ… ê°•í™”ëœ Knowledge Distillation ì™„ë£Œ")
            print(f"ğŸ“Š í¬ê¸°: {distilled_size:.2f}MB ({size_reduction:.1f}% ê°ì†Œ)")
            print(f"ğŸ§  ì†ë„: {distilled_speed:.2f}ì´ˆ ({speed_improvement:.1f}% í–¥ìƒ)")
            print(f"ğŸ’¾ ë©”ëª¨ë¦¬: {distilled_memory:.2f}MB ({memory_reduction:.1f}% ê°ì†Œ)")
            
        except Exception as e:
            print(f"âŒ Knowledge Distillation ì‹¤íŒ¨: {str(e)}")
            results = {"error": str(e)}
        
        # ê²°ê³¼ ì €ì¥
        results_path = self.output_dir / "robust_distillation_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ ì‹¤í—˜ ê²°ê³¼ê°€ {results_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return results


def main():
    """ê°•í™”ëœ Knowledge Distillation ì‹¤í–‰"""
    print("ğŸš€ í¬íŠ¸í´ë¦¬ì˜¤ìš© ê°•í™”ëœ Knowledge Distillation")
    print("=" * 60)
    
    # Teacher: 20% Pruned ëª¨ë¸, Student: KoGPT2-base
    distiller = RobustKnowledgeDistiller(
        teacher_model_name="models/pruned/model_20_percent_pruned",
        student_model_name="skt/kogpt2-base-v2",
        output_dir="models/distilled"
    )
    
    # Distillation ì‹¤í–‰
    results = distiller.run_robust_distillation(
        temperature=3.0,
        alpha=0.7,
        num_epochs=2,
        batch_size=2,
        learning_rate=5e-5
    )
    
    print("\nâœ… í¬íŠ¸í´ë¦¬ì˜¤ìš© Knowledge Distillation ì™„ë£Œ!")


if __name__ == "__main__":
    main()
