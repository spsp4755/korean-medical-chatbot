import json
import os
from datetime import datetime

def load_evaluation_results():
    """3ê°œ í”„ë¡¬í”„íŠ¸ í‰ê°€ ê²°ê³¼ ë¡œë“œ"""
    results = {}
    
    # Basic í”„ë¡¬í”„íŠ¸ ê²°ê³¼
    if os.path.exists("results/peft_model_evaluation.json"):
        with open("results/peft_model_evaluation.json", "r", encoding="utf-8") as f:
            results["basic"] = json.load(f)
    
    # Improved í”„ë¡¬í”„íŠ¸ ê²°ê³¼
    if os.path.exists("results/improved_prompt_evaluation.json"):
        with open("results/improved_prompt_evaluation.json", "r", encoding="utf-8") as f:
            results["improved"] = json.load(f)
    
    # Optimized í”„ë¡¬í”„íŠ¸ ê²°ê³¼
    if os.path.exists("results/optimized_prompt_evaluation.json"):
        with open("results/optimized_prompt_evaluation.json", "r", encoding="utf-8") as f:
            results["optimized"] = json.load(f)
    
    return results

def compare_three_prompts(results):
    """3ê°œ í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ë¹„êµ"""
    comparison = {
        "comparison_timestamp": datetime.now().isoformat(),
        "prompt_types": {
            "basic": "ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ (ì§ˆë¬¸: {question}\\në‹µë³€:)",
            "improved": "ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ (ì‘ê¸‰ìƒí™© íŒë‹¨, ì‹¤ìš©ì  ì¡°ì¹˜ ë“±)",
            "optimized": "ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ (Basic + ìµœì†Œí•œì˜ ê°œì„ )"
        },
        "performance_comparison": {}
    }
    
    # ê° í”„ë¡¬í”„íŠ¸ë³„ ë©”íŠ¸ë¦­ ì¶”ì¶œ
    basic_bertscore = results["basic"]["bertscore"]
    basic_medical = results["basic"]["medical_metrics"]
    
    improved_bertscore = results["improved"]["bertscore"]
    improved_medical = results["improved"]["medical_metrics"]
    
    optimized_bertscore = results["optimized"]["bertscore"]
    optimized_medical = results["optimized"]["medical_metrics"]
    
    # BERTScore F1 ë¹„êµ
    comparison["performance_comparison"]["bertscore_f1"] = {
        "basic": {
            "value": basic_bertscore["bertscore_f1"],
            "formatted": f"{basic_bertscore['bertscore_f1']:.4f}"
        },
        "improved": {
            "value": improved_bertscore["bertscore_f1"],
            "formatted": f"{improved_bertscore['bertscore_f1']:.4f}",
            "vs_basic": f"{((improved_bertscore['bertscore_f1'] - basic_bertscore['bertscore_f1']) / basic_bertscore['bertscore_f1'] * 100):+.1f}%"
        },
        "optimized": {
            "value": optimized_bertscore["bertscore_f1"],
            "formatted": f"{optimized_bertscore['bertscore_f1']:.4f}",
            "vs_basic": f"{((optimized_bertscore['bertscore_f1'] - basic_bertscore['bertscore_f1']) / basic_bertscore['bertscore_f1'] * 100):+.1f}%"
        },
        "best": "basic" if basic_bertscore["bertscore_f1"] >= max(improved_bertscore["bertscore_f1"], optimized_bertscore["bertscore_f1"]) 
                else "improved" if improved_bertscore["bertscore_f1"] >= optimized_bertscore["bertscore_f1"] else "optimized"
    }
    
    # ì‘ë‹µ ê¸¸ì´ ë¹„êµ
    comparison["performance_comparison"]["response_length"] = {
        "basic": {
            "value": basic_medical["avg_response_length"],
            "formatted": f"{basic_medical['avg_response_length']:.1f}ì"
        },
        "improved": {
            "value": improved_medical["avg_response_length"],
            "formatted": f"{improved_medical['avg_response_length']:.1f}ì",
            "vs_basic": f"{((improved_medical['avg_response_length'] - basic_medical['avg_response_length']) / basic_medical['avg_response_length'] * 100):+.1f}%"
        },
        "optimized": {
            "value": optimized_medical["avg_response_length"],
            "formatted": f"{optimized_medical['avg_response_length']:.1f}ì",
            "vs_basic": f"{((optimized_medical['avg_response_length'] - basic_medical['avg_response_length']) / basic_medical['avg_response_length'] * 100):+.1f}%"
        },
        "best": "basic" if basic_medical["avg_response_length"] <= min(improved_medical["avg_response_length"], optimized_medical["avg_response_length"])
                else "improved" if improved_medical["avg_response_length"] <= optimized_medical["avg_response_length"] else "optimized"
    }
    
    # ì˜ë£Œ ìš©ì–´ í¬í•¨ë„ ë¹„êµ
    comparison["performance_comparison"]["medical_coverage"] = {
        "basic": {
            "value": basic_medical["medical_coverage"],
            "formatted": f"{basic_medical['medical_coverage']:.4f}"
        },
        "improved": {
            "value": improved_medical["medical_coverage"],
            "formatted": f"{improved_medical['medical_coverage']:.4f}",
            "vs_basic": f"{((improved_medical['medical_coverage'] - basic_medical['medical_coverage']) / basic_medical['medical_coverage'] * 100):+.1f}%"
        },
        "optimized": {
            "value": optimized_medical["medical_coverage"],
            "formatted": f"{optimized_medical['medical_coverage']:.4f}",
            "vs_basic": f"{((optimized_medical['medical_coverage'] - basic_medical['medical_coverage']) / basic_medical['medical_coverage'] * 100):+.1f}%"
        },
        "best": "basic" if basic_medical["medical_coverage"] >= max(improved_medical["medical_coverage"], optimized_medical["medical_coverage"])
                else "improved" if improved_medical["medical_coverage"] >= optimized_medical["medical_coverage"] else "optimized"
    }
    
    # ì•ˆì „ì„± ì ìˆ˜ ë¹„êµ
    comparison["performance_comparison"]["safety_score"] = {
        "basic": {
            "value": basic_medical["safety_score"],
            "formatted": f"{basic_medical['safety_score']:.4f}"
        },
        "improved": {
            "value": improved_medical["safety_score"],
            "formatted": f"{improved_medical['safety_score']:.4f}",
            "vs_basic": f"{((improved_medical['safety_score'] - basic_medical['safety_score']) / basic_medical['safety_score'] * 100):+.1f}%"
        },
        "optimized": {
            "value": optimized_medical["safety_score"],
            "formatted": f"{optimized_medical['safety_score']:.4f}",
            "vs_basic": f"{((optimized_medical['safety_score'] - basic_medical['safety_score']) / basic_medical['safety_score'] * 100):+.1f}%"
        },
        "best": "basic" if basic_medical["safety_score"] >= max(improved_medical["safety_score"], optimized_medical["safety_score"])
                else "improved" if improved_medical["safety_score"] >= optimized_medical["safety_score"] else "optimized"
    }
    
    # ì‘ë‹µ í’ˆì§ˆ ë¹„êµ
    comparison["performance_comparison"]["response_quality"] = {
        "basic": {
            "value": basic_medical["response_quality"],
            "formatted": f"{basic_medical['response_quality']:.4f}"
        },
        "improved": {
            "value": improved_medical["response_quality"],
            "formatted": f"{improved_medical['response_quality']:.4f}",
            "vs_basic": f"{((improved_medical['response_quality'] - basic_medical['response_quality']) / basic_medical['response_quality'] * 100):+.1f}%"
        },
        "optimized": {
            "value": optimized_medical["response_quality"],
            "formatted": f"{optimized_medical['response_quality']:.4f}",
            "vs_basic": f"{((optimized_medical['response_quality'] - basic_medical['response_quality']) / basic_medical['response_quality'] * 100):+.1f}%"
        },
        "best": "basic" if basic_medical["response_quality"] >= max(improved_medical["response_quality"], optimized_medical["response_quality"])
                else "improved" if improved_medical["response_quality"] >= optimized_medical["response_quality"] else "optimized"
    }
    
    # ì „ì²´ ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚° (ê° ë©”íŠ¸ë¦­ì˜ ìƒëŒ€ì  ì„±ëŠ¥)
    def calculate_overall_score(bertscore_f1, response_length, medical_coverage, safety_score, response_quality):
        # BERTScore F1 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        f1_score = bertscore_f1
        
        # ì‘ë‹µ ê¸¸ì´ (ì§§ì„ìˆ˜ë¡ ì¢‹ìŒ, 200ì ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”)
        length_score = max(0, 1 - abs(response_length - 200) / 200)
        
        # ì˜ë£Œ ìš©ì–´ í¬í•¨ë„ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        coverage_score = medical_coverage
        
        # ì•ˆì „ì„± ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        safety_score_norm = safety_score
        
        # ì‘ë‹µ í’ˆì§ˆ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        quality_score = response_quality
        
        # ê°€ì¤‘ í‰ê·  (BERTScoreì™€ ì•ˆì „ì„±ì´ ê°€ì¥ ì¤‘ìš”)
        overall = (f1_score * 0.3 + length_score * 0.1 + coverage_score * 0.1 + 
                  safety_score_norm * 0.3 + quality_score * 0.2)
        
        return overall
    
    basic_overall = calculate_overall_score(
        basic_bertscore["bertscore_f1"],
        basic_medical["avg_response_length"],
        basic_medical["medical_coverage"],
        basic_medical["safety_score"],
        basic_medical["response_quality"]
    )
    
    improved_overall = calculate_overall_score(
        improved_bertscore["bertscore_f1"],
        improved_medical["avg_response_length"],
        improved_medical["medical_coverage"],
        improved_medical["safety_score"],
        improved_medical["response_quality"]
    )
    
    optimized_overall = calculate_overall_score(
        optimized_bertscore["bertscore_f1"],
        optimized_medical["avg_response_length"],
        optimized_medical["medical_coverage"],
        optimized_medical["safety_score"],
        optimized_medical["response_quality"]
    )
    
    comparison["performance_comparison"]["overall_score"] = {
        "basic": {
            "value": basic_overall,
            "formatted": f"{basic_overall:.4f}"
        },
        "improved": {
            "value": improved_overall,
            "formatted": f"{improved_overall:.4f}",
            "vs_basic": f"{((improved_overall - basic_overall) / basic_overall * 100):+.1f}%"
        },
        "optimized": {
            "value": optimized_overall,
            "formatted": f"{optimized_overall:.4f}",
            "vs_basic": f"{((optimized_overall - basic_overall) / basic_overall * 100):+.1f}%"
        },
        "best": "basic" if basic_overall >= max(improved_overall, optimized_overall)
                else "improved" if improved_overall >= optimized_overall else "optimized"
    }
    
    return comparison

def print_triple_comparison(comparison):
    """3ê°œ í”„ë¡¬í”„íŠ¸ ë¹„êµ ê²°ê³¼ ì¶œë ¥"""
    print("ğŸ† 3ê°œ í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
    print("="*80)
    
    perf = comparison["performance_comparison"]
    
    # BERTScore F1
    print(f"\nğŸ“Š BERTScore F1 (ì •í™•ë„)")
    print(f"  Basic:    {perf['bertscore_f1']['basic']['formatted']}")
    print(f"  Improved: {perf['bertscore_f1']['improved']['formatted']} ({perf['bertscore_f1']['improved']['vs_basic']})")
    print(f"  Optimized: {perf['bertscore_f1']['optimized']['formatted']} ({perf['bertscore_f1']['optimized']['vs_basic']})")
    print(f"  ğŸ¥‡ ìµœê³ : {perf['bertscore_f1']['best'].upper()}")
    
    # ì‘ë‹µ ê¸¸ì´
    print(f"\nğŸ“ ì‘ë‹µ ê¸¸ì´")
    print(f"  Basic:    {perf['response_length']['basic']['formatted']}")
    print(f"  Improved: {perf['response_length']['improved']['formatted']} ({perf['response_length']['improved']['vs_basic']})")
    print(f"  Optimized: {perf['response_length']['optimized']['formatted']} ({perf['response_length']['optimized']['vs_basic']})")
    print(f"  ğŸ¥‡ ìµœê³ : {perf['response_length']['best'].upper()}")
    
    # ì˜ë£Œ ìš©ì–´ í¬í•¨ë„
    print(f"\nğŸ¥ ì˜ë£Œ ìš©ì–´ í¬í•¨ë„")
    print(f"  Basic:    {perf['medical_coverage']['basic']['formatted']}")
    print(f"  Improved: {perf['medical_coverage']['improved']['formatted']} ({perf['medical_coverage']['improved']['vs_basic']})")
    print(f"  Optimized: {perf['medical_coverage']['optimized']['formatted']} ({perf['medical_coverage']['optimized']['vs_basic']})")
    print(f"  ğŸ¥‡ ìµœê³ : {perf['medical_coverage']['best'].upper()}")
    
    # ì•ˆì „ì„± ì ìˆ˜
    print(f"\nğŸ›¡ï¸ ì•ˆì „ì„± ì ìˆ˜")
    print(f"  Basic:    {perf['safety_score']['basic']['formatted']}")
    print(f"  Improved: {perf['safety_score']['improved']['formatted']} ({perf['safety_score']['improved']['vs_basic']})")
    print(f"  Optimized: {perf['safety_score']['optimized']['formatted']} ({perf['safety_score']['optimized']['vs_basic']})")
    print(f"  ğŸ¥‡ ìµœê³ : {perf['safety_score']['best'].upper()}")
    
    # ì‘ë‹µ í’ˆì§ˆ
    print(f"\nâ­ ì‘ë‹µ í’ˆì§ˆ")
    print(f"  Basic:    {perf['response_quality']['basic']['formatted']}")
    print(f"  Improved: {perf['response_quality']['improved']['formatted']} ({perf['response_quality']['improved']['vs_basic']})")
    print(f"  Optimized: {perf['response_quality']['optimized']['formatted']} ({perf['response_quality']['optimized']['vs_basic']})")
    print(f"  ğŸ¥‡ ìµœê³ : {perf['response_quality']['best'].upper()}")
    
    # ì „ì²´ ì„±ëŠ¥ ì ìˆ˜
    print(f"\nğŸ† ì „ì²´ ì„±ëŠ¥ ì ìˆ˜ (ê°€ì¤‘ í‰ê· )")
    print(f"  Basic:    {perf['overall_score']['basic']['formatted']}")
    print(f"  Improved: {perf['overall_score']['improved']['formatted']} ({perf['overall_score']['improved']['vs_basic']})")
    print(f"  Optimized: {perf['overall_score']['optimized']['formatted']} ({perf['overall_score']['optimized']['vs_basic']})")
    print(f"  ğŸ¥‡ ìµœê³ : {perf['overall_score']['best'].upper()}")
    
    # ìµœì¢… ê¶Œì¥ì‚¬í•­
    best_prompt = perf['overall_score']['best']
    print(f"\nğŸ’¡ ìµœì¢… ê¶Œì¥ì‚¬í•­")
    print(f"  ğŸ¯ ì¶”ì²œ í”„ë¡¬í”„íŠ¸: {best_prompt.upper()}")
    
    if best_prompt == "basic":
        print("  ğŸ“ ì´ìœ : ê°€ì¥ ì•ˆì •ì ì´ê³  ê· í˜•ì¡íŒ ì„±ëŠ¥")
    elif best_prompt == "improved":
        print("  ğŸ“ ì´ìœ : ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ê°€ ì˜¤íˆë ¤ ì„±ëŠ¥ í–¥ìƒ")
    else:
        print("  ğŸ“ ì´ìœ : Basicì˜ ì¥ì ì„ ìœ ì§€í•˜ë©´ì„œ ê°œì„ ëœ ì„±ëŠ¥")

def main():
    print("3ê°œ í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ ì‹œì‘...")
    
    # í‰ê°€ ê²°ê³¼ ë¡œë“œ
    results = load_evaluation_results()
    
    if len(results) != 3:
        print(f"âŒ 3ê°œ í”„ë¡¬í”„íŠ¸ ê²°ê³¼ê°€ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬: {len(results)}ê°œ")
        print("í•„ìš”í•œ íŒŒì¼:")
        print("- results/peft_model_evaluation.json (Basic)")
        print("- results/improved_prompt_evaluation.json (Improved)")
        print("- results/optimized_prompt_evaluation.json (Optimized)")
        return
    
    # ë¹„êµ ë¶„ì„
    comparison = compare_three_prompts(results)
    
    # ê²°ê³¼ ì¶œë ¥
    print_triple_comparison(comparison)
    
    # ê²°ê³¼ ì €ì¥
    with open("results/triple_prompt_comparison.json", "w", encoding="utf-8") as f:
        json.dump(comparison, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ë¹„êµ ê²°ê³¼ê°€ results/triple_prompt_comparison.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()

