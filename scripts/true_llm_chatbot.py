#!/usr/bin/env python3
"""
ì§„ì§œ LLM ì±—ë´‡
- ê·œì¹™ ê¸°ë°˜ì´ ì•„ë‹Œ ì‹¤ì œ ëª¨ë¸ ê¸°ë°˜ ì‘ë‹µ
- ëª¨ë“  ì§ˆë¬¸ì— ììœ ë¡­ê²Œ ë‹µë³€
- ëª¨ë¥´ë©´ "ëª¨ë¥¸ë‹¤"ê³  ì •ì§í•˜ê²Œ ë‹µë³€
"""

import os
import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import warnings
warnings.filterwarnings("ignore")

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["TOKENIZERS_PARALLELISM"] = "false"

class TrueLLMChatbot:
    def __init__(self, model_path="models/true_llm_chatbot"):
        """ì§„ì§œ LLM ì±—ë´‡ ì´ˆê¸°í™”"""
        self.model_path = model_path
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ëª¨ë¸ ë¡œë”©
        self._load_model()
        
    def _load_model(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©"""
        print(f"\nğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_path}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None
            )
            
            if self.device == "cpu":
                self.model = self.model.to(self.device)
            
            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            print("ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self._load_base_model()
    
    def _load_base_model(self):
        """ê¸°ë³¸ ëª¨ë¸ ë¡œë”© (í•™ìŠµëœ ëª¨ë¸ì´ ì—†ì„ ê²½ìš°)"""
        print(f"\nğŸ“¥ ê¸°ë³¸ ëª¨ë¸ ë¡œë”© ì¤‘: skt/kogpt2-base-v2")
        
        self.tokenizer = AutoTokenizer.from_pretrained("skt/kogpt2-base-v2")
        self.model = AutoModelForCausalLM.from_pretrained(
            "skt/kogpt2-base-v2",
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto" if self.device == "cuda" else None
        )
        
        if self.device == "cpu":
            self.model = self.model.to(self.device)
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        print("âœ… ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def generate_response(self, question: str) -> str:
        """ëª¨ë¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        try:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = f"ì‚¬ìš©ì: {question}\nì˜ë£Œì§„:"
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
            
            # ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=inputs.shape[1] + 200,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    attention_mask=torch.ones_like(inputs),
                    repetition_penalty=1.2,
                    no_repeat_ngram_size=3
                )
            
            # ë””ì½”ë”©
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = response.replace(prompt, "").strip()
            
            # ì‘ë‹µ ì •ë¦¬
            if "ì˜ë£Œì§„:" in response:
                response = response.split("ì˜ë£Œì§„:")[-1].strip()
            
            # ë°˜ë³µ ì œê±°
            lines = response.split('\n')
            cleaned_lines = []
            for line in lines:
                line = line.strip()
                if line and not any(word in line for word in ["ì‚¬ìš©ì:", "ì˜ë£Œì§„:", "ê°„í˜¸ì‚¬:", "ì§„ë£Œ:"]):
                    cleaned_lines.append(line)
            
            if cleaned_lines:
                return cleaned_lines[0]
            else:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
                
        except Exception as e:
            print(f"âš ï¸ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
    
    def get_response(self, question: str) -> tuple:
        """ì‘ë‹µ ìƒì„± ë° ì •ë³´ ë°˜í™˜"""
        start_time = time.time()
        
        # ëª¨ë¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
        response = self.generate_response(question)
        
        response_time = time.time() - start_time
        
        return response, "model_based", response_time
    
    def print_welcome(self):
        """í™˜ì˜ ë©”ì‹œì§€ ì¶œë ¥"""
        print("=" * 60)
        print("ğŸ¤– ì§„ì§œ LLM ì˜ë£Œ ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!")
        print("=" * 60)
        print("ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"   â€¢ ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   â€¢ ëª¨ë¸: {self.model_path}")
        print(f"   â€¢ ì‘ë‹µ ë°©ì‹: ì‹¤ì œ LLM ëª¨ë¸")
        print("\nğŸ’¬ ì–´ë–¤ ì§ˆë¬¸ì´ë“  ììœ ë¡­ê²Œ ë¬¼ì–´ë³´ì„¸ìš”!")
        print("   ëª¨ë¥´ëŠ” ê²ƒì´ ìˆìœ¼ë©´ ì†”ì§í•˜ê²Œ 'ëª¨ë¥¸ë‹¤'ê³  ë‹µë³€í•©ë‹ˆë‹¤.")
        print("   ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print("=" * 60)
    
    def print_response(self, response: str, source: str, response_time: float):
        """ì‘ë‹µ ì¶œë ¥"""
        print(f"\nğŸ¤– ì±—ë´‡ ì‘ë‹µ:")
        print("-" * 40)
        print(response)
        print("-" * 40)
        print(f"ğŸ“Š ì‘ë‹µ ì •ë³´:")
        print(f"   â€¢ ì‘ë‹µ ì‹œê°„: {response_time:.3f}ì´ˆ")
        print(f"   â€¢ ì‘ë‹µ ì†ŒìŠ¤: {source}")
        print()
    
    def run(self):
        """ì±—ë´‡ ì‹¤í–‰"""
        self.print_welcome()
        
        while True:
            try:
                # ì‚¬ìš©ì ì…ë ¥
                user_input = input("ğŸ‘¤ ì‚¬ìš©ì: ").strip()
                
                # ì¢…ë£Œ ëª…ë ¹ì–´ ì²´í¬
                if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                    print("\nğŸ‘‹ ì§„ì§œ LLM ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                    break
                
                if not user_input:
                    print("âŒ ì…ë ¥ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                # ì±—ë´‡ ì‘ë‹µ ìƒì„±
                response, source, response_time = self.get_response(user_input)
                
                # ì‘ë‹µ ì¶œë ¥
                self.print_response(response, source, response_time)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì§„ì§œ LLM ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                print("ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    chatbot = TrueLLMChatbot()
    chatbot.run()

if __name__ == "__main__":
    main()
