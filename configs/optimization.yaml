# 경량화 최적화 설정

# 양자화 설정
quantization:
  # 양자화 방법
  methods:
    - "int8"  # INT8 동적 양자화
    - "int4"  # INT4 양자화 (bitsandbytes)
  
  # INT8 설정
  int8:
    enabled: true
    dtype: "qint8"
    target_modules: ["Linear", "Embedding", "LayerNorm"]
  
  # INT4 설정
  int4:
    enabled: true
    compute_dtype: "float16"
    use_double_quant: true
    quant_type: "nf4"

# Pruning 설정
pruning:
  enabled: true
  sparsity: 0.2  # 20% 가중치 제거
  method: "magnitude"  # magnitude, gradient
  target_modules: ["Linear"]

# Knowledge Distillation 설정
knowledge_distillation:
  enabled: true
  teacher_model: "42dot/42dot_LLM-SFT-1.3B"
  student_model: "42dot/42dot_LLM-SFT-1.3B"
  temperature: 4.0
  alpha: 0.7
  beta: 0.3

# LoRA 설정
lora:
  enabled: true
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]

# 성능 목표
performance_targets:
  size_reduction_percent: 50  # 50% 크기 감소
  speed_improvement_percent: 100  # 2배 속도 향상
  memory_reduction_percent: 70  # 70% 메모리 감소
  accuracy_threshold: 0.9  # 90% 정확도 유지

# 출력 설정
output:
  quantized_models_dir: "models/quantized"
  pruned_models_dir: "models/pruned"
  distilled_models_dir: "models/distilled"
  lora_models_dir: "models/lora"
  results_dir: "outputs/optimization"
