"""
ëª¨ë¸ ì„ íƒ ë° í‰ê°€ë¥¼ ìœ„í•œ í´ë˜ìŠ¤
"""

import json
import torch
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from transformers import AutoTokenizer, AutoModelForCausalLM

@dataclass
class ModelSpecs:
    """ëª¨ë¸ ì‚¬ì–‘ ì •ë³´"""
    name: str
    size_gb: float
    parameter_count: int
    vocab_size: int
    max_length: int
    inference_speed: float  # tokens per second
    korean_score: float  # 0-1
    memory_usage: float  # GB
    load_time: float  # seconds

@dataclass
class ModelComparison:
    """ëª¨ë¸ ë¹„êµ ê²°ê³¼"""
    model1: ModelSpecs
    model2: ModelSpecs
    winner: str
    reasoning: str
    recommendation: str

class ModelSelector:
    """ëª¨ë¸ ì„ íƒ ë° í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, config_path: str = "configs/config.yaml"):
        self.config_path = config_path
        self.available_models = [
            "psymon/KoLlama2-7b",
            "beomi/KcBERT-base"
        ]
    
    def load_model_specs(self, results_path: str = "outputs/model_comparison_results.json") -> Dict[str, ModelSpecs]:
        """ëª¨ë¸ ë¹„êµ ê²°ê³¼ë¥¼ ë¡œë“œí•˜ì—¬ ModelSpecs ê°ì²´ë¡œ ë³€í™˜"""
        results_path = Path(results_path)
        
        if not results_path.exists():
            raise FileNotFoundError(f"ëª¨ë¸ ë¹„êµ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {results_path}")
        
        with open(results_path, 'r', encoding='utf-8') as f:
            results = json.load(f)
        
        model_specs = {}
        
        for model_name, result in results.items():
            if "error" in result.get("model_info", {}):
                continue
            
            info = result["model_info"]
            speed = result.get("speed_benchmark", {})
            korean = result.get("korean_understanding", {})
            
            model_specs[model_name] = ModelSpecs(
                name=model_name,
                size_gb=info.get("model_size_gb", 0),
                parameter_count=info.get("parameter_count", 0),
                vocab_size=info.get("vocab_size", 0),
                max_length=info.get("max_length", 0),
                inference_speed=speed.get("tokens_per_second", 0),
                korean_score=korean.get("avg_keyword_score", 0),
                memory_usage=0,  # ë³„ë„ ì¸¡ì • í•„ìš”
                load_time=0  # ë³„ë„ ì¸¡ì • í•„ìš”
            )
        
        return model_specs
    
    def compare_models(self, model1_name: str, model2_name: str) -> ModelComparison:
        """ë‘ ëª¨ë¸ì„ ë¹„êµí•˜ì—¬ ì¶”ì²œ ëª¨ë¸ì„ ê²°ì •"""
        try:
            model_specs = self.load_model_specs()
            
            if model1_name not in model_specs or model2_name not in model_specs:
                raise ValueError("ëª¨ë¸ ì‚¬ì–‘ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            model1 = model_specs[model1_name]
            model2 = model_specs[model2_name]
            
            # ë¹„êµ ê¸°ì¤€ë³„ ì ìˆ˜ ê³„ì‚°
            scores = {
                model1_name: 0,
                model2_name: 0
            }
            
            # 1. ëª¨ë¸ í¬ê¸° (ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ) - 25% ê°€ì¤‘ì¹˜
            if model1.size_gb < model2.size_gb:
                scores[model1_name] += 25
            elif model2.size_gb < model1.size_gb:
                scores[model2_name] += 25
            else:
                scores[model1_name] += 12.5
                scores[model2_name] += 12.5
            
            # 2. ì¶”ë¡  ì†ë„ (ë¹ ë¥¼ìˆ˜ë¡ ì¢‹ìŒ) - 25% ê°€ì¤‘ì¹˜
            if model1.inference_speed > model2.inference_speed:
                scores[model1_name] += 25
            elif model2.inference_speed > model1.inference_speed:
                scores[model2_name] += 25
            else:
                scores[model1_name] += 12.5
                scores[model2_name] += 12.5
            
            # 3. í•œêµ­ì–´ ì´í•´ ëŠ¥ë ¥ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ) - 30% ê°€ì¤‘ì¹˜
            if model1.korean_score > model2.korean_score:
                scores[model1_name] += 30
            elif model2.korean_score > model1.korean_score:
                scores[model2_name] += 30
            else:
                scores[model1_name] += 15
                scores[model2_name] += 15
            
            # 4. íŒŒë¼ë¯¸í„° ìˆ˜ (ì ë‹¹í•œ ìˆ˜ì¤€ì´ ì¢‹ìŒ) - 20% ê°€ì¤‘ì¹˜
            # 7B-12B ë²”ìœ„ì—ì„œ 7Bê°€ ë” íš¨ìœ¨ì 
            if 6e9 <= model1.parameter_count <= 8e9 and not (6e9 <= model2.parameter_count <= 8e9):
                scores[model1_name] += 20
            elif 6e9 <= model2.parameter_count <= 8e9 and not (6e9 <= model1.parameter_count <= 8e9):
                scores[model2_name] += 20
            else:
                scores[model1_name] += 10
                scores[model2_name] += 10
            
            # ìŠ¹ì ê²°ì •
            winner = model1_name if scores[model1_name] > scores[model2_name] else model2_name
            
            # ì¶”ì²œ ì´ìœ  ìƒì„±
            reasoning = self._generate_reasoning(model1, model2, scores)
            
            # ìµœì¢… ì¶”ì²œì‚¬í•­
            recommendation = self._generate_recommendation(winner, model1, model2)
            
            return ModelComparison(
                model1=model1,
                model2=model2,
                winner=winner,
                reasoning=reasoning,
                recommendation=recommendation
            )
            
        except Exception as e:
            raise Exception(f"ëª¨ë¸ ë¹„êµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    def _generate_reasoning(self, model1: ModelSpecs, model2: ModelSpecs, scores: Dict[str, float]) -> str:
        """ë¹„êµ ê²°ê³¼ì— ëŒ€í•œ ìƒì„¸í•œ ì´ìœ  ìƒì„±"""
        reasoning_parts = []
        
        # í¬ê¸° ë¹„êµ
        if model1.size_gb < model2.size_gb:
            reasoning_parts.append(f"{model1.name}ì´ {model2.size_gb - model1.size_gb:.1f}GB ë” ì‘ìŠµë‹ˆë‹¤.")
        elif model2.size_gb < model1.size_gb:
            reasoning_parts.append(f"{model2.name}ì´ {model1.size_gb - model2.size_gb:.1f}GB ë” ì‘ìŠµë‹ˆë‹¤.")
        
        # ì†ë„ ë¹„êµ
        if model1.inference_speed > model2.inference_speed:
            reasoning_parts.append(f"{model1.name}ì´ {model1.inference_speed - model2.inference_speed:.1f} í† í°/ì´ˆ ë” ë¹ ë¦…ë‹ˆë‹¤.")
        elif model2.inference_speed > model1.inference_speed:
            reasoning_parts.append(f"{model2.name}ì´ {model2.inference_speed - model1.inference_speed:.1f} í† í°/ì´ˆ ë” ë¹ ë¦…ë‹ˆë‹¤.")
        
        # í•œêµ­ì–´ ëŠ¥ë ¥ ë¹„êµ
        if model1.korean_score > model2.korean_score:
            reasoning_parts.append(f"{model1.name}ì˜ í•œêµ­ì–´ ì´í•´ ì ìˆ˜ê°€ {model1.korean_score - model2.korean_score:.2f} ë” ë†’ìŠµë‹ˆë‹¤.")
        elif model2.korean_score > model1.korean_score:
            reasoning_parts.append(f"{model2.name}ì˜ í•œêµ­ì–´ ì´í•´ ì ìˆ˜ê°€ {model2.korean_score - model1.korean_score:.2f} ë” ë†’ìŠµë‹ˆë‹¤.")
        
        return " ".join(reasoning_parts)
    
    def _generate_recommendation(self, winner: str, model1: ModelSpecs, model2: ModelSpecs) -> str:
        """ìµœì¢… ì¶”ì²œì‚¬í•­ ìƒì„±"""
        winner_model = model1 if winner == model1.name else model2
        
        recommendations = [
            f"ì¶”ì²œ ëª¨ë¸: {winner}",
            f"ëª¨ë¸ í¬ê¸°: {winner_model.size_gb:.1f}GB",
            f"ì¶”ë¡  ì†ë„: {winner_model.inference_speed:.1f} í† í°/ì´ˆ",
            f"í•œêµ­ì–´ ì ìˆ˜: {winner_model.korean_score:.2f}",
            "",
            "ë‹¤ìŒ ë‹¨ê³„:",
            "1. ì„ íƒëœ ëª¨ë¸ë¡œ ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •",
            "2. ì˜ë£Œ ë„ë©”ì¸ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹ ì¤€ë¹„",
            "3. ê²½ëŸ‰í™” ê¸°ë²• ì ìš© ê³„íš ìˆ˜ë¦½"
        ]
        
        return "\n".join(recommendations)
    
    def save_comparison_report(self, comparison: ModelComparison, output_path: str = "outputs/model_selection_report.md"):
        """ëª¨ë¸ ë¹„êµ ë³´ê³ ì„œë¥¼ Markdown í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        report = f"""# ëª¨ë¸ ì„ íƒ ë³´ê³ ì„œ

## ğŸ“Š ëª¨ë¸ ë¹„êµ ê²°ê³¼

### {comparison.model1.name}
- **í¬ê¸°**: {comparison.model1.size_gb:.1f}GB
- **íŒŒë¼ë¯¸í„° ìˆ˜**: {comparison.model1.parameter_count:,}
- **ì¶”ë¡  ì†ë„**: {comparison.model1.inference_speed:.1f} í† í°/ì´ˆ
- **í•œêµ­ì–´ ì ìˆ˜**: {comparison.model1.korean_score:.2f}

### {comparison.model2.name}
- **í¬ê¸°**: {comparison.model2.size_gb:.1f}GB
- **íŒŒë¼ë¯¸í„° ìˆ˜**: {comparison.model2.parameter_count:,}
- **ì¶”ë¡  ì†ë„**: {comparison.model2.inference_speed:.1f} í† í°/ì´ˆ
- **í•œêµ­ì–´ ì ìˆ˜**: {comparison.model2.korean_score:.2f}

## ğŸ† ì¶”ì²œ ëª¨ë¸: {comparison.winner}

### ğŸ“‹ ì„ íƒ ì´ìœ 
{comparison.reasoning}

### ğŸ¯ ì¶”ì²œì‚¬í•­
{comparison.recommendation}

## ğŸ“… ë‹¤ìŒ ë‹¨ê³„
1. **ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ êµ¬ì¶•**: ì„ íƒëœ ëª¨ë¸ë¡œ ê¸°ë³¸ ì„±ëŠ¥ ì¸¡ì •
2. **ì˜ë£Œ ë„ë©”ì¸ íŒŒì¸íŠœë‹**: AI Hub ì˜ë£Œ ë°ì´í„° í™œìš©
3. **ê²½ëŸ‰í™” ê¸°ë²• ì ìš©**: Quantization, Pruning, Knowledge Distillation
4. **ì„±ëŠ¥ ìµœì í™”**: ì¶”ë¡  ì†ë„ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°œì„ 

---
*ìƒì„±ì¼ì‹œ: {Path().cwd()}*
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“‹ ëª¨ë¸ ì„ íƒ ë³´ê³ ì„œê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    selector = ModelSelector()
    
    try:
        # ëª¨ë¸ ë¹„êµ ìˆ˜í–‰
        comparison = selector.compare_models(
            "beomi/KULLM-2-7B",
            "EleutherAI/polyglot-ko-12.8b"
        )
        
        # ê²°ê³¼ ì¶œë ¥
        print("ğŸ† ëª¨ë¸ ì„ íƒ ê²°ê³¼")
        print("=" * 50)
        print(f"ì¶”ì²œ ëª¨ë¸: {comparison.winner}")
        print(f"ì„ íƒ ì´ìœ : {comparison.reasoning}")
        print("\nì¶”ì²œì‚¬í•­:")
        print(comparison.recommendation)
        
        # ë³´ê³ ì„œ ì €ì¥
        selector.save_comparison_report(comparison)
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print("ë¨¼ì € scripts/model_comparison.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ ë¹„êµ ë°ì´í„°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
