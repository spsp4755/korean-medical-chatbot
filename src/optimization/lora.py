"""
LoRA (Low-Rank Adaptation) νμΈνλ‹ λ¨λ“
ν¨μ¨μ μΈ λ„λ©”μΈ νΉν™” ν•™μµμ„ ν†µν• λ¨λΈ μµμ ν™”
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
import numpy as np
from typing import Dict, Any, List, Tuple, Optional
import time
import psutil
import os
from pathlib import Path
import json
import yaml
from datasets import Dataset
import logging
import warnings
warnings.filterwarnings("ignore")

# MPS λΉ„ν™μ„±ν™”
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"

# λ΅κΉ… μ„¤μ •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LoRAFineTuner:
    """LoRA κΈ°λ° ν¨μ¨μ  νμΈνλ‹"""
    
    def __init__(self, model_name: str, output_dir: str = "models/lora"):
        self.model_name = model_name
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # λ¨λΈ λ΅λ“
        self._load_model()
        
        # μ„±λ¥ μΈ΅μ •
        self._measure_baseline_performance()
        
    def _load_model(self):
        """λ¨λΈ λ΅λ“"""
        print(f"π”„ λ¨λΈ λ΅λ”© μ¤‘: {self.model_name}")
        self.base_model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float32,
            device_map="cpu"
        )
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        
        # ν¨λ”© ν† ν° μ„¤μ •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print(f"β… λ¨λΈ λ΅λ”© μ™„λ£")
    
    def _measure_baseline_performance(self):
        """λ² μ΄μ¤λΌμΈ μ„±λ¥ μΈ΅μ •"""
        print("π“ λ² μ΄μ¤λΌμΈ μ„±λ¥ μΈ΅μ • μ¤‘...")
        
        self.base_size = self._get_model_size(self.base_model)
        self.base_speed = self._benchmark_inference(self.base_model, self.tokenizer)
        self.base_memory = self._get_memory_usage()
        
        print(f"π“ λ² μ΄μ¤ ν¬κΈ°: {self.base_size:.2f}MB")
        print(f"π§  λ² μ΄μ¤ μ†λ„: {self.base_speed:.2f}μ΄")
        print(f"π’Ύ λ² μ΄μ¤ λ©”λ¨λ¦¬: {self.base_memory:.2f}MB")
    
    def _get_model_size(self, model) -> float:
        """λ¨λΈ ν¬κΈ° μΈ΅μ • (MB)"""
        param_size = 0
        buffer_size = 0
        
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()
        
        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()
        
        size_all_mb = (param_size + buffer_size) / 1024**2
        return size_all_mb
    
    def _benchmark_inference(self, model, tokenizer, num_runs: int = 3) -> float:
        """μ¶”λ΅  μ†λ„ λ²¤μΉλ§ν¬ (μ΄)"""
        model.eval()
        test_text = "μ•λ…•ν•μ„Έμ”. μλ£ μƒλ‹΄μ„ λ°›κ³  μ‹¶μµλ‹λ‹¤."
        
        inputs = tokenizer(test_text, return_tensors="pt", padding=True, truncation=True, max_length=128)
        
        # μ›λ°μ—…
        with torch.no_grad():
            for _ in range(2):
                _ = model.generate(
                    inputs.input_ids,
                    max_length=inputs.input_ids.shape[1] + 20,
                    num_return_sequences=1,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id
                )
        
        # μ‹¤μ  μΈ΅μ •
        start_time = time.time()
        with torch.no_grad():
            for _ in range(num_runs):
                _ = model.generate(
                    inputs.input_ids,
                    max_length=inputs.input_ids.shape[1] + 20,
                    num_return_sequences=1,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id
                )
        end_time = time.time()
        
        avg_time = (end_time - start_time) / num_runs
        return avg_time
    
    def _get_memory_usage(self) -> float:
        """λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μΈ΅μ • (MB)"""
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / 1024**2
        else:
            process = psutil.Process(os.getpid())
            return process.memory_info().rss / 1024**2
    
    def _prepare_medical_data(self) -> Dataset:
        """μλ£ λ°μ΄ν„° μ¤€λΉ„"""
        print("π“ μλ£ λ°μ΄ν„° μ¤€λΉ„ μ¤‘...")
        
        medical_data = []
        medical_files = [
            "data/processed/medical_data.json",
            "data/processed/splits/essential_medical_test.json",
            "data/processed/splits/professional_medical_test.json"
        ]
        
        for file_path in medical_files:
            if Path(file_path).exists():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if isinstance(data, list):
                            medical_data.extend(data)
                        else:
                            medical_data.append(data)
                    print(f"β… λ΅λ“λ¨: {file_path}")
                except Exception as e:
                    logger.warning(f"νμΌ λ΅λ“ μ‹¤ν¨: {file_path} - {e}")
        
        # ν…μ¤νΈ μ¶”μ¶ λ° μ „μ²λ¦¬
        texts = []
        for item in medical_data[:300]:  # 300κ°λ΅ μ ν•
            if isinstance(item, dict):
                for key in ['text', 'content', 'question', 'answer', 'instruction', 'input', 'output']:
                    if key in item and isinstance(item[key], str) and len(item[key]) > 10:
                        texts.append(item[key])
                        break
            elif isinstance(item, str) and len(item) > 10:
                texts.append(item)
        
        # μ¤‘λ³µ μ κ±°
        texts = list(set(texts))
        print(f"π“ μ „μ²λ¦¬λ ν…μ¤νΈ: {len(texts)}κ°")
        
        if len(texts) == 0:
            # λ”λ―Έ λ°μ΄ν„° μƒμ„±
            texts = [
                "μ•λ…•ν•μ„Έμ”. μλ£ μƒλ‹΄μ„ λ°›κ³  μ‹¶μµλ‹λ‹¤.",
                "λ¨Έλ¦¬κ°€ μ•„ν”λ° μ–΄λ–»κ² ν•΄μ•Ό ν• κΉμ”?",
                "κ°κΈ° μ¦μƒμ΄ μλ”λ° λ³‘μ›μ— κ°€μ•Ό ν• κΉμ”?",
                "λ³µν†µμ΄ μ‹¬ν•λ° μ‘κΈ‰μ‹¤μ— κ°€μ•Ό ν• κΉμ”?",
                "ν”Όλ¶€μ— λ°μ§„μ΄ μƒκ²Όλ”λ° μ›μΈμ΄ λ­κΉμ”?"
            ] * 30
        
        return Dataset.from_dict({"text": texts})
    
    def _tokenize_data(self, dataset: Dataset) -> Dataset:
        """λ°μ΄ν„° ν† ν¬λ‚μ΄μ§•"""
        def tokenize_function(examples):
            return self.tokenizer(
                examples["text"],
                truncation=True,
                padding=True,
                max_length=128,
                return_tensors="pt"
            )
        
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names
        )
        
        return tokenized_dataset
    
    def _create_lora_config(self, r: int = 16, lora_alpha: int = 32, lora_dropout: float = 0.1) -> LoraConfig:
        """LoRA μ„¤μ • μƒμ„±"""
        # KoGPT2 λ¨λΈμ— λ§λ” target modules
        target_modules = ["c_attn", "c_proj", "c_fc"]  # GPT-2 κ³„μ—΄ λ¨λΈμ©
        
        config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=r,  # rank
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            target_modules=target_modules,
            bias="none",
        )
        return config
    
    def run_lora_finetuning(self, 
                           r: int = 16,
                           lora_alpha: int = 32,
                           lora_dropout: float = 0.1,
                           num_epochs: int = 3,
                           batch_size: int = 2,
                           learning_rate: float = 2e-4) -> Dict[str, Any]:
        """LoRA νμΈνλ‹ μ‹¤ν–‰"""
        print("π€ LoRA νμΈνλ‹ μ‹μ‘")
        print("=" * 60)
        
        # LoRA μ„¤μ • μƒμ„±
        lora_config = self._create_lora_config(r, lora_alpha, lora_dropout)
        print(f"π“ LoRA μ„¤μ •: r={r}, alpha={lora_alpha}, dropout={lora_dropout}")
        
        # LoRA λ¨λΈ μƒμ„±
        print("π”§ LoRA λ¨λΈ μƒμ„± μ¤‘...")
        lora_model = get_peft_model(self.base_model, lora_config)
        
        # LoRA λ¨λΈ ν¬κΈ° μΈ΅μ •
        lora_size = self._get_model_size(lora_model)
        print(f"π“ LoRA λ¨λΈ ν¬κΈ°: {lora_size:.2f}MB")
        print(f"π“ ν¬κΈ° μ¦κ°€: {((lora_size - self.base_size) / self.base_size * 100):.2f}%")
        
        # λ°μ΄ν„° μ¤€λΉ„
        dataset = self._prepare_medical_data()
        tokenized_dataset = self._tokenize_data(dataset)
        
        # Data Collator μ„¤μ •
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False
        )
        
        # Training Arguments μ„¤μ •
        training_args = TrainingArguments(
            output_dir=str(self.output_dir / "training_output"),
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            warmup_steps=50,
            weight_decay=0.01,
            logging_dir=str(self.output_dir / "logs"),
            logging_steps=5,
            save_steps=100,
            eval_strategy="no",
            save_total_limit=1,
            learning_rate=learning_rate,
            fp16=False,
            dataloader_pin_memory=False,
            remove_unused_columns=False,
            report_to=None,
            use_cpu=True,
            dataloader_num_workers=0,
        )
        
        # Trainer μ΄κΈ°ν™”
        trainer = Trainer(
            model=lora_model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer,
        )
        
        # LoRA νμΈνλ‹ μ‹¤ν–‰
        print("π“ LoRA νμΈνλ‹ ν•™μµ μ‹μ‘...")
        start_time = time.time()
        
        try:
            trainer.train()
            training_time = time.time() - start_time
            print(f"β… LoRA νμΈνλ‹ μ™„λ£: {training_time:.2f}μ΄")
            
            # LoRA λ¨λΈ μ €μ¥
            lora_model_path = self.output_dir / "lora_model"
            trainer.save_model(str(lora_model_path))
            self.tokenizer.save_pretrained(str(lora_model_path))
            
            # νμΈνλ‹λ λ¨λΈ μ„±λ¥ μΈ΅μ •
            print("π“ νμΈνλ‹λ λ¨λΈ μ„±λ¥ μΈ΅μ • μ¤‘...")
            finetuned_speed = self._benchmark_inference(lora_model, self.tokenizer)
            finetuned_memory = self._get_memory_usage()
            
            # κ²°κ³Ό κ³„μ‚°
            speed_improvement = ((self.base_speed - finetuned_speed) / self.base_speed) * 100
            memory_change = ((self.base_memory - finetuned_memory) / self.base_memory) * 100
            
            results = {
                "lora_config": {
                    "r": r,
                    "lora_alpha": lora_alpha,
                    "lora_dropout": lora_dropout,
                    "num_epochs": num_epochs,
                    "batch_size": batch_size,
                    "learning_rate": learning_rate,
                    "training_time_sec": training_time
                },
                "base_model": {
                    "size_mb": self.base_size,
                    "inference_speed_sec": self.base_speed,
                    "memory_usage_mb": self.base_memory
                },
                "lora_model": {
                    "size_mb": lora_size,
                    "inference_speed_sec": finetuned_speed,
                    "memory_usage_mb": finetuned_memory,
                    "size_increase_percent": ((lora_size - self.base_size) / self.base_size * 100),
                    "speed_improvement_percent": speed_improvement,
                    "memory_change_percent": memory_change
                }
            }
            
            print(f"β… LoRA νμΈνλ‹ μ™„λ£")
            print(f"π“ ν¬κΈ°: {lora_size:.2f}MB ({((lora_size - self.base_size) / self.base_size * 100):.2f}% μ¦κ°€)")
            print(f"π§  μ†λ„: {finetuned_speed:.2f}μ΄ ({speed_improvement:.1f}% ν–¥μƒ)")
            print(f"π’Ύ λ©”λ¨λ¦¬: {finetuned_memory:.2f}MB ({memory_change:.1f}% λ³€ν™”)")
            
        except Exception as e:
            print(f"β LoRA νμΈνλ‹ μ‹¤ν¨: {str(e)}")
            results = {"error": str(e)}
        
        # κ²°κ³Ό μ €μ¥
        results_path = self.output_dir / "lora_finetuning_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"π“‹ μ‹¤ν— κ²°κ³Όκ°€ {results_path}μ— μ €μ¥λμ—μµλ‹λ‹¤.")
        
        return results
    
    def analyze_results(self, results: Dict[str, Any]) -> None:
        """μ‹¤ν— κ²°κ³Ό λ¶„μ„"""
        print("\nπ“ LoRA νμΈνλ‹ κ²°κ³Ό λ¶„μ„:")
        print("=" * 60)
        
        if "error" in results:
            print(f"β μ‹¤ν— μ‹¤ν¨: {results['error']}")
            return
        
        base = results["base_model"]
        lora = results["lora_model"]
        
        print(f"π”Ή λ² μ΄μ¤ λ¨λΈ:")
        print(f"  - ν¬κΈ°: {base['size_mb']:.2f}MB")
        print(f"  - μ¶”λ΅  μ†λ„: {base['inference_speed_sec']:.2f}μ΄")
        print(f"  - λ©”λ¨λ¦¬ μ‚¬μ©λ‰: {base['memory_usage_mb']:.2f}MB")
        
        print(f"\nπ”Ή LoRA λ¨λΈ:")
        print(f"  - ν¬κΈ°: {lora['size_mb']:.2f}MB ({lora['size_increase_percent']:.2f}% μ¦κ°€)")
        print(f"  - μ¶”λ΅  μ†λ„: {lora['inference_speed_sec']:.2f}μ΄ ({lora['speed_improvement_percent']:.1f}% ν–¥μƒ)")
        print(f"  - λ©”λ¨λ¦¬ μ‚¬μ©λ‰: {lora['memory_usage_mb']:.2f}MB ({lora['memory_change_percent']:.1f}% λ³€ν™”)")
        
        # λ©ν‘ λ‹¬μ„±λ„ ν‰κ°€
        speed_goal = 100.0  # 2λ°° ν–¥μƒ λ©ν‘
        
        print(f"\nπ― λ©ν‘ λ‹¬μ„±λ„:")
        print(f"  - μ†λ„ ν–¥μƒ: {lora['speed_improvement_percent']:.1f}% / {speed_goal}% ({'β…' if lora['speed_improvement_percent'] >= speed_goal else 'β'})")
        
        if lora['speed_improvement_percent'] >= speed_goal:
            print("π‰ μ†λ„ ν–¥μƒ λ©ν‘ λ‹¬μ„±!")
        else:
            print("π“ μ¶”κ°€ μµμ ν™” ν•„μ”")
        
        print(f"\nπ€ λ‹¤μ λ‹¨κ³„:")
        print("1. μλ£ λ„λ©”μΈ νμΈνλ‹")
        print("2. μµμΆ… μ„±λ¥ λ²¤μΉλ§ν¬")
        print("3. μ‹¤μ  μ„λΉ„μ¤ λ°°ν¬ μ¤€λΉ„")
