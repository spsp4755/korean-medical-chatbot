"""
INT8 ì–‘ìí™” ëª¨ë“ˆ
ëª¨ë¸ì„ INT8ë¡œ ì–‘ìí™”í•˜ì—¬ í¬ê¸°ì™€ ì¶”ë¡  ì†ë„ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from pathlib import Path
import json
import time
from typing import Dict, Any, Optional
import psutil
import os

class ModelQuantizer:
    """ëª¨ë¸ ì–‘ìí™” í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str, output_dir: str = "models/quantized"):
        self.model_name = model_name
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ì›ë³¸ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €
        self.original_model = None
        self.original_tokenizer = None
        self.quantized_model = None
        
        # ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼
        self.quantization_results = {}
    
    def load_original_model(self) -> Dict[str, Any]:
        """ì›ë³¸ ëª¨ë¸ ë¡œë“œ ë° ì„±ëŠ¥ ì¸¡ì •"""
        print(f"ğŸ”„ ì›ë³¸ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")
        
        start_time = time.time()
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.original_tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        if self.original_tokenizer.pad_token is None:
            self.original_tokenizer.pad_token = self.original_tokenizer.eos_token
        
        # ëª¨ë¸ ë¡œë“œ (float32ë¡œ ë¡œë“œí•˜ì—¬ ì••ì¶• íš¨ê³¼ í™•ì¸)
        self.original_model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float32,  # float32ë¡œ ë¡œë“œ
            device_map="auto" if torch.cuda.is_available() else "cpu"
        )
        
        load_time = time.time() - start_time
        
        # ëª¨ë¸ í¬ê¸° ì¸¡ì •
        model_size = self._get_model_size(self.original_model)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        memory_usage = self._get_memory_usage()
        
        # ì¶”ë¡  ì†ë„ ì¸¡ì •
        inference_speed = self._benchmark_inference(self.original_model, self.original_tokenizer)
        
        results = {
            "model_name": self.model_name,
            "load_time": load_time,
            "model_size_mb": model_size,
            "memory_usage_mb": memory_usage,
            "inference_speed_seconds": inference_speed,
            "device": str(next(self.original_model.parameters()).device)
        }
        
        print(f"âœ… ì›ë³¸ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {load_time:.2f}ì´ˆ")
        print(f"ğŸ“Š ëª¨ë¸ í¬ê¸°: {model_size:.2f}MB")
        print(f"ğŸ§  ì¶”ë¡  ì†ë„: {inference_speed:.2f}ì´ˆ")
        
        return results
    
    def quantize_model(self, method: str = "int8") -> Dict[str, Any]:
        """ëª¨ë¸ ì–‘ìí™”"""
        print(f"ğŸ”§ {method.upper()} ì–‘ìí™” ì‹œì‘...")
        
        if method == "int8":
            return self._quantize_int8()
        elif method == "int8_linear":
            return self._quantize_int8_linear_only()
        elif method == "int4":
            return self._quantize_int4()
        elif method == "compression":
            return self._compress_model()
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–‘ìí™” ë°©ë²•: {method}")
    
    def _quantize_int8(self) -> Dict[str, Any]:
        """INT8 ì–‘ìí™” êµ¬í˜„"""
        try:
            # PyTorchì˜ ë™ì  ì–‘ìí™” ì‚¬ìš©
            print("  - ë™ì  ì–‘ìí™” ì ìš© ì¤‘...")
            
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            self.original_model.eval()
            
            # ë™ì  ì–‘ìí™” ì ìš© (Embedding ì œì™¸)
            quantized_model = torch.quantization.quantize_dynamic(
                self.original_model,
                {nn.Linear, nn.LayerNorm},  # Embedding ì œì™¸
                dtype=torch.qint8
            )
            
            self.quantized_model = quantized_model
            
            # ì–‘ìí™”ëœ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
            quantized_size = self._get_model_size(quantized_model)
            quantized_speed = self._benchmark_inference(quantized_model, self.original_tokenizer)
            quantized_memory = self._get_memory_usage()
            
            results = {
                "quantization_method": "int8_dynamic",
                "quantized_size_mb": quantized_size,
                "quantized_speed_seconds": quantized_speed,
                "quantized_memory_mb": quantized_memory,
                "size_reduction_percent": (self.quantization_results["original"]["model_size_mb"] - quantized_size) / self.quantization_results["original"]["model_size_mb"] * 100,
                "speed_improvement_percent": (self.quantization_results["original"]["inference_speed_seconds"] - quantized_speed) / self.quantization_results["original"]["inference_speed_seconds"] * 100
            }
            
            print(f"âœ… INT8 ì–‘ìí™” ì™„ë£Œ")
            print(f"ğŸ“Š ì–‘ìí™”ëœ í¬ê¸°: {quantized_size:.2f}MB")
            print(f"ğŸ§  ì–‘ìí™”ëœ ì†ë„: {quantized_speed:.2f}ì´ˆ")
            print(f"ğŸ“‰ í¬ê¸° ê°ì†Œ: {results['size_reduction_percent']:.1f}%")
            print(f"âš¡ ì†ë„ í–¥ìƒ: {results['speed_improvement_percent']:.1f}%")
            
            return results
            
        except Exception as e:
            print(f"âŒ INT8 ì–‘ìí™” ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e)}
    
    def _quantize_int8_linear_only(self) -> Dict[str, Any]:
        """INT8 ì–‘ìí™” êµ¬í˜„ (Linear ë ˆì´ì–´ë§Œ)"""
        try:
            print("  - Linear ë ˆì´ì–´ë§Œ ì–‘ìí™” ì ìš© ì¤‘...")
            
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            self.original_model.eval()
            
            # CPUì—ì„œ ì–‘ìí™” ì—”ì§„ ì„¤ì •
            torch.backends.quantized.engine = 'qnnpack'
            
            # Linear ë ˆì´ì–´ë§Œ ì–‘ìí™”
            quantized_model = torch.quantization.quantize_dynamic(
                self.original_model,
                {nn.Linear},  # Linear ë ˆì´ì–´ë§Œ
                dtype=torch.qint8
            )
            
            self.quantized_model = quantized_model
            
            # ì–‘ìí™”ëœ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
            quantized_size = self._get_model_size(quantized_model)
            quantized_speed = self._benchmark_inference(quantized_model, self.original_tokenizer)
            quantized_memory = self._get_memory_usage()
            
            results = {
                "quantization_method": "int8_linear_only",
                "quantized_size_mb": quantized_size,
                "quantized_speed_seconds": quantized_speed,
                "quantized_memory_mb": quantized_memory,
                "size_reduction_percent": (self.quantization_results["original"]["model_size_mb"] - quantized_size) / self.quantization_results["original"]["model_size_mb"] * 100,
                "speed_improvement_percent": (self.quantization_results["original"]["inference_speed_seconds"] - quantized_speed) / self.quantization_results["original"]["inference_speed_seconds"] * 100
            }
            
            print(f"âœ… INT8 Linear ì–‘ìí™” ì™„ë£Œ")
            print(f"ğŸ“Š ì–‘ìí™”ëœ í¬ê¸°: {quantized_size:.2f}MB")
            print(f"ğŸ§  ì–‘ìí™”ëœ ì†ë„: {quantized_speed:.2f}ì´ˆ")
            print(f"ğŸ“‰ í¬ê¸° ê°ì†Œ: {results['size_reduction_percent']:.1f}%")
            print(f"âš¡ ì†ë„ í–¥ìƒ: {results['speed_improvement_percent']:.1f}%")
            
            return results
            
        except Exception as e:
            print(f"âŒ INT8 Linear ì–‘ìí™” ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e)}
    
    def _compress_model(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì••ì¶• (ê°€ì¤‘ì¹˜ ì •ë°€ë„ ê°ì†Œ)"""
        try:
            print("  - ëª¨ë¸ ì••ì¶• ì ìš© ì¤‘...")
            
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            self.original_model.eval()
            
            # ì›ë³¸ ëª¨ë¸ì˜ ë°ì´í„° íƒ€ì… í™•ì¸
            original_dtype = next(self.original_model.parameters()).dtype
            print(f"  - ì›ë³¸ ëª¨ë¸ ë°ì´í„° íƒ€ì…: {original_dtype}")
            
            # ëª¨ë¸ì„ float16ìœ¼ë¡œ ë³€í™˜ (ì••ì¶•)
            compressed_model = self.original_model.half()
            
            # ì••ì¶•ëœ ëª¨ë¸ì˜ ë°ì´í„° íƒ€ì… í™•ì¸
            compressed_dtype = next(compressed_model.parameters()).dtype
            print(f"  - ì••ì¶•ëœ ëª¨ë¸ ë°ì´í„° íƒ€ì…: {compressed_dtype}")
            
            self.quantized_model = compressed_model
            
            # ì••ì¶•ëœ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
            compressed_speed = self._benchmark_inference(compressed_model, self.original_tokenizer)
            compressed_memory = self._get_memory_usage()
            
            # ì´ë¡ ì  í¬ê¸° ê³„ì‚°
            original_size = self._get_theoretical_size(self.original_model, torch.float32)
            compressed_size = self._get_theoretical_size(compressed_model, torch.float16)
            
            results = {
                "quantization_method": "float16_compression",
                "original_dtype": str(original_dtype),
                "compressed_dtype": str(compressed_dtype),
                "original_size_mb": original_size,
                "quantized_size_mb": compressed_size,
                "quantized_speed_seconds": compressed_speed,
                "quantized_memory_mb": compressed_memory,
                "size_reduction_percent": (original_size - compressed_size) / original_size * 100,
                "speed_improvement_percent": (self.quantization_results["original"]["inference_speed_seconds"] - compressed_speed) / self.quantization_results["original"]["inference_speed_seconds"] * 100
            }
            
            print(f"âœ… ëª¨ë¸ ì••ì¶• ì™„ë£Œ")
            print(f"ğŸ“Š ì›ë³¸ í¬ê¸°: {original_size:.2f}MB")
            print(f"ğŸ“Š ì••ì¶•ëœ í¬ê¸°: {compressed_size:.2f}MB")
            print(f"ğŸ§  ì••ì¶•ëœ ì†ë„: {compressed_speed:.2f}ì´ˆ")
            print(f"ğŸ“‰ í¬ê¸° ê°ì†Œ: {results['size_reduction_percent']:.1f}%")
            print(f"âš¡ ì†ë„ í–¥ìƒ: {results['speed_improvement_percent']:.1f}%")
            
            return results
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì••ì¶• ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e)}
    
    def _quantize_int4(self) -> Dict[str, Any]:
        """INT4 ì–‘ìí™” êµ¬í˜„ (bitsandbytes ì‚¬ìš©)"""
        try:
            print("  - INT4 ì–‘ìí™” ì ìš© ì¤‘...")
            
            # bitsandbytesë¥¼ ì‚¬ìš©í•œ INT4 ì–‘ìí™”
            from transformers import BitsAndBytesConfig
            
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            
            quantized_model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                quantization_config=quantization_config,
                device_map="auto" if torch.cuda.is_available() else "cpu"
            )
            
            self.quantized_model = quantized_model
            
            # ì„±ëŠ¥ ì¸¡ì •
            quantized_size = self._get_model_size(quantized_model)
            quantized_speed = self._benchmark_inference(quantized_model, self.original_tokenizer)
            quantized_memory = self._get_memory_usage()
            
            results = {
                "quantization_method": "int4_bitsandbytes",
                "quantized_size_mb": quantized_size,
                "quantized_speed_seconds": quantized_speed,
                "quantized_memory_mb": quantized_memory,
                "size_reduction_percent": (self.quantization_results["original"]["model_size_mb"] - quantized_size) / self.quantization_results["original"]["model_size_mb"] * 100,
                "speed_improvement_percent": (self.quantization_results["original"]["inference_speed_seconds"] - quantized_speed) / self.quantization_results["original"]["inference_speed_seconds"] * 100
            }
            
            print(f"âœ… INT4 ì–‘ìí™” ì™„ë£Œ")
            print(f"ğŸ“Š ì–‘ìí™”ëœ í¬ê¸°: {quantized_size:.2f}MB")
            print(f"ğŸ§  ì–‘ìí™”ëœ ì†ë„: {quantized_speed:.2f}ì´ˆ")
            print(f"ğŸ“‰ í¬ê¸° ê°ì†Œ: {results['size_reduction_percent']:.1f}%")
            print(f"âš¡ ì†ë„ í–¥ìƒ: {results['speed_improvement_percent']:.1f}%")
            
            return results
            
        except Exception as e:
            print(f"âŒ INT4 ì–‘ìí™” ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e)}
    
    def _get_model_size(self, model) -> float:
        """ëª¨ë¸ í¬ê¸° ì¸¡ì • (MB)"""
        param_size = 0
        buffer_size = 0
        
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()
        
        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()
        
        size_all_mb = (param_size + buffer_size) / 1024**2
        return size_all_mb
    
    def _get_model_size_by_dtype(self, model) -> float:
        """ë°ì´í„° íƒ€ì…ë³„ ëª¨ë¸ í¬ê¸° ì¸¡ì • (MB) - ì´ë¡ ì  ê³„ì‚°"""
        param_size = 0
        buffer_size = 0
        
        for param in model.parameters():
            # íŒŒë¼ë¯¸í„° ìˆ˜ * ë°ì´í„° íƒ€ì… í¬ê¸° (ì´ë¡ ì )
            if param.dtype == torch.float32:
                param_size += param.nelement() * 4  # float32 = 4 bytes
            elif param.dtype == torch.float16:
                param_size += param.nelement() * 2  # float16 = 2 bytes
            elif param.dtype == torch.int8:
                param_size += param.nelement() * 1  # int8 = 1 byte
            else:
                param_size += param.nelement() * param.element_size()
        
        for buffer in model.buffers():
            if buffer.dtype == torch.float32:
                buffer_size += buffer.nelement() * 4
            elif buffer.dtype == torch.float16:
                buffer_size += buffer.nelement() * 2
            else:
                buffer_size += buffer.nelement() * buffer.element_size()
        
        size_all_mb = (param_size + buffer_size) / 1024**2
        return size_all_mb
    
    def _get_theoretical_size(self, model, target_dtype) -> float:
        """íŠ¹ì • ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë³€í™˜í–ˆì„ ë•Œì˜ ì´ë¡ ì  í¬ê¸° (MB)"""
        param_size = 0
        buffer_size = 0
        
        # íƒ€ì…ë³„ ë°”ì´íŠ¸ í¬ê¸°
        dtype_bytes = {
            torch.float32: 4,
            torch.float16: 2,
            torch.int8: 1,
            torch.int32: 4
        }
        
        target_bytes = dtype_bytes.get(target_dtype, 4)
        
        for param in model.parameters():
            param_size += param.nelement() * target_bytes
        
        for buffer in model.buffers():
            buffer_size += buffer.nelement() * target_bytes
        
        size_all_mb = (param_size + buffer_size) / 1024**2
        return size_all_mb
    
    def _get_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (MB)"""
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / 1024**2
        else:
            process = psutil.Process(os.getpid())
            return process.memory_info().rss / 1024**2
    
    def _benchmark_inference(self, model, tokenizer, num_runs: int = 3) -> float:
        """ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
        test_prompts = [
            "ì•ˆë…•í•˜ì„¸ìš”.",
            "ì˜ë£Œ ìƒë‹´ì„ ë°›ê³  ì‹¶ìŠµë‹ˆë‹¤.",
            "ê°ê¸° ì¦ìƒì´ ìˆìŠµë‹ˆë‹¤."
        ]
        
        times = []
        
        for _ in range(num_runs):
            for prompt in test_prompts:
                inputs = tokenizer(prompt, return_tensors="pt")
                
                start_time = time.time()
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=20,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id
                    )
                end_time = time.time()
                
                times.append(end_time - start_time)
        
        return sum(times) / len(times)
    
    def save_quantized_model(self, model_name: str = "quantized_model"):
        """ì–‘ìí™”ëœ ëª¨ë¸ ì €ì¥"""
        if self.quantized_model is None:
            print("âŒ ì–‘ìí™”ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        save_path = self.output_dir / model_name
        save_path.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ’¾ ì–‘ìí™”ëœ ëª¨ë¸ ì €ì¥ ì¤‘: {save_path}")
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥
        self.quantized_model.save_pretrained(save_path)
        self.original_tokenizer.save_pretrained(save_path)
        
        # ì–‘ìí™” ê²°ê³¼ ì €ì¥
        results_path = save_path / "quantization_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(self.quantization_results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
    
    def run_quantization_experiment(self, methods: list = ["int8"]) -> Dict[str, Any]:
        """ì–‘ìí™” ì‹¤í—˜ ì‹¤í–‰"""
        print("ğŸš€ ì–‘ìí™” ì‹¤í—˜ ì‹œì‘")
        print("=" * 60)
        
        # ì›ë³¸ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
        original_results = self.load_original_model()
        self.quantization_results["original"] = original_results
        
        # ê° ì–‘ìí™” ë°©ë²•ë³„ ì‹¤í—˜
        for method in methods:
            print(f"\n{'='*50}")
            print(f"ğŸ”§ {method.upper()} ì–‘ìí™” ì‹¤í—˜")
            print(f"{'='*50}")
            
            quantized_results = self.quantize_model(method)
            self.quantization_results[method] = quantized_results
            
            # ì–‘ìí™”ëœ ëª¨ë¸ ì €ì¥
            if "error" not in quantized_results:
                self.save_quantized_model(f"model_{method}")
        
        # ì „ì²´ ê²°ê³¼ ì €ì¥
        results_path = self.output_dir / "quantization_experiment_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(self.quantization_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“‹ ì‹¤í—˜ ê²°ê³¼ê°€ {results_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return self.quantization_results

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # 42dot ëª¨ë¸ë¡œ ì–‘ìí™” ì‹¤í—˜
    quantizer = ModelQuantizer("42dot/42dot_LLM-SFT-1.3B")
    
    # INT8 ì–‘ìí™” ì‹¤í—˜ ì‹¤í–‰
    results = quantizer.run_quantization_experiment(["int8"])
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print("\nğŸ“Š ì–‘ìí™” ì‹¤í—˜ ê²°ê³¼ ìš”ì•½:")
    print("=" * 60)
    
    if "original" in results:
        orig = results["original"]
        print(f"ì›ë³¸ ëª¨ë¸: {orig['model_size_mb']:.2f}MB, {orig['inference_speed_seconds']:.2f}ì´ˆ")
    
    if "int8" in results and "error" not in results["int8"]:
        int8 = results["int8"]
        print(f"INT8 ëª¨ë¸: {int8['quantized_size_mb']:.2f}MB, {int8['quantized_speed_seconds']:.2f}ì´ˆ")
        print(f"í¬ê¸° ê°ì†Œ: {int8['size_reduction_percent']:.1f}%")
        print(f"ì†ë„ í–¥ìƒ: {int8['speed_improvement_percent']:.1f}%")

if __name__ == "__main__":
    main()
