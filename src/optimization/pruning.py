"""
Structured Pruning ëª¨ë“ˆ
ê°€ì¤‘ì¹˜ 20-30% ì œê±°ë¥¼ í†µí•œ ëª¨ë¸ ê²½ëŸ‰í™”
"""

import torch
import torch.nn as nn
import torch.nn.utils.prune as prune
import numpy as np
from typing import Dict, Any, List, Tuple
from transformers import AutoModelForCausalLM, AutoTokenizer
import time
import psutil
import os
from pathlib import Path
import json


class ModelPruner:
    """Structured Pruningì„ í†µí•œ ëª¨ë¸ ê²½ëŸ‰í™”"""
    
    def __init__(self, model_name: str, output_dir: str = "models/pruned"):
        self.model_name = model_name
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        self.original_model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32,
            device_map="auto" if torch.cuda.is_available() else "cpu"
        )
        self.original_tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if self.original_tokenizer.pad_token is None:
            self.original_tokenizer.pad_token = self.original_tokenizer.eos_token
        
        self.pruned_model = None
        self.pruning_results = {}
        
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    def _get_model_size(self, model) -> float:
        """ëª¨ë¸ í¬ê¸° ì¸¡ì • (MB)"""
        param_size = 0
        buffer_size = 0
        
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()
        
        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()
        
        size_all_mb = (param_size + buffer_size) / 1024**2
        return size_all_mb
    
    def _benchmark_inference(self, model, tokenizer, num_runs: int = 3) -> float:
        """ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬ (ì´ˆ)"""
        model.eval()
        test_text = "ì•ˆë…•í•˜ì„¸ìš”. ì˜ë£Œ ìƒë‹´ì„ ë°›ê³  ì‹¶ìŠµë‹ˆë‹¤."
        
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(test_text, return_tensors="pt", padding=True, truncation=True)
        
        # ì›Œë°ì—…
        with torch.no_grad():
            for _ in range(2):
                _ = model.generate(
                    inputs.input_ids,
                    max_length=inputs.input_ids.shape[1] + 20,
                    num_return_sequences=1,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id
                )
        
        # ì‹¤ì œ ì¸¡ì •
        start_time = time.time()
        with torch.no_grad():
            for _ in range(num_runs):
                _ = model.generate(
                    inputs.input_ids,
                    max_length=inputs.input_ids.shape[1] + 20,
                    num_return_sequences=1,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id
                )
        end_time = time.time()
        
        avg_time = (end_time - start_time) / num_runs
        return avg_time
    
    def _get_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (MB)"""
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / 1024**2
        else:
            process = psutil.Process(os.getpid())
            return process.memory_info().rss / 1024**2
    
    def _apply_structured_pruning(self, sparsity: float) -> nn.Module:
        """Structured Pruning ì ìš©"""
        print(f"  - {sparsity*100:.1f}% ê°€ì¤‘ì¹˜ ì œê±° ì¤‘...")
        
        # ëª¨ë¸ ë³µì‚¬
        pruned_model = self.original_model
        
        # Pruningí•  ë ˆì´ì–´ ì‹ë³„
        layers_to_prune = []
        for name, module in pruned_model.named_modules():
            if isinstance(module, nn.Linear):
                layers_to_prune.append((module, 'weight'))
        
        print(f"  - {len(layers_to_prune)}ê°œ Linear ë ˆì´ì–´ ë°œê²¬")
        
        # Structured Pruning ì ìš©
        for module, param_name in layers_to_prune:
            # Magnitude-based pruningìœ¼ë¡œ ì¤‘ìš”í•˜ì§€ ì•Šì€ ë‰´ëŸ° ì‹ë³„
            prune.ln_structured(
                module, 
                name=param_name, 
                amount=sparsity, 
                n=2,  # L2 norm
                dim=0  # ì¶œë ¥ ì°¨ì›ì—ì„œ ì œê±°
            )
        
        # Pruning ë§ˆìŠ¤í¬ ì ìš©
        for module, param_name in layers_to_prune:
            prune.remove(module, param_name)
        
        return pruned_model
    
    def _evaluate_pruning_quality(self, original_model, pruned_model, tokenizer) -> Dict[str, float]:
        """Pruning í’ˆì§ˆ í‰ê°€"""
        print("  - Pruning í’ˆì§ˆ í‰ê°€ ì¤‘...")
        
        # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë“¤
        test_texts = [
            "ì•ˆë…•í•˜ì„¸ìš”. ì˜ë£Œ ìƒë‹´ì„ ë°›ê³  ì‹¶ìŠµë‹ˆë‹¤.",
            "ë¨¸ë¦¬ê°€ ì•„í”ˆë° ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?",
            "ê°ê¸° ì¦ìƒì´ ìˆëŠ”ë° ë³‘ì›ì— ê°€ì•¼ í• ê¹Œìš”?",
            "ë³µí†µì´ ì‹¬í•œë° ì‘ê¸‰ì‹¤ì— ê°€ì•¼ í• ê¹Œìš”?",
            "í”¼ë¶€ì— ë°œì§„ì´ ìƒê²¼ëŠ”ë° ì›ì¸ì´ ë­˜ê¹Œìš”?"
        ]
        
        quality_scores = []
        
        for text in test_texts:
            # ì›ë³¸ ëª¨ë¸ ì¶”ë¡ 
            inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
            
            with torch.no_grad():
                original_output = original_model.generate(
                    inputs.input_ids,
                    max_length=inputs.input_ids.shape[1] + 30,
                    num_return_sequences=1,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id
                )
                
                pruned_output = pruned_model.generate(
                    inputs.input_ids,
                    max_length=inputs.input_ids.shape[1] + 30,
                    num_return_sequences=1,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id
                )
            
            # ì¶œë ¥ ë¹„êµ (ê°„ë‹¨í•œ ìœ ì‚¬ë„ ì¸¡ì •)
            original_text = tokenizer.decode(original_output[0], skip_special_tokens=True)
            pruned_text = tokenizer.decode(pruned_output[0], skip_special_tokens=True)
            
            # ë‹¨ì–´ ìˆ˜ ê¸°ë°˜ ìœ ì‚¬ë„ (ê°„ë‹¨í•œ ë©”íŠ¸ë¦­)
            original_words = set(original_text.split())
            pruned_words = set(pruned_text.split())
            
            if len(original_words) > 0:
                similarity = len(original_words.intersection(pruned_words)) / len(original_words)
                quality_scores.append(similarity)
        
        avg_quality = np.mean(quality_scores) if quality_scores else 0.0
        return {"quality_score": avg_quality, "num_tests": len(test_texts)}
    
    def run_pruning_experiment(self, sparsity_levels: List[float] = [0.1, 0.2, 0.3]) -> Dict[str, Any]:
        """Pruning ì‹¤í—˜ ì‹¤í–‰"""
        print("ğŸš€ Structured Pruning ì‹¤í—˜ ì‹œì‘")
        print("=" * 50)
        
        # ì›ë³¸ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
        print("ğŸ“Š ì›ë³¸ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        original_size = self._get_model_size(self.original_model)
        original_speed = self._benchmark_inference(self.original_model, self.original_tokenizer)
        original_memory = self._get_memory_usage()
        
        print(f"ğŸ“Š ì›ë³¸ í¬ê¸°: {original_size:.2f}MB")
        print(f"ğŸ§  ì›ë³¸ ì†ë„: {original_speed:.2f}ì´ˆ")
        print(f"ğŸ’¾ ì›ë³¸ ë©”ëª¨ë¦¬: {original_memory:.2f}MB")
        
        results = {
            "original_model": {
                "size_mb": original_size,
                "inference_speed_sec": original_speed,
                "memory_usage_mb": original_memory
            },
            "pruning_experiments": []
        }
        
        # ê° sparsity levelì— ëŒ€í•´ ì‹¤í—˜
        for sparsity in sparsity_levels:
            print(f"\n{'='*50}")
            print(f"ğŸ”§ {sparsity*100:.1f}% Pruning ì‹¤í—˜")
            print(f"{'='*50}")
            
            try:
                # Pruning ì ìš©
                pruned_model = self._apply_structured_pruning(sparsity)
                
                # ì„±ëŠ¥ ì¸¡ì •
                pruned_size = self._get_model_size(pruned_model)
                pruned_speed = self._benchmark_inference(pruned_model, self.original_tokenizer)
                pruned_memory = self._get_memory_usage()
                
                # í’ˆì§ˆ í‰ê°€
                quality_results = self._evaluate_pruning_quality(
                    self.original_model, pruned_model, self.original_tokenizer
                )
                
                # ê²°ê³¼ ê³„ì‚°
                size_reduction = ((original_size - pruned_size) / original_size) * 100
                speed_improvement = ((original_speed - pruned_speed) / original_speed) * 100
                memory_reduction = ((original_memory - pruned_memory) / original_memory) * 100
                
                experiment_result = {
                    "sparsity": sparsity,
                    "size_mb": pruned_size,
                    "inference_speed_sec": pruned_speed,
                    "memory_usage_mb": pruned_memory,
                    "size_reduction_percent": size_reduction,
                    "speed_improvement_percent": speed_improvement,
                    "memory_reduction_percent": memory_reduction,
                    "quality_score": quality_results["quality_score"],
                    "num_quality_tests": quality_results["num_tests"]
                }
                
                results["pruning_experiments"].append(experiment_result)
                
                print(f"âœ… {sparsity*100:.1f}% Pruning ì™„ë£Œ")
                print(f"ğŸ“Š í¬ê¸°: {pruned_size:.2f}MB ({size_reduction:.1f}% ê°ì†Œ)")
                print(f"ğŸ§  ì†ë„: {pruned_speed:.2f}ì´ˆ ({speed_improvement:.1f}% í–¥ìƒ)")
                print(f"ğŸ’¾ ë©”ëª¨ë¦¬: {pruned_memory:.2f}MB ({memory_reduction:.1f}% ê°ì†Œ)")
                print(f"ğŸ¯ í’ˆì§ˆ ì ìˆ˜: {quality_results['quality_score']:.3f}")
                
                # ìµœì  ëª¨ë¸ ì €ì¥ (20% pruning)
                if sparsity == 0.2:
                    self.pruned_model = pruned_model
                    model_path = self.output_dir / "model_20_percent_pruned"
                    print(f"ğŸ’¾ ìµœì  ëª¨ë¸ ì €ì¥ ì¤‘: {model_path}")
                    pruned_model.save_pretrained(model_path)
                    self.original_tokenizer.save_pretrained(model_path)
                    print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
                
            except Exception as e:
                print(f"âŒ {sparsity*100:.1f}% Pruning ì‹¤íŒ¨: {str(e)}")
                results["pruning_experiments"].append({
                    "sparsity": sparsity,
                    "error": str(e)
                })
        
        # ê²°ê³¼ ì €ì¥
        results_path = self.output_dir / "pruning_experiment_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“‹ ì‹¤í—˜ ê²°ê³¼ê°€ {results_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return results
    
    def analyze_results(self, results: Dict[str, Any]) -> None:
        """ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­ ì œì‹œ"""
        print("\nğŸ“Š Pruning ì‹¤í—˜ ê²°ê³¼ ë¶„ì„:")
        print("=" * 60)
        
        original = results["original_model"]
        print(f"ğŸ”¹ ì›ë³¸ ëª¨ë¸:")
        print(f"  - í¬ê¸°: {original['size_mb']:.2f}MB")
        print(f"  - ì¶”ë¡  ì†ë„: {original['inference_speed_sec']:.2f}ì´ˆ")
        print(f"  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {original['memory_usage_mb']:.2f}MB")
        
        print(f"\nğŸ”¹ Pruning ì‹¤í—˜ ê²°ê³¼:")
        for exp in results["pruning_experiments"]:
            if "error" not in exp:
                print(f"  - {exp['sparsity']*100:.1f}% ì œê±°:")
                print(f"    í¬ê¸°: {exp['size_mb']:.2f}MB ({exp['size_reduction_percent']:.1f}% ê°ì†Œ)")
                print(f"    ì†ë„: {exp['inference_speed_sec']:.2f}ì´ˆ ({exp['speed_improvement_percent']:.1f}% í–¥ìƒ)")
                print(f"    ë©”ëª¨ë¦¬: {exp['memory_usage_mb']:.2f}MB ({exp['memory_reduction_percent']:.1f}% ê°ì†Œ)")
                print(f"    í’ˆì§ˆ: {exp['quality_score']:.3f}")
        
        # ìµœì  sparsity ì¶”ì²œ
        best_experiment = None
        best_score = -1
        
        for exp in results["pruning_experiments"]:
            if "error" not in exp:
                # ì¢…í•© ì ìˆ˜ ê³„ì‚° (í¬ê¸° ê°ì†Œ + ì†ë„ í–¥ìƒ + í’ˆì§ˆ ë³´ì¡´)
                score = (exp['size_reduction_percent'] * 0.3 + 
                        exp['speed_improvement_percent'] * 0.3 + 
                        exp['quality_score'] * 100 * 0.4)
                
                if score > best_score:
                    best_score = score
                    best_experiment = exp
        
        if best_experiment:
            print(f"\nğŸ† ìµœì  Pruning ì„¤ì •: {best_experiment['sparsity']*100:.1f}% ì œê±°")
            print(f"   ì¢…í•© ì ìˆ˜: {best_score:.2f}")
        
        print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. Knowledge Distillation êµ¬í˜„")
        print("2. LoRA íŒŒì¸íŠœë‹ ì ìš©")
        print("3. ì˜ë£Œ ë„ë©”ì¸ íŒŒì¸íŠœë‹")
